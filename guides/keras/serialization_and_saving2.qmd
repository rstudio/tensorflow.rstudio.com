---
title: Save, serialize, and export models
authors: Neel Kovelamudi, Francois Chollet, Tomasz Kalinowski
Last modified: 2023/08/29
description: Complete guide to saving, serializing, and exporting models.
---

**Note: this guide assumes Keras \>= 2.13**

## Introduction

A Keras model consists of multiple components:

-   The architecture, or configuration, which specifies what layers the
    model contain, and how they're connected.
-   A set of weights values (the "state of the model").
-   An optimizer (defined by compiling the model).
-   A set of losses and metrics (defined by compiling the model).

The Keras API saves all of these pieces together in a unified format,
marked by the `.keras` extension. This is a zip archive consisting of
the following:

-   A JSON-based configuration file (config.json): Records of model,
    layer, and other trackables' configuration.
-   A H5-based state file, such as `model.weights.h5` (for the whole
    model), with directory keys for layers and their weights.
-   A metadata file in JSON, storing things such as the current Keras
    version.

Let's take a look at how this works.

## How to save and load a model

If you only have 10 seconds to read this guide, here's what you need to
know.

**Saving a Keras model:**

``` r
model = ...  # Get model (Sequential, Functional Model, or Model subclass)
model$save('path/to/location.keras')  # The file needs to end with the .keras extension
```

**Loading the model back:**

``` r
model <- keras$models$load_model('path/to/location.keras')
```

Now, let's look at the details.

## Setup

```{r}
library(keras)
```

## Saving

This section is about saving an entire model to a single file. The file
will include:

-   The model's architecture/config
-   The model's weight values (which were learned during training)
-   The model's compilation information (if `compile()` was called)
-   The optimizer and its state, if any (this enables you to restart
    training where you left)

#### APIs

You can save a model with `model$save()` or `keras$models$save_model()`
(which is equivalent). You can load it back with
`keras$models$load_model()`.

The recommended format is the "Keras v3" format, which uses the `.keras`
extension. There are, however, two legacy formats that are available:
the **TensorFlow SavedModel format** and the older Keras **H5 format**.

You can switch to the SavedModel format by:

-   Passing `save_format='tf'` to `save()`
-   Passing a filename without an extension

You can switch to the H5 format by:

-   Passing `save_format='h5'` to `save()`
-   Passing a filename that ends in `.h5`

**Example:**

```{r}
get_model <- function() {
    # Create a simple model.
  inputs <- layer_input(shape(32))  
  outputs <- inputs %>% layer_dense(1)
  model <- keras_model(inputs, outputs)
  model %>% compile(optimizer = optimizer_adam(),
                    loss = "mean_squared_error")
  model
}


model <- get_model()

# Train the model.
test_input <- k_random_uniform(c(128, 32))
test_target <- k_random_uniform(c(128, 1))
model %>% fit(test_input, test_target)

# Calling `save('my_model.keras')` creates a zip archive `my_model.keras`.
model$save("my_model.keras")

# It can be used to reconstruct the model identically.
reconstructed_model <- keras$models$load_model("my_model.keras")

# Let's check:
all.equal(
    model %>% predict(test_input), 
    reconstructed_model %>% predict(test_input)
)
```

### Custom objects

This section covers the basic workflows for handling custom layers,
functions, and models in Keras saving and reloading.

When saving a model that includes custom objects, such as a subclassed
Layer, you **must** define a `get_config()` method on the object class.
If the arguments passed to the constructor (`initialize()` method) of
the custom object aren't Python objects (anything other than base types
like ints, strings, etc.), then you **must** serialize these arguments
in `get_config()` method and also explicitly deserialize these arguments
in the `from_config()` class method.

Like this:

```{r}
classmethod <- reticulate::py_eval("classmethod")

layer_custom <- new_layer_class(
  classname = "CustomLayer",
  
  initialize = function(sublayer, ...) {
    super$initialize(...)
    self$sublayer <- sublayer
  },
  
  call = function(x) {
    self$sublayer(x)
  },
  
  get_config = function() {
    base_config <- super$get_config()
    config <- list(sublayer = keras$saving$serialize_keras_object(self$sublayer))
    c(base_config, config)
  },
  
  from_config = function(config) {
    sublayer_config <- config$sublayer
    sublayer <- keras$saving$deserialize_keras_object(sublayer_config)
    config$sublayer <- NULL
    # `__class__`(sublayer, !!!config)
    layer_custom(NULL, sublayer, !!!config)
  }
)
```

Please see the [Defining the config methods section](#config_methods)
for more details and examples.

The saved `.keras` file is lightweight and does not store the Python
code for custom objects. Therefore, to reload the model, `load_model`
requires access to the definition of any custom objects used through one
of the following methods:

1.  Registering custom objects **(preferred)**,
2.  Passing custom objects directly when loading, or
3.  Using a custom object scope

Below are examples of each workflow:

#### Registering custom objects (**preferred**)

This is the preferred method, as custom object registration greatly
simplifies saving and loading code. Using the
`keras$saving$register_keras_serializable` decorator to the class
definition of a custom object registers the object globally in a master
list, allowing Keras to recognize the object when loading the model.

Let's create a custom model involving both a custom layer and a custom
activation function to demonstrate this.

**Example:**

```{r}

# Clear all previously registered custom objects
reticulate::r_to_py(keras$saving)$get_custom_objects()$clear()

# define a custom layer class
CustomLayer(keras$layers$Layer) %py_class% {
  initialize <- function(factor) {
    super$initialize()
    self$factor <- factor
  }
  
  call <- function(x) {
    x * self$factor
  }
  
  get_config <- function() {
    list(factor = self$factor)
  }
}

# Upon registration, you can optionally specify a package or a name.
# If left blank, the package defaults to `Custom` and the name defaults to
# the class name.
CustomLayer %<>% {keras$saving$register_keras_serializable(package = "MyLayers")(.)}


# define a custom function
custom_fn <- function(x) x**2

# register the custom function
custom_fn %<>% 
  {keras$saving$register_keras_serializable(package="my_package", 
                                           name = "custom_fn")(.)}

layer_custom_layer <- create_layer_wrapper(CustomLayer) 

# Create the model.
get_model <- function() {
    inputs <- layer_input(shape(4))
    mid <- inputs %>% layer_custom_layer(0.5)
    outputs <- mid %>% layer_dense(1, activation=custom_fn)
    model <- keras_model(inputs, outputs)
    model %>%
      compile(optimizer = "rmsprop", loss = "mean_squared_error")
    model
}

# Train the model.
train_model <- function(model) {
  input <- k_random_uniform(c(4, 4))
  target <- k_random_uniform(c(4, 1))
  model %>% fit(input, target)
  model
}

test_input <- k_random_uniform(c(4, 4))
test_target <- k_random_uniform(c(4, 1))

model <- get_model()
model <- train_model(model)
model$save("custom_model.keras")

# Now, we can simply load without worrying about our custom objects.
reconstructed_model <- keras$models$load_model("custom_model.keras")

# Let's check:
all.equal(
    model %>% predict(test_input), 
    reconstructed_model %>% predict(test_input)
)
```

#### Passing custom objects to `load_model()`

```{r}

model <- get_model()
model <- train_model(model)

# Calling `save('my_model.keras')` creates a zip archive `my_model.keras`.
model$save("custom_model.keras")

# Upon loading, pass a dict containing the custom objects used in the
# `custom_objects` argument of `keras.models.load_model()`.
reconstructed_model <- keras$models$load_model(
  "custom_model.keras",
  custom_objects = list("CustomLayer" = CustomLayer, 
                        "custom_fn" = custom_fn)
)

# Let's check:
all.equal(
    model %>% predict(test_input), 
    reconstructed_model %>% predict(test_input)
)
```

#### Using a custom object scope

Any code within the custom object scope will be able to recognize the
custom objects passed to the scope argument. Therefore, loading the
model within the scope will allow the loading of our custom objects.

**Example:**

```{r}
model <- get_model()
model <- train_model(model)
model$save("custom_model.keras")

# Pass the custom objects dictionary to a custom object scope and place
# the `keras.models.load_model()` call within the scope.
custom_objects <- list("CustomLayer" = CustomLayer, 
                       "custom_fn" = custom_fn)

with(keras$saving$custom_object_scope(custom_objects), {
  reconstructed_model <- keras$models$load_model("custom_model.keras")
})

# Let's check:
all.equal(
  model %>% predict(test_input), 
  reconstructed_model %>% predict(test_input)
)
```

### Model serialization

This section is about saving only the model's configuration, without its
state. The model's configuration (or architecture) specifies what layers
the model contains, and how these layers are connected. If you have the
configuration of a model, then the model can be created with a freshly
initialized state (no weights or compilation information).

#### APIs

The following serialization APIs are available:

-   `keras$models$clone_model(model)`: make a (randomly initialized)
    copy of a model.
-   `get_config()` and `cls$from_config()`: retrieve the configuration
    of a layer or model, and recreate a model instance from its config,
    respectively.
-   `keras$models$to_json()` and `keras$models$model_from_json()`:
    similar, but as JSON strings.
-   `keras$saving$serialize_keras_object()`: retrieve the configuration
    any arbitrary Keras object.
-   `keras$saving$deserialize_keras_object()`: recreate an object
    instance from its configuration.

#### In-memory model cloning

You can do in-memory cloning of a model via
`keras$models$clone_model()`. This is equivalent to getting the config
then recreating the model from its config (so it does not preserve
compilation information or layer weights values).

**Example:**

```{r}
new_model <- keras$models$clone_model(model)
```

#### `get_config()` and `from_config()`

Calling `model$get_config()` or `layer$get_config()` will return a
Python dict containing the configuration of the model or layer,
respectively. You should define `get_config()` to contain arguments
needed for the `initialize()` method of the model or layer. At loading
time, the `from_config(config)` method will then call `initialize()`
with these arguments to reconstruct the model or layer.

**Layer example:**

```{r}
layer <- keras$layers$Dense(3L, activation="relu")
layer_config <- layer$get_config()
print(layer_config)
```

Now let's reconstruct the layer using the `from_config()` method:

```{r}
new_layer <- keras$layers$Dense$from_config(layer_config)
```

**Sequential model example:**

```{r}
model <- keras_model_sequential(input_shape = c(32)) %>% 
  layer_dense(1)
config <- model$get_config()
new_model <- keras$Sequential$from_config(config)
```

**Functional model example:**

```{r}
inputs <- layer_input(c(32))
outputs <- inputs %>% layer_dense(1) 
model <- keras_model(inputs, outputs)
config <- model$get_config()
new_model <- keras$Model$from_config(config)
```

#### `to_json()` and `keras$models$model_from_json()`

This is similar to `get_config` / `from_config`, except it turns the
model into a JSON string, which can then be loaded without the original
model class. It is also specific to models, it isn't meant for layers.

**Example:**

```{r}
model <- keras_model_sequential(input_shape = c(32)) %>% 
  layer_dense(1)
json_config <- model$to_json()
new_model <- keras$models$model_from_json(json_config)
```

#### Arbitrary object serialization and deserialization

The `keras$saving$serialize_keras_object()` and
`keras$saving$deserialize_keras_object()` APIs are general-purpose APIs
that can be used to serialize or deserialize any Keras object and any
custom object. It is at the foundation of saving model architecture and
is behind all `serialize()`/`deserialize()` calls in keras.

**Example**:

```{r}
my_reg <- regularizer_l1(0.005)
config <- keras$saving$serialize_keras_object(my_reg)
print(config)
```

Note the serialization format containing all the necessary information
for proper reconstruction:

-   `module` containing the name of the Keras module or other
    identifying module the object comes from
-   `class_name` containing the name of the object's class.
-   `config` with all the information needed to reconstruct the object
-   `registered_name` for custom objects. See
    [here](#custom_object_serialization).

Now we can reconstruct the regularizer.

```{r}
new_reg <- keras$saving$deserialize_keras_object(config)
```

### Model weights saving

You can choose to only save & load a model's weights. This can be useful
if:

-   You only need the model for inference: in this case you won't need
    to restart training, so you don't need the compilation information
    or optimizer state.
-   You are doing transfer learning: in this case you will be training a
    new model reusing the state of a prior model, so you don't need the
    compilation information of the prior model.

#### APIs for in-memory weight transfer

Weights can be copied between different objects by using `get_weights()`
and `set_weights()`:

-   `keras$layers$Layer$get_weights()`: Returns a list of NumPy arrays
    of weight values.
-   `keras$layers$Layer$set_weights(weights)`: Sets the model weights to
    the values provided (as arrays).

Examples:

***Transfering weights from one layer to another, in memory***

```{r}
create_layer <- function() {
  layer <- keras$layers$Dense(64, activation = "relu", name = "dense_2")
  layer$build(shape(NULL, 784))
  layer
}

layer_1 <- create_layer()
layer_2 <- create_layer()

# Copy weights from layer 1 to layer 2
layer_2$set_weights(layer_1$get_weights())
```

***Transfering weights from one model to another model with a compatible
architecture, in memory***

```{r}
# Create a simple functional model
inputs <- layer_input(shape(784), name = "digits")
outputs <- inputs %>%
  layer_dense(64, activation = "relu", name = "dense_1") %>%
  layer_dense(64, activation = "relu", name = "dense_2") %>%
  layer_dense(10, name = "predictions")

functional_model <- keras_model(inputs, outputs, name = "3_layer_mlp")

# Define a subclassed model with the same architecture
SubclassedModel <- new_model_class(classname = "SubclassedModel",
  initialize = function(output_dim, name = NULL) {
    super$initialize(name = name)
    self$output_dim <- output_dim
    self$dense_1 <-
      keras$layers$Dense(64, activation = "relu", name = "dense_1")
    self$dense_2 <-
      keras$layers$Dense(64, activation = "relu", name = "dense_2")
    self$dense_3 <-
      keras$layers$Dense(output_dim, name = "predictions")
  },
  
  call = function(inputs) {
    inputs %>%
      self$dense_1() %>%
      self$dense_2() %>%
      self$dense_3()
  },
  
  get_config = function() {
    list("output_dim" = self$output_dim,
         "name":self$name)
  }
)
    


subclassed_model <- SubclassedModel(10)

# Call the subclassed model once to create the weights.
subclassed_model(tf$ones(shape(1, 784)))

# Copy weights from functional_model to subclassed_model.
subclassed_model$set_weights(functional_model$get_weights())

stopifnot({
  length(functional_model$weights) == length(subclassed_model$weights)
  all.equal(
    lapply(functional_model$weights, as.array),
    lapply(subclassed_model$weights, as.array)
  )
})
```

***The case of stateless layers***

Because stateless layers do not change the order or number of weights,
models can have compatible architectures even if there are extra/missing
stateless layers.

```{r}
inputs <- layer_input(shape(784), name = "digits")
outputs <- inputs %>%
  layer_dense(64, activation = "relu", name = "dense_1") %>%
  layer_dense(64, activation = "relu", name = "dense_2") %>%
  layer_dense(10, name = "predictions")

functional_model = keras_model(inputs, outputs, name = "3_layer_mlp")

inputs <- layer_input(shape(784), name = "digits")
outputs <- inputs %>%
  layer_dense(64, activation = "relu", name = "dense_1") %>%
  layer_dense(64, activation = "relu", name = "dense_2") %>%
  # Add a dropout layer, which does not contain any weights.
  layer_dropout(0.5) %>%
  layer_dense(10, name = "predictions")

functional_model_with_dropout = keras_model(inputs, outputs,
                                            name = "3_layer_mlp")

functional_model_with_dropout$set_weights(functional_model$get_weights())
```

#### APIs for saving weights to disk & loading them back

Weights can be saved to disk by calling `model.save_weights(filepath)`.
The filename should end in `.weights.h5`.

**Example:**

```{r}

# Runnable example
sequential_model <-
  keras_model_sequential(input_shape = c(784)) %>%
  layer_dense(64, activation = "relu", name = "dense_1") %>%
  layer_dense(64, activation = "relu", name = "dense_2") %>%
  layer_dense(10, name = "predictions")

sequential_model$save_weights("my_model.weights.h5")
sequential_model$load_weights("my_model.weights.h5")
```

Note that changing `layer$trainable` may result in a different
`layer$weights` ordering when the model contains nested layers.

```{r}
layer_nested_dense_layer <- new_layer_class(
  "NestedDenseLayer",
  initialize = function(units, name = NULL) {
    super$initialize(name = name)
    self$dense_1 <- keras$layers$Dense(units, name = "dense_1")
    self$dense_2 <- keras$layers$Dense(units, name = "dense_2")
  }
  
  call = function(inputs) {
    inputs %>%
      self$dense_1() %>%
      self$dense_2()
  }
)


nested_model <- keras_model_sequential(input_shape = c(784)) %>% 
  layer_nested_dense_layer(10, "nested")

variable_names <- map_chr(nested_model.weights, \(v) v$name)
variable_names

cat("Changing trainable status of one of the nested layers...")
nested_model$get_layer("nested")$dense_1$trainable <- FALSE

variable_names_2 <- map_chr(nested_model.weights, \(v) v$name)
variable_names_2
print("variable ordering changed:")
variable_names != variable_names_2
```

##### **Transfer learning example**

When loading pretrained weights from a weights file, it is recommended
to load the weights into the original checkpointed model, and then
extract the desired weights/layers into a new model.

**Example:**

```{r}
create_functional_model <- function() {
  inputs <- layer_input(shape(784), name = "digits")
  outputs <- inputs %>%
    layer_dense(64, activation = "relu", name = "dense_1") %>%
    layer_dense(64, activation = "relu", name = "dense_2") %>%
    layer_dense(10, name = "predictions")
  keras_model(inputs, outputs, name = "3_layer_mlp")
}

functional_model <- create_functional_model()
functional_model$save_weights("pretrained.weights.h5")

# In a separate program:
pretrained_model <- create_functional_model()
pretrained_model$load_weights("pretrained.weights.h5")

# Create a new model by extracting layers from the original model:
extracted_layers <- pretrained_model$layers %>% head(-1)
extracted_layers %<>% c(keras$layers$Dense(5, name="dense_3"))
model <- keras_model_sequential(extracted_layers)
model
```

## Exporting

Keras also lets you to create a lightweight version of your model for
inferencing that contains the model's forward pass only (the `call()`
method). This TensorFlow SavedModel artifact can then be served via
TF-Serving, and all original code of the model (including custom layers)
are no longer necessary to reload the artifact--it is entirely
standalone.

#### APIs

-   `model$export()`, which exports the model to a lightweight
    SavedModel artifact for inference
-   `artifact$serve()`, which calls the exported artifact's forward pass

Lower level API for customization:

-   `keras$export$ExportArchive`, which can be used to customize the
    serving endpoints. This is used internally by `model$export()`.

### Simple exporting with .export()

Let's go through a simple example of `model$export()` using a Functional
model.

**Example:**

```{r}
inputs <- layer_input(shape(16))
outputs <- inputs %>%
  layer_dense(8, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)

input_data <- k_random_normal(c(8, 16))
output_data <- model(input_data)  # **NOTE**: Make sure your model is built!

# Export the model as a SavedModel artifact in a filepath.
model$export("exported_model")

# Reload the SavedModel artifact
reloaded_artifact <- tensorflow::tf$saved_model$load("exported_model")

# Use the `.serve()` endpoint to call the forward pass on the input data
new_output_data <- reloaded_artifact$serve(input_data)
```

### Customizing export artifacts with ExportArchive

The `ExportArchive` object allows you to customize exporting the model
and add additional endpoints for serving. Here are its associated APIs:

-   `track()` to register the layer(s) or model(s) to be used,
-   `add_endpoint()` method to register a new serving endpoint.
-   `write_out()` method to save the artifact.
-   `add_variable_collection` method to register a set of variables to
    be retrieved after reloading.

By default, `model.export("path/to/location")` does the following:

```{r}
export_archive <- ExportArchive()
export_archive$track(model)
export_archive$add_endpoint(
    name="serve",
    fn=model$call,
input_signature <- list(tf$TensorSpec(shape(NA, 3), dtype=tf$float32)),  
# `input_signature` changes depending on model.
)
export_archive$write_out("path/to/location")
```

Let's look at an example customizing this for a MultiHeadAttention
layer.

**Example:**

```{r}

layer <-  keras$layers$MultiHeadAttention(2L, 2L)
x1 <- tf$random$normal(shape(3, 2, 2))
x2 <- tf$random$normal(shape(3, 2, 2))
ref_output <- layer(x1, x2) %>% as.array()  # **NOTE**: Make sure layer is built!

export_archive <- keras$export$ExportArchive()  # Instantiate ExportArchive object
export_archive$track(layer)  # Register the layer to be used
export_archive$add_endpoint(# New endpoint `call` corresponding to `model.call`
  "call",
  layer$call,
  input_signature = list(
    # input signature corresponding to 2 inputs
    tf$TensorSpec(shape = shape(NA, 2, 2),
                  dtype = tf$float32),
    tf$TensorSpec(shape = shape(NA, 2, 2),
                  dtype = tf$float32)
  ))

# Register the layer weights as a set of variables to be retrieved
export_archive$add_variable_collection("my_vars", layer$weights)
stopifnot(length(export_archive$my_vars) == 8)
# weights corresponding to 2 inputs, each of which are 2*2

# Save the artifact
export_archive$write_out("exported_mha_layer")

# Reload the artifact
revived_layer <- tf$saved_model$load("exported_mha_layer")
stopifnot({
  all.equal(ref_output,
            as.array(revived_layer$call(query = x1, value = x2)),
            tolerance = 1e-6,)
  length(revived_layer.my_vars) == 8
})
```

### Appendix: Handling custom objects

### Defining the config methods

Specifications:

-   `get_config()` should return a JSON-serializable dictionary in order
    to be compatible with the Keras architecture- and model-saving APIs.
-   `from_config(config)` (a `classmethod`) should return a new layer or
    model object that is created from the config. The default
    implementation returns `cls(!!!config)`.

**NOTE**: If all your constructor arguments are already serializable,
e.g. strings and ints, or non-custom Keras objects, overriding
`from_config` is not necessary. However, for more complex objects such
as layers or models passed to `initialize()`, deserialization must be
handled explicitly either in `initialize()` itself or overriding the
`from_config()` method.

**Example:**

```{r}
@keras.saving.register_keras_serializable(package="MyLayers", name="KernelMult")
MyDense(keras$layers$Layer) %py_class% {
  
    initialize <- function(units, ...,
        kernel_regularizer=NULL,
        kernel_initializer=NULL,
        nested_model=NULL) {
      
        super$initialize(...)
        self$hidden_units <- units
        self$kernel_regularizer <- kernel_regularizer
        self$kernel_initializer <- kernel_initializer
        self$nested_model <- nested_model
    }

    get_config <- function(self) {
      config <- super$get_config()
      # Update the config with the custom layer's parameters
      config %<>% modifyList(list(
        "units" = self.hidden_units,
        "kernel_regularizer" = self.kernel_regularizer,
        "kernel_initializer" = self.kernel_initializer,
        "nested_model" = self.nested_model))
      
    }

    build <- function(input_shape) {
      
        input_units <- input_shape[-1]
        self.kernel <- self$add_weight(
            name="kernel",
            shape=shape(input_units, self$hidden_units),
            regularizer=self$kernel_regularizer,
            initializer=self$kernel_initializer,
        )
    }

    call <- function(inputs) inputs %*% self$kernel
}


layer = MyDense(units=16, kernel_regularizer="l1", kernel_initializer="ones")
layer3 = MyDense(units=64, nested_model=layer)

config = keras$layers$serialize(layer3)

print(config)

new_layer = keras$layers$deserialize(config)

print(new_layer)
```

Note that overriding `from_config` is unnecessary above for `MyDense`
because `hidden_units`, `kernel_initializer`, and `kernel_regularizer`
are ints, strings, and a built-in Keras object, respectively. This means
that the default `from_config` implementation of `cls(**config)` will
work as intended.

For more complex objects, such as layers and models passed to
`__init__`, for example, you must explicitly deserialize these objects.
Let's take a look at an example of a model where a `from_config`
override is necessary.

**Example:** <a name="registration_example"></a>

```{r}



CustomModel(keras$layers$Layer) %py_class% {
  initialize <- function(first_layer, second_layer = None, ...) {
    super$initialize(...)
    self$first_layer <- first_layer
    self$second_layer <-
      second_layer %||% keras$layers$Dense(8L)
  }
  
  get_config <- function() {
    config <- super$get_config()
    config %<>% modifyList(list(
      "first_layer" = self$first_layer,
      "second_layer" = self$second_layer,
    ))
  }
  
  from_config <- py_eval("classmethod")(function(cls, config) {
    # Note that you can also use `keras.saving.deserialize_keras_object` here
    config$first_layer <-
      keras$layers$deserialize(config$first_layer)
    config$second_layer <-
      keras$layers$deserialize(config$second_layer)
    cls(!!!config)
  })
  
  call <- function(inputs) {
    inputs %>%
      self$first_layer() %>%
      self$second_layer()
  }
}

CustomModel %<>% {
  keras$saving$register_keras_serializable(package = "ComplexModels")(.)
}

# Let's make our first layer the custom layer from the previous example (MyDense)
inputs <- layer_input(32)
outputs <- CustomModel(first_layer = layer)(inputs)
model <- keras_model(inputs, outputs)

config <- model$get_config()
new_model <- keras$Model$from_config(config)
```

#### How custom objects are serialized

The serialization format has a special key for custom objects registered
via `keras$saving$register_keras_serializable`. This `registered_name`
key allows for easy retrieval at loading/deserialization time while also
allowing users to add custom naming.

Let's take a look at the config from serializing the custom layer
`MyDense` we defined above.

**Example**:

```{r}
layer <- MyDense(
    units=16,
    kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
    kernel_initializer="ones",
)
config <- keras$layers$serialize(layer)
print(config)
```

As shown, the `registered_name` key contains the lookup information for
the Keras master list, including the package `MyLayers` and the custom
name `KernelMult` that we gave in the
`keras$saving$register_keras_serializable` decorator. Take a look again
at the custom class definition/registration
[here](#registration_example).

Note that the `class_name` key contains the original name of the class,
allowing for proper re-initialization in `from_config`.

Additionally, note that the `module` key is `None` since this is a
custom object.
