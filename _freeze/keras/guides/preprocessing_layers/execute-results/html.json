{
  "hash": "811833996f83e3c2fc612b729d5d3c2b",
  "result": {
    "markdown": "---\ntitle: Working with preprocessing layers\nauthor: Francois Chollet, Mark Omernick, Tomasz Kalinowski\ndescription: Overview of how to leverage preprocessing layers to create end-to-end models.\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n## Keras preprocessing\n\nThe Keras preprocessing layers API allows developers to build\nKeras-native input processing pipelines. These input processing\npipelines can be used as independent preprocessing code in non-Keras\nworkflows, combined directly with Keras models, and exported as part of\na Keras SavedModel.\n\nWith Keras preprocessing layers, you can build and export models that\nare truly end-to-end: models that accept raw images or raw structured\ndata as input; models that handle feature normalization or feature value\nindexing on their own.\n\n## Available preprocessing layers\n\n### Text preprocessing\n\n-   `layer_text_vectorization()`: turns raw strings into an encoded\n    representation that can be read by a `layer_embedding()` or\n    `layer_dense()` layer.\n\n### Numerical features preprocessing\n\n-   `layer_normalization()`: performs feature-wise normalization of\n    input features.\n-   `layer_discretization()`: turns continuous numerical features into\n    integer categorical features.\n\n### Categorical features preprocessing\n\n-   `layer_category_encoding()`: turns integer categorical features into\n    one-hot, multi-hot, or count-based, dense representations.\n-   `layer_hashing()`: performs categorical feature hashing, also known\n    as the \"hashing trick\".\n-   `layer_string_lookup()`: turns string categorical values into an\n    encoded representation that can be read by an `Embedding` layer or\n    `Dense` layer.\n-   `layer_integer_lookup()`: turns integer categorical values into an\n    encoded representation that can be read by an `Embedding` layer or\n    `Dense` layer.\n\n### Image preprocessing\n\nThese layers are for standardizing the inputs of an image model.\n\n-   `layer_resizing()`: resizes a batch of images to a target size.\n-   `layer_rescaling()`: rescales and offsets the values of a batch of\n    images (e.g., going from inputs in the `[0, 255]` range to inputs in\n    the `[0, 1]` range.\n-   `layer_center_crop()`: returns a center crop of a batch of images.\n\n### Image data augmentation\n\nThese layers apply random augmentation transforms to a batch of images.\nThey are only active during training.\n\n-   `layer_random_crop()`\n-   `layer_random_flip()`\n-   `layer_random_flip()`\n-   `layer_random_translation()`\n-   `layer_random_rotation()`\n-   `layer_random_zoom()`\n-   `layer_random_height()`\n-   `layer_random_width()`\n-   `layer_random_contrast()`\n\n## The `adapt()` function\n\nSome preprocessing layers have an internal state that can be computed\nbased on a sample of the training data. The list of stateful\npreprocessing layers is:\n\n-   `layer_text_vectorization()`: holds a mapping between string tokens\n    and integer indices\n-   `layer_string_lookup()` and `layer_integer_lookup()`: hold a mapping\n    between input values and integer indices.\n-   `layer_normalization()`: holds the mean and standard deviation of\n    the features.\n-   `layer_discretization()`: holds information about value bucket\n    boundaries.\n\nCrucially, these layers are **non-trainable**. Their state is not set\nduring training; it must be set **before training**, either by\ninitializing them from a precomputed constant, or by \"adapting\" them on\ndata.\n\nYou set the state of a preprocessing layer by exposing it to training\ndata, via `adapt()`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n\n```{.r .cell-code}\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\"Features mean: %.2f\", mean(normalized_data))\n```\n\n::: {.cell-output-stdout}\n```\n[1] \"Features mean: -0.00\"\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"Features std: %.2f\", sd(normalized_data))\n```\n\n::: {.cell-output-stdout}\n```\n[1] \"Features std: 1.06\"\n```\n:::\n:::\n\n`adapt()` takes either an array or a `tf_dataset`. In the case of\n`layer_string_lookup()` and `layer_text_vectorization()`, you can also\npass a character vector:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(\n  \"Congratulations!\",\n  \"Today is your day.\",\n  \"You're off to Great Places!\",\n  \"You're off and away!\",\n  \"You have brains in your head.\",\n  \"You have feet in your shoes.\",\n  \"You can steer yourself\",\n  \"any direction you choose.\",\n  \"You're on your own. And you know what you know.\",\n  \"And YOU are the one who'll decide where to go.\"\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[31  0  0  0  0  0  0  0  0  0]\n [15 23  3 30  0  0  0  0  0  0]\n [ 4  7  6 25 19  0  0  0  0  0]\n [ 4  7  5 35  0  0  0  0  0  0]\n [ 2 10 34  9  3 24  0  0  0  0]\n [ 2 10 27  9  3 18  0  0  0  0]\n [ 2 33 17 11  0  0  0  0  0  0]\n [37 28  2 32  0  0  0  0  0  0]\n [ 4 22  3 20  5  2  8 14  2  8]\n [ 5  2 36 16 21 12 29 13  6 26]], shape=(10, 10), dtype=int64)\n```\n:::\n:::\n\nIn addition, adaptable layers always expose an option to directly set\nstate via constructor arguments or weight assignment. If the intended\nstate values are known at layer construction time, or are calculated\noutside of the `adapt()` call, they can be set without relying on the\nlayer's internal computation. For instance, if external vocabulary files\nfor the `layer_text_vectorization()`, `layer_string_lookup()`, or\n`layer_integer_lookup()` layers already exist, those can be loaded\ndirectly into the lookup tables by passing a path to the vocabulary file\nin the layer's constructor arguments.\n\nHere's an example where we instantiate a `layer_string_lookup()` layer\nwith precomputed vocabulary:\n\n::: {.cell}\n\n```{.r .cell-code}\nvocab <- c(\"a\", \"b\", \"c\", \"d\")\ndata <- as_tensor(rbind(c(\"a\", \"c\", \"d\"),\n                        c(\"d\", \"z\", \"b\")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[1 3 4]\n [4 0 2]], shape=(2, 3), dtype=int64)\n```\n:::\n:::\n\n## Preprocessing data before the model or inside the model\n\nThere are two ways you could be using preprocessing layers:\n\n**Option 1:** Make them part of the model, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n```\n:::\n\nWith this option, preprocessing will happen on device, synchronously\nwith the rest of the model execution, meaning that it will benefit from\nGPU acceleration. If you're training on GPU, this is the best option for\nthe `layer_normalization()` layer, and for all image preprocessing and\ndata augmentation layers.\n\n**Option 2:** apply it to your `tf_dataset`, so as to obtain a dataset\nthat yields batches of preprocessed data, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n```\n:::\n\nWith this option, your preprocessing will happen on CPU, asynchronously,\nand will be buffered before going into the model. In addition, if you\ncall `tfdatasets::dataset_prefetch()` on your dataset, the preprocessing\nwill happen efficiently in parallel with training:\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n```\n:::\n\nThis is the best option for `layer_text_vectorization()`, and all\nstructured data preprocessing layers. It can also be a good option if\nyou're training on CPU and you use image preprocessing layers.\n\n## Benefits of doing preprocessing inside the model at inference time\n\nEven if you go with option 2, you may later want to export an\ninference-only end-to-end model that will include the preprocessing\nlayers. The key benefit to doing this is that **it makes your model\nportable** and it **helps reduce the [training/serving\nskew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)**.\n\nWhen all data preprocessing is part of the model, other people can load\nand use your model without having to be aware of how each feature is\nexpected to be encoded & normalized. Your inference model will be able\nto process raw images or raw structured data, and will not require users\nof the model to be aware of the details of e.g. the tokenization scheme\nused for text, the indexing scheme used for categorical features,\nwhether image pixel values are normalized to `[-1, +1]` or to `[0, 1]`,\netc. This is especially powerful if you're exporting your model to\nanother runtime, such as TensorFlow.js: you won't have to reimplement\nyour preprocessing pipeline in JavaScript.\n\nIf you initially put your preprocessing layers in your `tf_dataset`\npipeline, you can export an inference model that packages the\npreprocessing. Simply instantiate a new model that chains your\npreprocessing layers and your training model:\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)\n```\n:::\n\n## Preprocessing during multi-worker training\n\nPreprocessing layers are compatible with the\n[tf.distribute](https://www.tensorflow.org/api_docs/python/tf/distribute)\nAPI for running training across multiple machines.\n\nIn general, preprocessing layers should be placed inside a\n`strategy$scope()` and called either inside or before the model as\ndiscussed above.\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n```\n:::\n\nFor more details, refer to the [preprocessing\nsection](https://www.tensorflow.org/tutorials/distribute/input#data_preprocessing)\nof the distributed input guide.\n\n## Quick recipes\n\n### Image data augmentation\n\nNote that image data augmentation layers are only active during training\n(similar to the `layer_dropout()` layer).\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n```\n:::\n\nYou can see a similar setup in action in the example [image\nclassification from\nscratch](https://keras.io/examples/vision/image_classification_from_scratch/).\n\n### Normalizing numerical features\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"adam\",\n          loss = \"sparse_categorical_crossentropy\")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n```\n:::\n\n### Encoding string categorical features via one-hot encoding\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some toy data\ndata <- as_tensor(c(\"a\", \"b\", \"c\", \"b\", \"c\", \"a\")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\"a\", \"b\", \"c\", \"d\", \"e\", \"\")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n```\n:::\n:::\n\nNote that, here, index 0 is reserved for out-of-vocabulary values\n(values that were not seen during `adapt()`).\n\nYou can see the `layer_string_lookup()` in action in the [Structured\ndata classification from\nscratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\nexample.\n\n### Encoding integer categorical features via one-hot encoding\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \"int32\")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\"one_hot\")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \"int32\")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n```\n:::\n:::\n\nNote that index 0 is reserved for missing values (which you should\nspecify as the value 0), and index 1 is reserved for out-of-vocabulary\nvalues (values that were not seen during `adapt()`). You can configure\nthis by using the `mask_token` and `oov_token` constructor arguments of\n`layer_integer_lookup()`.\n\nYou can see the `layer_integer_lookup()` in action in the example\n[structured data classification from\nscratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/).\n\n### Applying the hashing trick to an integer categorical feature\n\nIf you have a categorical feature that can take many different values\n(on the order of 10e3 or higher), where each value only appears a few\ntimes in the data, it becomes impractical and ineffective to index and\none-hot encode the feature values. Instead, it can be a good idea to\napply the \"hashing trick\": hash the values to a vector of fixed size.\nThis keeps the size of the feature space manageable, and removes the\nneed for explicit indexing.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \"int64\")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\"multi_hot\")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n```\n\n::: {.cell-output-stdout}\n```\nTensorShape([10000, 64])\n```\n:::\n:::\n\n### Encoding text as a sequence of token indices\n\nThis is how you should preprocess text to be passed to an `Embedding`\nlayer.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\"int\")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\",\n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n```\n\n::: {.cell-output-stdout}\n```\nEncoded text:\n 2 19 14 1 9 2 1\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\"int64\")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer = \"rmsprop\", loss = \"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output:\", as.array(test_output), \"\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nModel output: 0.1437887 \n```\n:::\n:::\n\nYou can see the `layer_text_vectorization()` layer in action, combined\nwith an `Embedding` mode, in the example [text classification from\nscratch](https://keras.io/examples/nlp/text_classification_from_scratch/).\n\nNote that when training such a model, for best performance, you should\nalways use the `layer_text_vectorization()` layer as part of the input\npipeline.\n\n### Encoding text as a dense matrix of ngrams with multi-hot encoding\n\nThis is how you can preprocess text to be passed to a `Dense` layer.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"multi_hot\" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"multi_hot\", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n```\n\n::: {.cell-output-stdout}\n```\nEncoded text:\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\"Training model...\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output); cat(\"\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nModel output: \n```\n:::\n\n::: {.cell-output-stdout}\n```\ntf.Tensor([[0.31572872]], shape=(1, 1), dtype=float32)\n```\n:::\n:::\n\n### Encoding text as a dense matrix of ngrams with TF-IDF weighting\n\nThis is an alternative way of preprocessing text before passing it to a\n`layer_dense` layer.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \"The Brain is wider than the Sky\",\n  \"For put them side by side\",\n  \"The one the other will contain\",\n  \"With ease and You beside\"\n))\n\n# Instantiate layer_text_vectorization() with \"tf-idf\" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\"tf-idf\", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\"CPU\"), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\"Encoded text:\\n\", \n    as.array(text_vectorizer(\"The Brain is deeper than the sea\")))\n```\n\n::: {.cell-output-stdout}\n```\nEncoded text:\n 5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\"int64\")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\"Training model...\")\n```\n\n::: {.cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer=\"rmsprop\", loss=\"mse\") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\"Calling end-to-end model on test string...\\n\")\n```\n\n::: {.cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\"The one the other will absorb\"))\ntest_output <- end_to_end_model(test_data)\ncat(\"Model output: \"); print(test_output)\n```\n\n::: {.cell-output-stdout}\n```\nModel output: \n```\n:::\n\n::: {.cell-output-stdout}\n```\ntf.Tensor([[0.09238248]], shape=(1, 1), dtype=float32)\n```\n:::\n:::\n\n## Important gotchas\n\n### Working with lookup layers with very large vocabularies\n\nYou may find yourself working with a very large vocabulary in a\n`layer_text_vectorization()`, a `layer_string_lookup()` layer, or an\n`layer_integer_lookup()` layer. Typically, a vocabulary larger than\n500MB would be considered \"very large\".\n\nIn such case, for best performance, you should avoid using `adapt()`.\nInstead, pre-compute your vocabulary in advance (you could use Apache\nBeam or TF Transform for this) and store it in a file. Then load the\nvocabulary into the layer at construction time by passing the filepath\nas the `vocabulary` argument.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}