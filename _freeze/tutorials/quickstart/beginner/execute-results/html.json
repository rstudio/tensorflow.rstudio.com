{
  "hash": "56ae4e2765192297c49497783e2b003a",
  "result": {
    "markdown": "---\ntitle: Beginner\ndescription: 'This \"Hello, World!\" shows the Keras Sequential API and `fit()`.'\nid: 1\naliases:\n  - ../beginners/index.html\n  - ../../guide/keras/index.html\n  - ../../guide/keras/guide_keras/index.html\n---\n\n\n# TensorFlow 2 quickstart for beginners\n\nThis short introduction uses [Keras](https://www.tensorflow.org/guide/keras/overview) to:\n\n1.  Load a prebuilt dataset.\n2.  Build a neural network machine learning model that classifies images.\n3.  Train this neural network.\n4.  Evaluate the accuracy of the model.\n\n## Set up TensorFlow\n\nImport TensorFlow into your program to get started:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\nSee the [installation guide](/install/) to learn how to correctly install TensorFlow for R.\n\n## Load a dataset\n\nLoad and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\nConvert the sample data from integers to floating-point numbers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\nx_train <- x_train / 255\nx_test <-  x_test / 255\n```\n:::\n\n\n## Build a machine learning model\n\nBuild a sequential model by stacking layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(28, 28)) %>%\n  layer_flatten() %>%\n  layer_dense(128, activation = \"relu\") %>%\n  layer_dropout(0.2) %>%\n  layer_dense(10)\n```\n:::\n\n\nFor each example, the model returns a vector of [logits](https://developers$google.com/machine-learning/glossary#logits) or [log-odds](https://developers$google.com/machine-learning/glossary#log-odds) scores, one for each class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(model, x_train[1:2, , ])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1/1 - 0s - 73ms/epoch - 73ms/step\n```\n:::\n\n```{.r .cell-code}\npredictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]       [,2]        [,3]       [,4]      [,5]       [,6]\n[1,]  0.1651525 0.01435302  0.06943683 -0.6274899 0.3188712 0.51209235\n[2,] -0.5096894 0.11335309 -0.02402511 -0.8324738 0.2125668 0.07842563\n            [,7]      [,8]      [,9]      [,10]\n[1,] -0.01122756 0.3119570 0.9944065 -0.2704043\n[2,]  0.07199600 0.4042611 0.8706847 -0.6965256\n```\n:::\n:::\n\n\nThe `tf$nn$softmax` function converts these logits to *probabilities* for each class:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$nn$softmax(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0.09313394 0.08009707 0.0846329  0.04215682 0.10860934 0.13175954\n  0.07807412 0.10786099 0.21342654 0.06024875]\n [0.05505754 0.10265987 0.0894825  0.03986881 0.11336753 0.09913611\n  0.09850075 0.13732209 0.21893019 0.04567461]], shape=(2, 10), dtype=float64)\n```\n:::\n:::\n\n\nNote: It is possible to bake the `tf$nn$softmax` function into the activation function for the last layer of the network.\nWhile this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output.\n\nDefine a loss function for training using `loss_sparse_categorical_crossentropy()`, which takes a vector of logits and an integer index of which are `TRUE` and returns a scalar loss for each example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n```\n:::\n\n\nThis loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\nThis untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-log(1/10) ~= 2.3`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_fn(y_train[1:2], predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(2.4630766052891806, shape=(), dtype=float64)\n```\n:::\n:::\n\n\nBefore you start training, configure and compile the model using Keras `compile()`.\nSet the [`optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) class to `\"adam\"`, set the `loss` to the `loss_fn` function you defined earlier, and specify a metric to be evaluated for the model by setting the `metrics` parameter to `\"accuracy\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = loss_fn,\n  metrics = \"accuracy\"\n)\n```\n:::\n\n\n## Train and evaluate your model\n\nUse the `fit()` method to adjust your model parameters and minimize the loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% fit(x_train, y_train, epochs = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n1875/1875 - 3s - loss: 0.2946 - accuracy: 0.9149 - 3s/epoch - 2ms/step\nEpoch 2/5\n1875/1875 - 3s - loss: 0.1414 - accuracy: 0.9582 - 3s/epoch - 1ms/step\nEpoch 3/5\n1875/1875 - 3s - loss: 0.1054 - accuracy: 0.9676 - 3s/epoch - 1ms/step\nEpoch 4/5\n1875/1875 - 3s - loss: 0.0824 - accuracy: 0.9746 - 3s/epoch - 1ms/step\nEpoch 5/5\n1875/1875 - 3s - loss: 0.0711 - accuracy: 0.9775 - 3s/epoch - 1ms/step\n```\n:::\n:::\n\n\nThe `evaluate()` method checks the models performance, usually on a [Validation-set](https://developers$google.com/machine-learning/glossary#validation-set) or [Test-set](https://developers$google.com/machine-learning/glossary#test-set).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(x_test,  y_test, verbose = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.0712 - accuracy: 0.9783 - 360ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n      loss   accuracy \n0.07116836 0.97829998 \n```\n:::\n:::\n\n\nThe image classifier is now trained to \\~98% accuracy on this dataset.\nTo learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\nIf you want your model to return which class had the highest probability, you can reuse the trained model to define a new sequential model that also calls softmax and argmax:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobability_model <- keras_model_sequential() %>%\n  model() %>%\n  layer_activation_softmax() %>%\n  layer_lambda(tf$argmax)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobability_model(x_test[1:5, , ])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([3 2 1 0 4 3 2 0 2 4], shape=(10), dtype=int64)\n```\n:::\n:::\n\n\n## Conclusion\n\nCongratulations!\nYou have trained a machine learning model using a prebuilt dataset using the [Keras](https://www.tensorflow.org/guide/keras/overview) API.\n\nFor more examples of using Keras, check out the [tutorials](https://www.tensorflow.org/tutorials/keras/).\nTo learn more about building models with Keras, read the [guides](https://www.tensorflow.org/guide/keras).\nIf you want learn more about loading and preparing data, see the tutorials on [image data loading](https://www.tensorflow.org/tutorials/load_data/images) or [CSV data loading](https://www.tensorflow.org/tutorials/load_data/csv).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}