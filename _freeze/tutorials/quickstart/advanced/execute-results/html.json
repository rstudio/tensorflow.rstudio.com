{
  "hash": "42843e7dd211b2c7de8dd7e3e595452e",
  "result": {
    "markdown": "---\ntitle: Advanced\ndescription: 'This \"Hello, World!\" uses the Keras subclassing API and a custom training loop.'\naliases:\n  - ../avanced/index.html\n---\n\n\n# TensorFlow 2 quickstart for experts\n\nThe Keras functional and subclassing APIs provide a define-by-run interface for customization and advanced research.\nBuild your model, then write the forward and backward pass.\nCreate custom layers, activations, and training loops.\n\nImport TensorFlow into your program.\nIf you haven't installed TensorFlow yet, go to the [installation guide](/install/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n```\n:::\n\n\nLoad and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\nx_train %<>% { . / 255 }\nx_test  %<>% { . / 255 }\n```\n:::\n\n\nUse TensorFlow Datasets to batch and shuffle the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ds <- list(x_train, y_train) %>%\n  tensor_slices_dataset() %>%\n  dataset_shuffle(10000) %>%\n  dataset_batch(32)\n\ntest_ds <- list(x_test, y_test) %>%\n  tensor_slices_dataset() %>%\n  dataset_batch(32)\n```\n:::\n\n\nBuild the a model using the Keras [model subclassing API](https://www.tensorflow.org/guide/keras#model_subclassing):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model <- new_model_class(\n  classname = \"MyModel\",\n  initialize = function(...) {\n    super$initialize()\n    self$conv1 <- layer_conv_2d(filters = 32, kernel_size = 3,\n                                activation = 'relu')\n    self$flatten <- layer_flatten()\n    self$d1 <- layer_dense(units = 128, activation = 'relu')\n    self$d2 <- layer_dense(units = 10)\n  },\n  call = function(inputs) {\n    inputs %>%\n      tf$expand_dims(3L) %>%\n      self$conv1() %>%\n      self$flatten() %>%\n      self$d1() %>%\n      self$d2()\n  }\n)\n\n# Create an instance of the model\nmodel <- my_model()\n```\n:::\n\n\nChoose an optimizer and loss function for training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_object <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n```\n:::\n\n\nSelect metrics to measure the loss and the accuracy of the model.\nThese metrics accumulate the values over epochs and then print the overall result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_loss <- metric_mean(name = \"train_loss\")\ntrain_accuracy <- metric_sparse_categorical_accuracy(name = \"train_accuracy\")\n\ntest_loss <- metric_mean(name = \"test_loss\")\ntest_accuracy <- metric_sparse_categorical_accuracy(name = \"test_accuracy\")\n```\n:::\n\n\nUse `tf$GradientTape()` to train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_step <- function(images, labels) {\n  with(tf$GradientTape() %as% tape, {\n    # training = TRUE is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions <- model(images, training = TRUE)\n    loss <- loss_object(labels, predictions)\n  })\n  gradients <- tape$gradient(loss, model$trainable_variables)\n  optimizer$apply_gradients(zip_lists(gradients, model$trainable_variables))\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n}\n\ntrain <- tf_function(function(train_ds) {\n  for (batch in train_ds) {\n    c(images, labels) %<-% batch\n    train_step(images, labels)\n  }\n})\n```\n:::\n\n\nTest the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_step <- function(images, labels) {\n  # training = FALSE is only needed if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  predictions <- model(images, training = FALSE)\n  t_loss <- loss_object(labels, predictions)\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n}\n\ntest <- tf_function(function(test_ds) {\n  for (batch in test_ds) {\n    c(images, labels) %<-% batch\n    test_step(images, labels)\n  }\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreset_metrics <- function() {\n  for (metric in list(train_loss, train_accuracy,\n                       test_loss, test_accuracy))\n    metric$reset_states()\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nEPOCHS <- 1\nfor (epoch in seq_len(EPOCHS)) {\n  # Reset the metrics at the start of the next epoch\n  reset_metrics()\n  train(train_ds)\n  test(test_ds)\n  cat(sprintf('Epoch %d', epoch), \"\\n\")\n  cat(sprintf('Loss: %f', train_loss$result()), \"\\n\")\n  cat(sprintf('Accuracy: %f', train_accuracy$result() * 100), \"\\n\")\n  cat(sprintf('Test Loss: %f', test_loss$result()), \"\\n\")\n  cat(sprintf('Test Accuracy: %f', test_accuracy$result() * 100), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1 \nLoss: 0.148424 \nAccuracy: 95.521667 \nTest Loss: 0.068080 \nTest Accuracy: 97.739998 \n```\n:::\n:::\n\n\nThe image classifier is now trained to \\~98% accuracy on this dataset.\nTo learn more, read the [TensorFlow tutorials](/tutorials/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}