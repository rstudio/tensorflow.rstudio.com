{
  "hash": "877eb0a6351e318ab8b0d270ce5af60c",
  "result": {
    "markdown": "---\ntitle: Advanced\ndescription: \"This 'Hello, World!' notebook uses the Keras subclassing API and a custom training loop.\"\naliases:\n  - ../avanced/index.html\n---\n\n\n# TensorFlow 2 quickstart for experts\n\nImport TensorFlow into your program.\nIf you haven't installed TensorFlow yet, go to the [installation guide](/install/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n```\n:::\n\n\nLoad and prepare the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nx_train %<>% { . / 255 }\nx_test  %<>% { . / 255 }\n```\n:::\n\n\nUse TensorFlow Datasets to batch and shuffle the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ds <- list(x_train, y_train) %>%\n  tensor_slices_dataset() %>%\n  dataset_shuffle(10000) %>%\n  dataset_batch(32)\n\ntest_ds <- list(x_test, y_test) %>%\n  tensor_slices_dataset() %>%\n  dataset_batch(32)\n```\n:::\n\n\nBuild the a model using the Keras [model subclassing API](https://www.tensorflow.org/guide/keras#model_subclassing):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model <- new_model_class(\n  classname = \"MyModel\",\n  initialize = function(...) {\n    super$initialize()\n    self$conv1 <- layer_conv_2d(filters = 32, kernel_size = 3,\n                                activation = 'relu')\n    self$flatten <- layer_flatten()\n    self$d1 <- layer_dense(units = 128, activation = 'relu')\n    self$d2 <- layer_dense(units = 10)\n  },\n  call = function(inputs) {\n    inputs %>%\n      tf$expand_dims(3L) %>%\n      self$conv1() %>%\n      self$flatten() %>%\n      self$d1() %>%\n      self$d2()\n  }\n)\n\n# Create an instance of the model\nmodel <- my_model()\n```\n:::\n\n\nChoose an optimizer and loss function for training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_object <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n```\n:::\n\n\nSelect metrics to measure the loss and the accuracy of the model.\nThese metrics accumulate the values over epochs and then print the overall result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_loss <- metric_mean(name = \"train_loss\")\ntrain_accuracy <- metric_sparse_categorical_accuracy(name = \"train_accuracy\")\n\ntest_loss <- metric_mean(name = \"test_loss\")\ntest_accuracy <- metric_sparse_categorical_accuracy(name = \"test_accuracy\")\n```\n:::\n\n\nUse `tf$GradientTape()` to train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_step <- function(images, labels) {\n  with(tf$GradientTape() %as% tape, {\n    # training = TRUE is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions <- model(images, training = TRUE)\n    loss <- loss_object(labels, predictions)\n  })\n  gradients <- tape$gradient(loss, model$trainable_variables)\n  optimizer$apply_gradients(zip_lists(gradients, model$trainable_variables))\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n}\n\ntrain <- tf_function(function(train_ds) {\n  for (batch in train_ds) {\n    c(images, labels) %<-% batch\n    train_step(images, labels)\n  }\n})\n```\n:::\n\n\nTest the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_step <- function(images, labels) {\n  # training = FALSE is only needed if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  predictions <- model(images, training = FALSE)\n  t_loss <- loss_object(labels, predictions)\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n}\n\ntest <- tf_function(function(test_ds) {\n  for (batch in test_ds) {\n    c(images, labels) %<-% batch\n    test_step(images, labels)\n  }\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreset_metrics <- function() {\n  for (metric in list(train_loss, train_accuracy,\n                       test_loss, test_accuracy))\n    metric$reset_states()\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nEPOCHS <- 1\nfor (epoch in seq_len(EPOCHS)) {\n  # Reset the metrics at the start of the next epoch\n  reset_metrics()\n  train(train_ds)\n  test(test_ds)\n  cat(sprintf('Epoch %d', epoch), \"\\n\")\n  cat(sprintf('Loss: %f', train_loss$result()), \"\\n\")\n  cat(sprintf('Accuracy: %f', train_accuracy$result() * 100), \"\\n\")\n  cat(sprintf('Test Loss: %f', test_loss$result()), \"\\n\")\n  cat(sprintf('Test Accuracy: %f', test_accuracy$result() * 100), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1 \nLoss: 0.136656 \nAccuracy: 95.845001 \nTest Loss: 0.063304 \nTest Accuracy: 97.949997 \n```\n:::\n:::\n\n\nThe image classifier is now trained to \\~98% accuracy on this dataset.\nTo learn more, read the [TensorFlow tutorials](/tutorials/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}