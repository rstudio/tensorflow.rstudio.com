{
  "hash": "e1e93a6e5b1d469898cac2a9a7385268",
  "result": {
    "markdown": "---\ntitle: Build TensorFlow input pipelines\naliases:\n  - ../../tutorials/beginners/load/load_csv/index.html\n  - ../../tutorials/beginners/load/load_image/index.html\n  - ../../guide/tfdatasets/introduction/index.html\n  - /tools/tfdatasets/\n---\n\n\nThe tfdatasets API enables you to build complex input pipelines from simple,\nreusable pieces. For example, the pipeline for an image model might aggregate\ndata from files in a distributed file system, apply random perturbations to each\nimage, and merge randomly selected images into a batch for training. The\npipeline for a text model might involve extracting symbols from raw text data,\nconverting them to embedding identifiers with a lookup table, and batching\ntogether sequences of different lengths. The `tf$data` API makes it possible to\nhandle large amounts of data, read from different data formats, and perform\ncomplex transformations.\n\nThe tfdatasets API introduces a TensorFlow Dataset abstraction that represents a\nsequence of elements, in which each element consists of one or more components.\nFor example, in an image pipeline, an element might be a single training\nexample, with a pair of tensor components representing the image and its label.\n\nThere are two distinct ways to create a dataset:\n\n*   A data **source** constructs a `Dataset` from data stored in memory or in\n    one or more files.\n*   A data **transformation** constructs a dataset from one or more\n    `Dataset` objects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n```\n:::\n\n\n## Basic mechanics\n\nTo create an input pipeline, you must start with a data *source*. For example,\nto construct a `Dataset` from data in memory, you can use\n`tensors_dataset()` or `tensor_slices_dataset()`.\nAlternatively, if your input data is stored in a file in the recommended\nTFRecord format, you can use `tfrecord_dataset()`.\n\nOnce you have a `Dataset` object, you can *transform* it into a new `Dataset` by\nchaining method calls on the `Dataset` object. For example, you can\napply per-element transformations such as `dataset_map()`, and multi-element\ntransformations such as `dataset_batch()`. See the documentation for\n`tfdatasets` for a complete list of transformations.\n\nThe `Dataset` object is a iterable. This makes it possible to consume its\nelements using a the `coro::loop`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- tensor_slices_dataset(c(8, 3, 0, 8, 2, 1))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.float32, name=None)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoro::loop(for(elem in dataset) {\n  print(as.numeric(elem))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n[1] 3\n[1] 0\n[1] 8\n[1] 2\n[1] 1\n```\n:::\n:::\n\n\nOr by explicitly creating a Python iterator using `iter` and consuming its\nelements using `next`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nit <- reticulate::as_iterator(dataset)\nprint(reticulate::iter_next(it))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(8.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nAlternatively, dataset elements can be consumed using the `reduce`\ntransformation, which reduces all elements to produce a single result. The\nfollowing example illustrates how to use the `reduce` transformation to compute\nthe sum of a dataset of integers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  dataset_reduce(0, function(state, value) state + value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(22.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\n### Dataset structure\n\nA dataset produces a sequence of *elements*, where each element is\nthe same (nested) structure of *components*. Individual components\nof the structure can be of any type representable by\n`tf$TypeSpec`, including `tf$Tensor`, `tf$sparse$SparseTensor`,\n`tf$RaggedTensor`, `tf$TensorArray`, or `tf$data$Dataset`.\n\nThe constructs that can be used to express the (nested)\nstructure of elements include `tuple`, `dict`, `NamedTuple`, and\n`OrderedDict`. In particular, `list` is not a valid construct for\nexpressing the structure of dataset elements. This is because\nearly tfdataset users felt strongly about `list` inputs (e.g. passed\nto `tensors_dataset()`) being automatically packed as\ntensors and `list` outputs (e.g. return values of user-defined\nfunctions) being coerced into a `tuple`. As a consequence, if you\nwould like a `list` input to be treated as a structure, you need\nto convert it into `tuple` and if you would like a `list` output\nto be a single component, then you need to explicitly pack it\nusing `tf$stack`.\n\nThe `dataset_element_spec` property allows you to inspect the type\nof each element component. The property returns a *nested structure*\nof `tf$TypeSpec` objects, matching the structure of the element,\nwhich may be a single component, a tuple of components, or a nested\ntuple of components. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset1 <- tensor_slices_dataset(tf$random$uniform(shape(4, 10)))\ndataset1$element_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset2 <- tensor_slices_dataset(reticulate::tuple(\n  tf$random$uniform(shape(4)),\n  tf$random$uniform(shape(4, 100), maxval = 100L, dtype = tf$int32)\n))\n\ndataset2$element_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nTensorSpec(shape=(), dtype=tf.float32, name=None)\n\n[[2]]\nTensorSpec(shape=(100,), dtype=tf.int32, name=None)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset3 <- zip_datasets(dataset1, dataset2)\ndataset3$element_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n\n[[2]]\n[[2]][[1]]\nTensorSpec(shape=(), dtype=tf.float32, name=None)\n\n[[2]][[2]]\nTensorSpec(shape=(100,), dtype=tf.int32, name=None)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dataset containing a sparse tensor.\nsparse <- tf$SparseTensor(\n  indices = list(c(0L,0L), c(1L,2L)), \n  values = c(1,2),\n  dense_shape = shape(3,4)\n)\ndataset4 <- tensors_dataset(sparse)\ndataset4$element_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSparseTensorSpec(TensorShape([3, 4]), tf.float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use value_type to see the type of value represented by the element spec\ndataset4$element_spec$value_type\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n```\n:::\n:::\n\n\nThe `Dataset` transformations support datasets of any structure. When using the\n`dataset_map()`, and `dataset_filter()` transformations,\nwhich apply a function to each element, the element structure determines the\narguments of the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset1 <- tensor_slices_dataset(\n    tf$random$uniform(shape(4, 10), minval = 1L, maxval = 10L, dtype = tf$int32))\ndataset1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TensorSliceDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int32, name=None)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset1 %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1 7 8 2 3 7 7 9 3 6], shape=(10), dtype=int32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset2 <- tensor_slices_dataset(reticulate::tuple(\n  tf$random$uniform(shape(4)),\n  tf$random$uniform(shape(4, 100), maxval = 100L, dtype = tf$int32)\n))\ndataset2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset3 <- zip_datasets(dataset1, dataset2)\ndataset3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ZipDataset element_spec=(TensorSpec(shape=(10,), dtype=tf.int32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset3 %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ :<tf.Tensor: shape=(10), dtype=int32, numpy=array([1, 7, 8, 2, 3, 7, 7, 9, 3, 6], dtype=int32)>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(), dtype=float32, numpy=0.9568937>\n  ..$ :<tf.Tensor: shape=(100), dtype=int32, numpy=…>\n```\n:::\n:::\n\n\n## Reading input data\n\n### Consuming R arrays\n\nSee [Loading NumPy arrays](../tutorials/load_data/numpy.qmd) for more examples.\n\nIf all of your input data fits in memory, the simplest way to create a `Dataset`\nfrom them is to convert them to `tf$Tensor` objects and use\n`tensor_slices_dataset()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(train, test) %<-% dataset_fashion_mnist()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain[[1]][] <- train[[1]]/255\ndataset <- tensor_slices_dataset(train)\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TensorSliceDataset element_spec={'x': TensorSpec(shape=(28, 28), dtype=tf.float64, name=None), 'y': TensorSpec(shape=(), dtype=tf.int32, name=None)}>\n```\n:::\n:::\n\n\nNote: The above code snippet will embed the `features` and `labels` arrays\nin your TensorFlow graph as `tf$constant()` operations. This works well for a\nsmall dataset, but wastes memory---because the contents of the array will be\ncopied multiple times---and can run into the 2GB limit for the `tf$GraphDef`\nprotocol buffer.\n\n<!-- ### Consuming Python generators -->\n\n<!-- Another common data source that can easily be ingested as a `tf$data$Dataset` is the python generator. -->\n\n<!-- Caution: While this is a convienient approach it has limited portability and scalibility. It must run in the same python process that created the generator, and is still subject to the Python [GIL](https://en.wikipedia.org/wiki/Global_interpreter_lock). -->\n\n<!-- ```{r} -->\n<!-- count <- function(stop) {    } -->\n<!--   i <- 0 -->\n<!--   while i<stop: -->\n<!--     yield i -->\n<!--     i += 1 -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- for n in count(5): -->\n<!--   print(n) -->\n<!-- ``` -->\n\n<!-- The `dataset_from_generator` constructor converts the python generator to a fully functional `tf$data$Dataset`. -->\n\n<!-- The constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional `args` argument, which is passed as the callable's arguments. -->\n\n<!-- The `output_types` argument is required because `tf$data` builds a `tf$Graph` internally, and graph edges require a `tf$dtype`. -->\n\n<!-- ```{r} -->\n<!-- ds_counter <- tf$data$dataset_from_generator(count, args = [25], output_types = tf$int32, output_shapes = (), ) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- for count_batch in ds_counter$repeat().batch(10).take(10): -->\n<!--   print(count_batch$numpy()) -->\n<!-- ``` -->\n\n<!-- The `output_shapes` argument is not *required* but is highly recommended as many TensorFlow operations do not support tensors with an unknown rank. If the length of a particular axis is unknown or variable, set it as `NULL` in the `output_shapes`. -->\n\n<!-- It's also important to note that the `output_shapes` and `output_types` follow the same nesting rules as other dataset methods. -->\n\n<!-- Here is an example generator that demonstrates both aspects, it returns tuples of arrays, where the second array is a vector with unknown length. -->\n\n<!-- ```{r} -->\n<!-- gen_series <- function() {    } -->\n<!--   i <- 0 -->\n<!--   while TRUE: -->\n<!--     size <- np$random$randint(0, 10) -->\n<!--     yield i, np$random$normal(size = (size,)) -->\n<!--     i += 1 -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- for i, series in gen_series(): -->\n<!--   print(i, \":\", str(series)) -->\n<!--   if i > 5: -->\n<!--     break -->\n<!-- ``` -->\n\n<!-- The first output is an `int32` the second is a `float32`. -->\n\n<!-- The first item is a scalar, shape `()`, and the second is a vector of unknown length, shape `(NULL,)`  -->\n\n<!-- ```{r} -->\n<!-- ds_series <- tf$data$dataset_from_generator( -->\n<!--     gen_series,  -->\n<!--     output_types <- (tf$int32, tf$float32),  -->\n<!--     output_shapes <- ((), (NULL,))) -->\n\n<!-- ds_series -->\n<!-- ``` -->\n\n<!-- Now it can be used like a regular `tf$data$Dataset`. Note that when batching a dataset with a variable shape, you need to use `dataset_padded_batch`. -->\n\n<!-- ```{r} -->\n<!-- ds_series_batch <- ds_series$shuffle(20).padded_batch(10) -->\n\n<!-- ids, sequence_batch = next(iter(ds_series_batch)) -->\n<!-- print(ids$numpy()) -->\n<!-- print() -->\n<!-- print(sequence_batch$numpy()) -->\n<!-- ``` -->\n\n<!-- For a more realistic example, try wrapping `preprocessing$image$ImageDataGenerator` as a `tf$data$Dataset`. -->\n\n<!-- First download the data: -->\n\n<!-- ```{r} -->\n<!-- flowers <- tf$keras$utils$get_file( -->\n<!--     'flower_photos', -->\n<!--     'https://storage$googleapis.com/download$tensorflow.org/example_images/flower_photos$tgz', -->\n<!--     untar <- TRUE) -->\n<!-- ``` -->\n\n<!-- Create the `image$ImageDataGenerator` -->\n\n<!-- ```{r} -->\n<!-- img_gen <- tf$keras$preprocessing$image$ImageDataGenerator(rescale = 1./255, rotation_range = 20) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- images, labels = next(img_gen$flow_from_directory(flowers)) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- print(images$dtype, images$shape) -->\n<!-- print(labels$dtype, labels$shape) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- ds <- tf$data$dataset_from_generator( -->\n<!--     lambda: img_gen$flow_from_directory(flowers),  -->\n<!--     output_types <- (tf$float32, tf$float32),  -->\n<!--     output_shapes <- ([32,256,256,3], [32,5]) -->\n<!-- ) -->\n\n<!-- ds$element_spec -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- for images, labels in ds$take(1): -->\n<!--   print('images$shape: ', images$shape) -->\n<!--   print('labels$shape: ', labels$shape) -->\n<!-- ``` -->\n\n### Consuming TFRecord data\n\nSee [Loading TFRecords](../tutorials/load_data/tfrecord.qmd) for an end-to-end example.\n\nThe tfdatasets API supports a variety of file formats so that you can process\nlarge datasets that do not fit in memory. For example, the TFRecord file format\nis a simple record-oriented binary format that many TensorFlow applications use\nfor training data. The `tfrecord_dataset()` class enables you to\nstream over the contents of one or more TFRecord files as part of an input\npipeline.\n\nHere is an example using the test file from the French Street Name Signs (FSNS).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates a dataset that reads all of the examples from two files.\nfsns_test_file <- get_file(\n  \"fsns.tfrec\", \n  \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\"\n)\n```\n:::\n\n\nThe `filenames` argument to the `tfrecord_dataset()` initializer can either be a\nstring, a list of strings, or a `tf$Tensor` of strings. Therefore if you have\ntwo sets of files for training and validation purposes, you can create a factory\nmethod that produces the dataset, taking filenames as an input argument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- tfrecord_dataset(filenames = list(fsns_test_file))\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n```\n:::\n:::\n\n\nMany TensorFlow projects use serialized `tf$train$Example` records in their TFRecord files. These need to be decoded before they can be inspected:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_example <- dataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\nparsed <- tf$train$Example$FromString(raw_example$numpy())\nparsed$features$feature['image/text']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbytes_list {\n  value: \"Rue Perreyon\"\n}\n```\n:::\n:::\n\n\n### Consuming text data\n\nSee [Loading Text](../tutorials/load_data/text.qmd) for an end to end example.\n\nMany datasets are distributed as one or more text files. The\n`text_line_dataset()` provides an easy way to extract lines from one or more\ntext files. Given one or more filenames, a `text_line_dataset()` will produce one\nstring-valued element per line of those files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndirectory_url <- 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\nfile_names <- c('cowper.txt', 'derby.txt', 'butler.txt')\n\nfile_paths <- lapply(file_names, function(file_name) {\n get_file(file_name, paste0(directory_url, file_name)) \n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- text_line_dataset(file_paths)\n```\n:::\n\n\nHere are the first few lines of the first file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  dataset_take(5) %>% \n  coro::collect(5) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'His wrath pernicious, who ten thousand woes'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"Caused to Achaia's host, sent many a soul\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'Illustrious into Ades premature,'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'And Heroes gave (so stood the will of Jove)'>\n```\n:::\n:::\n\n\nTo alternate lines between files use `dataset_interleave()`. This makes it easier to shuffle files together. Here are the first, second and third lines from each translation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles_ds <- tensor_slices_dataset(unlist(file_paths))\nlines_ds <- files_ds %>% \n  dataset_interleave(text_line_dataset, cycle_length = 3)\n\nlines_ds %>% \n  coro::collect(9) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 9\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'His wrath pernicious, who ten thousand woes'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'The vengeance, deep and deadly; whence to Greece'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'countless ills upon the Achaeans. Many a brave soul did it send'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b\"Caused to Achaia's host, sent many a soul\">\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'Unnumbered ills arose; which many a soul'>\n  [list output truncated]\n```\n:::\n:::\n\n\nBy default, a `text_line_dataset()` yields *every* line of each file, which may\nnot be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the `dataset_skip()` or\n`dataset_filter()` transformations. Here, you skip the first line, then filter to\nfind only survivors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_file <- get_file(\n  \"train.csv\", \n  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n)\ntitanic_lines <- text_line_dataset(titanic_file)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_lines %>% \n  coro::collect(10) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,35.0,1,0,53.1,First,C,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'>\n  [list output truncated]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvived <- function(line) {\n  tf$not_equal(tf$strings$substr(line, 0L, 1L), \"0\")\n}\n\nsurvivors <- titanic_lines %>% \n  dataset_skip(1) %>% \n  dataset_filter(survived)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvivors %>% \n  coro::collect(10) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,35.0,1,0,53.1,First,C,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'>\n  [list output truncated]\n```\n:::\n:::\n\n\n### Consuming CSV data\n\nSee [Loading CSV Files](../tutorials/load_data/csv.qmd), and [Loading Data Frames](../tutorials/load_data/pandas_dataframe.qmd) for more examples. \n\nThe CSV file format is a popular format for storing tabular data in plain text.\n\nFor example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_file <- get_file(\n  \"train.csv\", \n  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- readr::read_csv(titanic_file)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 627 Columns: 10\n── Column specification ────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): sex, class, deck, embark_town, alone\ndbl (5): survived, age, n_siblings_spouses, parch, fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 10\n  survived sex      age n_siblings_s…¹ parch  fare class deck  embar…² alone\n     <dbl> <chr>  <dbl>          <dbl> <dbl> <dbl> <chr> <chr> <chr>   <chr>\n1        0 male      22              1     0  7.25 Third unkn… Southa… n    \n2        1 female    38              1     0 71.3  First C     Cherbo… n    \n3        1 female    26              0     0  7.92 Third unkn… Southa… y    \n4        1 female    35              1     0 53.1  First C     Southa… n    \n5        0 male      28              0     0  8.46 Third unkn… Queens… y    \n6        0 male       2              3     1 21.1  Third unkn… Southa… n    \n# … with abbreviated variable names ¹​n_siblings_spouses, ²​embark_town\n```\n:::\n:::\n\n\nIf your data fits in memory the same `tensor_slices_dataset()` method works on lists, allowing this data to be easily imported:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_slices <- tensor_slices_dataset(df)\n\ntitanic_slices %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ survived          :<tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n $ sex               :<tf.Tensor: shape=(), dtype=string, numpy=b'male'>\n $ age               :<tf.Tensor: shape=(), dtype=float32, numpy=22.0>\n $ n_siblings_spouses:<tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n $ parch             :<tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n $ fare              :<tf.Tensor: shape=(), dtype=float32, numpy=7.25>\n $ class             :<tf.Tensor: shape=(), dtype=string, numpy=b'Third'>\n $ deck              :<tf.Tensor: shape=(), dtype=string, numpy=b'unknown'>\n  [list output truncated]\n```\n:::\n:::\n\n\nA more scalable approach is to load from disk as necessary. \n\nThe tfdatasets package provides methods to extract records from one or more CSV files that comply with [RFC 4180](https://tools.ietf.org/html/rfc4180).\n\nThe `experimental$make_csv_dataset` function is the high level interface for reading sets of csv files. It supports column type inference and many other features, like batching and shuffling, to make usage simple.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_batches <- tf$data$experimental$make_csv_dataset(\n  titanic_file, \n  batch_size = 4L,\n  label_name = \"survived\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_batches %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ :OrderedDict([('sex', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'male', b'female', b'female', b'female'], dtype=object)>), ('age', <tf.Tensor: shape=(4,), dtype=float32, numpy=array([49., 28., 24., 40.], dtype=float32)>), ('n_siblings_spouses', <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 0, 1, 1], dtype=int32)>), ('parch', <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 2, 1], dtype=int32)>), ('fare', <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 89.1042,  79.2   ,  65.    , 134.5   ], dtype=float32)>), ('class', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'First', b'First', b'Second', b'First'], dtype=object)>), ('deck', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'C', b'unknown', b'unknown', b'E'], dtype=object)>), ('embark_town', <tf.Tensor: shape=(4,), dtype=string, numpy=\narray([b'Cherbourg', b'Cherbourg', b'Southampton', b'Cherbourg'],\n      dtype=object)>), ('alone', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'n', b'y', b'n', b'n'], dtype=object)>)])\n $ :<tf.Tensor: shape=(4), dtype=int32, numpy=array([1, 1, 1, 1], dtype=int32)>\n```\n:::\n:::\n\n\nYou can use the `select_columns` argument if you only need a subset of columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_batches <- tf$data$experimental$make_csv_dataset(\n  titanic_file, \n  batch_size = 4L,\n  label_name = \"survived\", \n  select_columns = c('class', 'fare', 'survived')\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_batches %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ :OrderedDict([('fare', <tf.Tensor: shape=(4,), dtype=float32, numpy=array([21.6792, 26.55  , 18.75  , 16.7   ], dtype=float32)>), ('class', <tf.Tensor: shape=(4,), dtype=string, numpy=array([b'Third', b'First', b'Second', b'Third'], dtype=object)>)])\n $ :<tf.Tensor: shape=(4), dtype=int32, numpy=array([0, 0, 1, 1], dtype=int32)>\n```\n:::\n:::\n\n\nThere is also a lower-level `tf$experimental$CsvDataset` class which provides finer grained control. It does not support column type inference. Instead you must specify the type of each column. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_types  = list(tf$int32, tf$string, tf$float32, tf$int32, tf$int32, tf$float32, tf$string, tf$string, tf$string, tf$string)\ndataset <- tf$data$experimental$CsvDataset(titanic_file, titanic_types , header = TRUE)\n\ndataset %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ :<tf.Tensor: shape=(), dtype=int32, numpy=0>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'male'>\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=22.0>\n $ :<tf.Tensor: shape=(), dtype=int32, numpy=1>\n $ :<tf.Tensor: shape=(), dtype=int32, numpy=0>\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=7.25>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'Third'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'unknown'>\n  [list output truncated]\n```\n:::\n:::\n\n\nIf some columns are empty, this low-level interface allows you to provide default values instead of column types.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwriteLines(con = \"missing.csv\", text = \n\"1,2,3,4\n,2,3,4\n1,,3,4\n1,2,,4\n1,2,3,\n,,,\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates a dataset that reads all of the records from two CSV files, each with\n# four float columns which may have missing values.\n\nrecord_defaults <- c(999,999,999,999)\ndataset <- tf$data$experimental$CsvDataset(\"missing.csv\", record_defaults)\ndataset <- dataset %>% \n  dataset_map(function(...) tf$stack(list(...)))\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<MapDataset element_spec=TensorSpec(shape=(4,), dtype=tf.float32, name=None)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  coro::collect() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 6\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([999.,   2.,   3.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([  1., 999.,   3.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([  1.,   2., 999.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([  1.,   2.,   3., 999.], dtype=float32)>\n $ :<tf.Tensor: shape=(4), dtype=float32, numpy=array([999., 999., 999., 999.], dtype=float32)>\n```\n:::\n:::\n\n\nBy default, a `CsvDataset` yields *every* column of *every* line of the file,\nwhich may not be desirable, for example if the file starts with a header line\nthat should be ignored, or if some columns are not required in the input.\nThese lines and fields can be removed with the `header` and `select_cols`\narguments respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates a dataset that reads all of the records from two CSV files with\n# headers, extracting float data from columns 2 and 4.\nrecord_defaults <- c(999, 999) # Only provide defaults for the selected columns\ndataset <- tf$data$experimental$CsvDataset(\n  \"missing.csv\", \n  record_defaults, \n  select_cols = c(1L, 3L)\n)\ndataset <- dataset %>% \n  dataset_map(function(...) tf$stack(list(...)))\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<MapDataset element_spec=TensorSpec(shape=(2,), dtype=tf.float32, name=None)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  coro::collect() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 6\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([2., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([2., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([999.,   4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([2., 4.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([  2., 999.], dtype=float32)>\n $ :<tf.Tensor: shape=(2), dtype=float32, numpy=array([999., 999.], dtype=float32)>\n```\n:::\n:::\n\n\n### Consuming sets of files\n\nThere are many datasets distributed as a set of files, where each file is an example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflowers_root <- get_file(\n  'flower_photos',\n  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n  untar = TRUE\n)\n```\n:::\n\n\nNote: these images are licensed CC-BY, see LICENSE.txt for details.\n\nThe root directory contains a directory for each class:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::dir_ls(flowers_root)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/home/tomasz/.keras/datasets/flower_photos/LICENSE.txt\n/home/tomasz/.keras/datasets/flower_photos/daisy\n/home/tomasz/.keras/datasets/flower_photos/dandelion\n/home/tomasz/.keras/datasets/flower_photos/roses\n/home/tomasz/.keras/datasets/flower_photos/sunflowers\n/home/tomasz/.keras/datasets/flower_photos/tulips\n```\n:::\n:::\n\n\nThe files in each class directory are examples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_ds <- file_list_dataset(fs::path(flowers_root, \"*\", \"*\"))\nlist_ds %>% \n  dataset_take(5) %>% \n  coro::collect() %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/home/tomasz/.keras/datasets/flower_photos/dandelion/8181477_8cb77d2e0f_n.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/home/tomasz/.keras/datasets/flower_photos/dandelion/17280886635_e384d91300_n.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/home/tomasz/.keras/datasets/flower_photos/daisy/5944315415_2be8abeb2f_m.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/home/tomasz/.keras/datasets/flower_photos/dandelion/5740633858_8fd54c23c9_n.jpg'>\n $ :<tf.Tensor: shape=(), dtype=string, numpy=b'/home/tomasz/.keras/datasets/flower_photos/dandelion/2596413098_7ef69b7e1d_m.jpg'>\n```\n:::\n:::\n\n\nRead the data using the `tf$io$read_file` function and extract the label from the path, returning `(image, label)` pairs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprocess_path <- function(file_path) {\n  label <- tf$strings$split(file_path, \"/\")[-2]\n  list(\n    tf$io$read_file(file_path), \n    label\n  )\n}\n\nlabeled_ds <- list_ds %>% \n  dataset_map(process_path)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nel <- labeled_ds %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next()\nel[[1]]$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([])\n```\n:::\n\n```{.r .cell-code}\nel[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'tulips', shape=(), dtype=string)\n```\n:::\n:::\n\n\n<!--\nTODO(mrry): Add this section.\n\n### Handling text data with unusual sizes\n\n-->\n\n## Batching dataset elements\n\n\n### Simple batching\n\n\nThe simplest form of batching stacks `n` consecutive elements of a dataset into\na single element. The `dataset_batch()` transformation does exactly this, with\nthe same constraints as the `tf$stack()` operator, applied to each component\nof the elements: ie. for each component *i*, all elements must have a tensor\nof the exact same shape.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninc_dataset <- range_dataset(to = 100)\ndec_dataset <- range_dataset(0, -100, -1)\ndataset <- zip_datasets(inc_dataset, dec_dataset)\n\nbatched_dataset <- dataset %>% \n  dataset_batch(4)\n\nbatched_dataset %>% \n  coro::collect(4) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([0, 1, 2, 3])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([ 0, -1, -2, -3])>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([4, 5, 6, 7])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([-4, -5, -6, -7])>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([ 8,  9, 10, 11])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([ -8,  -9, -10, -11])>\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([12, 13, 14, 15])>\n  ..$ :<tf.Tensor: shape=(4), dtype=int64, numpy=array([-12, -13, -14, -15])>\n```\n:::\n:::\n\n\nWhile tfdatasets tries to propagate shape information, the default settings of `dataset_batch` result in an unknown batch size because the last batch may not be full. Note the `NULL`s in the shape:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatched_dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n```\n:::\n:::\n\n\nUse the `drop_remainder` argument to ignore that last batch, and get full shape propagation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatched_dataset <- dataset %>% dataset_batch(7, drop_remainder = TRUE)\nbatched_dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BatchDataset element_spec=(TensorSpec(shape=(7,), dtype=tf.int64, name=None), TensorSpec(shape=(7,), dtype=tf.int64, name=None))>\n```\n:::\n:::\n\n\n### Batching tensors with padding\n\n\nThe above recipe works for tensors that all have the same size. However, many\nmodels (e.g. sequence models) work with input data that can have varying size\n(e.g. sequences of different lengths). To handle this case, the\n`dataset_padded_batch()` transformation enables you to batch tensors of\ndifferent shape by specifying one or more dimensions in which they may be\npadded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- range_dataset(to = 100)\ndataset <- dataset %>% \n  dataset_map(function(x) tf$fill(list(tf$cast(x, tf$int32)), x)) %>% \n  dataset_padded_batch(4, padded_shapes = shape(NULL))\n\ndataset %>% \n  coro::collect(2) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ :<tf.Tensor: shape=(4, 3), dtype=int64, numpy=…>\n $ :<tf.Tensor: shape=(4, 7), dtype=int64, numpy=…>\n```\n:::\n:::\n\n\nThe `dataset_padded_batch()` transformation allows you to set different padding\nfor each dimension of each component, and it may be variable-length (signified\nby `NULL` in the example above) or constant-length. It is also possible to\noverride the padding value, which defaults to 0.\n\n<!--\nTODO(mrry): Add this section.\n\n### Dense ragged -> tf$SparseTensor\n\n-->\n\n## Training workflows\n\n\n### Processing multiple epochs\n\n\nThe tfdatasets API offers two main ways to process multiple epochs of the same\ndata.\n\nThe simplest way to iterate over a dataset in multiple epochs is to use the\n`dataset_repeat()` transformation. First, create a dataset of titanic data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_file <- get_file(\n  \"train.csv\", \n  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n)\ntitanic_lines <- text_line_dataset(titanic_file)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_batch_sizes <- function(ds) {    \n  batch_sizes <- ds %>% \n    coro::collect() %>% \n    sapply(function(x) as.numeric(x$shape[1]))\n  plot(seq_along(batch_sizes), batch_sizes)\n}\n```\n:::\n\n\nApplying the `dataset_repeat()` transformation with no arguments will repeat\nthe input indefinitely.\n\nThe `dataset_repeat()` transformation concatenates its\narguments without signaling the end of one epoch and the beginning of the next\nepoch. Because of this a `dataset_batch()` applied after `dataset_repeat()` will yield batches that straddle epoch boundaries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_batches <- titanic_lines %>% \n  dataset_repeat(3) %>% \n  dataset_batch(128)\nplot_batch_sizes(titanic_batches)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n\nIf you need clear epoch separation, put `dataset_batch()` before the repeat:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_batches <- titanic_lines %>% \n  dataset_batch(128) %>% \n  dataset_repeat(3)\n\nplot_batch_sizes(titanic_batches)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n\nIf you would like to perform a custom computation (e.g. to collect statistics) at the end of each epoch then it's simplest to restart the dataset iteration on each epoch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepochs <- 3\ndataset <- titanic_lines %>% \n  dataset_batch(128)\n\nfor(epoch in seq_len(epochs)) {\n  coro::loop(for(batch in dataset) {\n    print(batch$shape)\n  })\n  cat(\"End of epoch \", epoch, \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([116])\nEnd of epoch  1 \nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([116])\nEnd of epoch  2 \nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([128])\nTensorShape([116])\nEnd of epoch  3 \n```\n:::\n:::\n\n\n### Randomly shuffling input data\n\nThe `dataset_shuffle()` transformation maintains a fixed-size\nbuffer and chooses the next element uniformly at random from that buffer.\n\nNote: While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and significant time to fill. Consider using `dataset_interleave()` across files if this becomes a problem.\n\nAdd an index to the dataset so you can see the effect:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlines <- text_line_dataset(titanic_file)\ncounter <- tf$data$experimental$Counter()\n\ndataset <- zip_datasets(counter, lines)\ndataset <- dataset %>% \n  dataset_shuffle(buffer_size = 100) %>% \n  dataset_batch(20)\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n```\n:::\n:::\n\n\nSince the `buffer_size` is 100, and the batch size is 20, the first batch contains no elements with an index over 120.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nline_batch <- coro::collect(dataset, 1)\nline_batch[[1]][[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[ 83  37  31  67  82 102  89  65 107  76  78  81  17  98  18 112  16  21\n   1   4], shape=(20), dtype=int64)\n```\n:::\n:::\n\n\nAs with `dataset_batch()` the order relative to `dataset_repeat()` matters.\n\n`dataset_shuffle()` doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- zip_datasets(counter, lines)\nshuffled <- dataset %>% \n  dataset_shuffle(buffer_size = 100) %>% \n  dataset_batch(10) %>% \n  dataset_repeat(2)\n\ncat(\"Here are the item ID's near the epoch boundary:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHere are the item ID's near the epoch boundary:\n```\n:::\n\n```{.r .cell-code}\nshuffled %>% \n  dataset_skip(60) %>% \n  dataset_take(5) %>% \n  dataset_collect() %>% \n  lapply(function(x) x[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor([411 614 604 480 406 518 589 607 583 484], shape=(10), dtype=int64)\n\n[[2]]\ntf.Tensor([578 555 467 605  66 623 354 616 460 571], shape=(10), dtype=int64)\n\n[[3]]\ntf.Tensor([615 534 428 533 504 499 608 609], shape=(8), dtype=int64)\n\n[[4]]\ntf.Tensor([62 12 29 67 60 76 56 73 77 19], shape=(10), dtype=int64)\n\n[[5]]\ntf.Tensor([  8  90  45  53  44  49   9  28 104  24], shape=(10), dtype=int64)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshuffle_repeat <- shuffled %>% \n  coro::collect() %>% \n  sapply(function(x) mean(as.numeric(x[[1]])))\nplot(shuffle_repeat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\nBut a repeat before a shuffle mixes the epoch boundaries together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- zip_datasets(counter, lines)\nshuffled <- dataset %>% \n  dataset_repeat(2) %>% \n  dataset_shuffle(buffer_size = 100) %>% \n  dataset_batch(10)\n\ncat(\"Here are the item ID's near the epoch boundary:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHere are the item ID's near the epoch boundary:\n```\n:::\n\n```{.r .cell-code}\nshuffled %>% \n  dataset_skip(55) %>% \n  dataset_take(15) %>% \n  coro::collect() %>% \n  lapply(function(x) x[[1]]) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 15\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([339, 549, 548, 441, 449, 445,  11,   6, 516, 542])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([  8, 627, 471, 613, 592, 509, 593,   0, 624, 526])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([  1, 515, 541, 240, 499, 328,  18,  27,  47, 557])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([209, 464,  42,  23,  48, 626, 612, 472,   9, 496])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 20, 459, 533,  51,  36,  61, 518,  59, 512, 590])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([601,  71, 555, 616, 622,  26,  53, 448,  79, 618])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 49, 540,   2,  76, 608,  17,  29,  88, 551,  86])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([603,  77,  46, 539, 589,  10, 606, 265,  54,  30])>\n  [list output truncated]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrepeat_shuffle <- shuffled %>% \n  coro::collect() %>% \n  sapply(function(x) mean(as.numeric(x[[1]])))\nplot(repeat_shuffle)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\n## Preprocessing data\n\n\nThe `dataset_map(f)` transformation produces a new dataset by applying a given\nfunction `f` to each element of the input dataset. It is based on the\n[`map()`](https://en.wikipedia.org/wiki/Map_(higher-order_function)) function\nthat is commonly applied to lists (and other structures) in functional\nprogramming languages. The function `f` takes the `tf$Tensor` objects that\nrepresent a single element in the input, and returns the `tf$Tensor` objects\nthat will represent a single element in the new dataset. Its implementation uses\nstandard TensorFlow operations to transform one element into another.\n\nThis section covers common examples of how to use `dataset_map()`.\n\n### Decoding image data and resizing it\n\n\n<!-- TODO(markdaoust): link to image augmentation when it exists -->\nWhen training a neural network on real-world image data, it is often necessary\nto convert images of different sizes to a common size, so that they may be\nbatched into a fixed size.\n\nRebuild the flower filenames dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_ds <- file_list_dataset(file.path(flowers_root, \"*\", \"*\"))\n```\n:::\n\n\nWrite a function that manipulates the dataset elements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reads an image from a file, decodes it into a dense tensor, and resizes it\n# to a fixed shape.\n\nparse_image <- function(filename) {\n  parts <- tf$strings$split(filename, \"/\")\n  label <- parts[-2]\n\n  image <- tf$io$read_file(filename)\n  image <- tf$io$decode_jpeg(image)\n  image <- tf$image$convert_image_dtype(image, tf$float32)\n  image <- tf$image$resize(image, shape(128, 128))\n  \n  list(image, label)\n}\n```\n:::\n\n\nTest that it works.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparsed <- list_ds %>% \n  reticulate::as_iterator() %>% \n  reticulate::iter_next() %>% \n  parse_image()\n\nshow <- function(parsed, maxcolor=1) {  \n  plot(as.raster(as.array(parsed[[1]]), max = maxcolor))\n  title(as.character(parsed[[2]]))\n}\n\nshow(parsed)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n:::\n\n\nMap it over the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimages_ds <- list_ds %>% \n  dataset_map(parse_image)\n\nimages_ds %>% \n  dataset_take(2) %>% \n  coro::collect() %>% \n  lapply(show)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-64-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nNULL\n\n[[2]]\nNULL\n```\n:::\n:::\n\n\n<!-- ### Applying arbitrary Python logic -->\n\n\n<!-- For performance reasons, use TensorFlow operations for -->\n<!-- preprocessing your data whenever possible. However, it is sometimes useful to -->\n<!-- call external Python libraries when parsing your input data. You can use the `tf$py_function()` operation in a `dataset_map()` transformation. -->\n\n<!-- For example, if you want to apply a random rotation, the `tf$image` module only has `tf$image$rot90`, which is not very useful for image augmentation.  -->\n\n<!-- Note: `tensorflow_addons` has a TensorFlow compatible `rotate` in `tensorflow_addons$image$rotate`. -->\n\n<!-- To demonstrate `tf$py_function`, try using the `scipy$ndimage$rotate` function instead: -->\n\n<!-- ```{r} -->\n<!-- import scipy$ndimage as ndimage -->\n\n<!-- random_rotate_image <- function(image) {    } -->\n<!--   image <- ndimage$rotate(image, np$random$uniform(-30, 30), reshape = FALSE) -->\n<!--   return image -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- image, label = next(iter(images_ds)) -->\n<!-- image <- random_rotate_image(image) -->\n<!-- show(image, label) -->\n<!-- ``` -->\n\n<!-- To use this function with `dataset_map` the same caveats apply as with `dataset_from_generator`, you need to describe the return shapes and types when you apply the function: -->\n\n<!-- ```{r} -->\n<!-- tf_random_rotate_image <- function(image, label) {    } -->\n<!--   im_shape <- image$shape -->\n<!--   [image,] <- tf$py_function(random_rotate_image, [image], [tf$float32]) -->\n<!--   image$set_shape(im_shape) -->\n<!--   return image, label -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- rot_ds <- images_ds$map(tf_random_rotate_image) -->\n\n<!-- for image, label in rot_ds$take(2): -->\n<!--   show(image, label) -->\n<!-- ``` -->\n\n### Parsing `tf$Example` protocol buffer messages\n\n\nMany input pipelines extract `tf$train$Example` protocol buffer messages from a\nTFRecord format. Each `tf$train$Example` record contains one or more \"features\",\nand the input pipeline typically converts these features into tensors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfsns_test_file <- get_file(\n  \"fsns.tfrec\", \n  \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\"\n)\ndataset <- tfrecord_dataset(filenames = fsns_test_file)\ndataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n```\n:::\n:::\n\n\nYou can work with `tf$train$Example` protos outside of a `tf$data$Dataset` to understand the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_example <- dataset %>% reticulate::as_iterator() %>% reticulate::iter_next()\nparsed <- tf$train$Example$FromString(raw_example$numpy())\n\nfeature <- parsed$features$feature\nraw_img <- feature['image/encoded']$bytes_list$value[0]\nimg <- tf$image$decode_png(raw_img)\nimg %>% \n  as.array() %>% \n  as.raster(max = 255) %>% \n  plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-66-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_example <- dataset %>% reticulate::as_iterator() %>% reticulate::iter_next()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_parse <- function(eg) {\n  example <- tf$io$parse_example(\n      eg[tf$newaxis], \n      reticulate::dict(\n        'image/encoded' = tf$io$FixedLenFeature(shape = shape(), dtype = tf$string),\n        'image/text' = tf$io$FixedLenFeature(shape = shape(), dtype = tf$string)\n      )\n  )\n  out <- list(example[['image/encoded']][1], example[['image/text']][1])\n  out[[1]] <- tf$image$decode_png(out[[1]])\n  out\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexample <- tf_parse(raw_example)\nprint(as.character(example[[2]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Rue Perreyon\"\n```\n:::\n\n```{.r .cell-code}\nshow(example, maxcolor = 255)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-69-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndecoded <- dataset %>% \n  dataset_map(tf_parse)\ndecoded\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<MapDataset element_spec=(TensorSpec(shape=(None, None, None), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndecoded %>% \n  dataset_batch(10) %>% \n  coro::collect(1) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 1\n $ :List of 2\n  ..$ :<tf.Tensor: shape=(10, 150, 600, 3), dtype=uint8, numpy=…>\n  ..$ :<tf.Tensor: shape=(10), dtype=string, numpy=…>\n```\n:::\n:::\n\n\n### Time series windowing\n\nFor an end to end time series example see: [Time series forecasting](../../tutorials/structured_data/time_series.qmd).\nTime series data is often organized with the time axis intact.\nUse a simple `range_dataset()` to demonstrate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange_ds <- range_dataset(to = 100000)\n```\n:::\n\n\nTypically, models based on this sort of data will want a contiguous time slice. \nThe simplest approach would be to batch the data:\n\n#### Using `batch`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatches <- range_ds %>% \n  dataset_batch(10, drop_remainder = TRUE)\n\nbatches %>% coro::collect(5) %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 5\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])>\n```\n:::\n:::\n\n\nOr to make dense predictions one step into the future, you might shift the features and labels by one step relative to each other:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndense_1_step <- function(batch) {\n  # Shift features and labels one step relative to each other.\n  list(batch[NULL:-1], batch[2:NULL])\n}\n  \npredict_dense_1_step <- batches %>% dataset_map(dense_1_step)\n\npredict_dense_1_step %>% \n  coro::collect(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]][[1]]\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([1 2 3 4 5 6 7 8 9], shape=(9), dtype=int64)\n\n\n[[2]]\n[[2]][[1]]\ntf.Tensor([10 11 12 13 14 15 16 17 18 19], shape=(10), dtype=int64)\n\n[[2]][[2]]\ntf.Tensor([11 12 13 14 15 16 17 18 19], shape=(9), dtype=int64)\n\n\n[[3]]\n[[3]][[1]]\ntf.Tensor([20 21 22 23 24 25 26 27 28 29], shape=(10), dtype=int64)\n\n[[3]][[2]]\ntf.Tensor([21 22 23 24 25 26 27 28 29], shape=(9), dtype=int64)\n```\n:::\n:::\n\n\nTo predict a whole window instead of a fixed offset you can split the batches into two parts:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatches <- range_ds %>% \n  dataset_batch(15, drop_remainder = TRUE)\n\nlabel_next_5_steps <- function(batch) {\n list(\n   batch[NULL:-6],# Inputs: All except the last 5 steps\n   batch[-5:NULL] # Labels: The last 5 steps\n )   \n}\n\npredict_5_steps <- batches %>% dataset_map(label_next_5_steps)\n\npredict_5_steps %>% coro::collect(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]][[1]]\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([10 11 12 13 14], shape=(5), dtype=int64)\n\n\n[[2]]\n[[2]][[1]]\ntf.Tensor([15 16 17 18 19 20 21 22 23 24], shape=(10), dtype=int64)\n\n[[2]][[2]]\ntf.Tensor([25 26 27 28 29], shape=(5), dtype=int64)\n\n\n[[3]]\n[[3]][[1]]\ntf.Tensor([30 31 32 33 34 35 36 37 38 39], shape=(10), dtype=int64)\n\n[[3]][[2]]\ntf.Tensor([40 41 42 43 44], shape=(5), dtype=int64)\n```\n:::\n:::\n\n\nTo allow some overlap between the features of one batch and the labels of another, use `zip_datasets()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeature_length <- 10\nlabel_length <- 3\n\nfeatures <- range_ds %>% \n  dataset_batch(feature_length, drop_remainder = TRUE)\nlabels <- range_ds %>% \n  dataset_batch(feature_length) %>% \n  dataset_skip(1) %>% \n  dataset_map(function(labels) labels[NULL:label_length])\n\npredicted_steps <- zip_datasets(features, labels)\ncoro::collect(predicted_steps, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]][[1]]\ntf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([10 11 12], shape=(3), dtype=int64)\n\n\n[[2]]\n[[2]][[1]]\ntf.Tensor([10 11 12 13 14 15 16 17 18 19], shape=(10), dtype=int64)\n\n[[2]][[2]]\ntf.Tensor([20 21 22], shape=(3), dtype=int64)\n\n\n[[3]]\n[[3]][[1]]\ntf.Tensor([20 21 22 23 24 25 26 27 28 29], shape=(10), dtype=int64)\n\n[[3]][[2]]\ntf.Tensor([30 31 32], shape=(3), dtype=int64)\n\n\n[[4]]\n[[4]][[1]]\ntf.Tensor([30 31 32 33 34 35 36 37 38 39], shape=(10), dtype=int64)\n\n[[4]][[2]]\ntf.Tensor([40 41 42], shape=(3), dtype=int64)\n\n\n[[5]]\n[[5]][[1]]\ntf.Tensor([40 41 42 43 44 45 46 47 48 49], shape=(10), dtype=int64)\n\n[[5]][[2]]\ntf.Tensor([50 51 52], shape=(3), dtype=int64)\n```\n:::\n:::\n\n\n#### Using `window`\n\nWhile using `dataset_batch` works, there are situations where you may need finer control. The `dataset_window()` method gives you complete control, but requires some care: it returns a `Dataset` of `Datasets`. See [Dataset structure](#dataset_structure) for details.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindow_size <- 5\nwindows <- range_ds %>% \n  dataset_window(window_size, shift = 1)\ncoro::collect(windows, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[2]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[3]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[4]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n[[5]]\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n```\n:::\n:::\n\n\nThe `dataset_flat_map` method can take a dataset of datasets and flatten it into a single dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindows %>% \n  dataset_flat_map(function(x) x) %>% \n  dataset_take(30) %>% \n  coro::collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor(0, shape=(), dtype=int64)\n\n[[2]]\ntf.Tensor(1, shape=(), dtype=int64)\n\n[[3]]\ntf.Tensor(2, shape=(), dtype=int64)\n\n[[4]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[5]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[6]]\ntf.Tensor(1, shape=(), dtype=int64)\n\n[[7]]\ntf.Tensor(2, shape=(), dtype=int64)\n\n[[8]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[9]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[10]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[11]]\ntf.Tensor(2, shape=(), dtype=int64)\n\n[[12]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[13]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[14]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[15]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[16]]\ntf.Tensor(3, shape=(), dtype=int64)\n\n[[17]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[18]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[19]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[20]]\ntf.Tensor(7, shape=(), dtype=int64)\n\n[[21]]\ntf.Tensor(4, shape=(), dtype=int64)\n\n[[22]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[23]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[24]]\ntf.Tensor(7, shape=(), dtype=int64)\n\n[[25]]\ntf.Tensor(8, shape=(), dtype=int64)\n\n[[26]]\ntf.Tensor(5, shape=(), dtype=int64)\n\n[[27]]\ntf.Tensor(6, shape=(), dtype=int64)\n\n[[28]]\ntf.Tensor(7, shape=(), dtype=int64)\n\n[[29]]\ntf.Tensor(8, shape=(), dtype=int64)\n\n[[30]]\ntf.Tensor(9, shape=(), dtype=int64)\n```\n:::\n:::\n\n\nIn nearly all cases, you will want to `.batch` the dataset first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsub_to_batch <- function(sub) {\n  sub %>% \n    dataset_batch(window_size, drop_remainder = TRUE)\n}\n\nwindows %>% \n  dataset_flat_map(sub_to_batch) %>% \n  dataset_take(5) %>% \n  coro::collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor([0 1 2 3 4], shape=(5), dtype=int64)\n\n[[2]]\ntf.Tensor([1 2 3 4 5], shape=(5), dtype=int64)\n\n[[3]]\ntf.Tensor([2 3 4 5 6], shape=(5), dtype=int64)\n\n[[4]]\ntf.Tensor([3 4 5 6 7], shape=(5), dtype=int64)\n\n[[5]]\ntf.Tensor([4 5 6 7 8], shape=(5), dtype=int64)\n```\n:::\n:::\n\n\nNow, you can see that the `shift` argument controls how much each window moves over.\nPutting this together you might write this function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_window_dataset <- function(ds, window_size = 5, shift = 1, stride = 1) {\n  windows <- ds %>% \n    dataset_window(window_size, shift = shift, stride = stride)\n\n  sub_to_batch <- function(sub) {\n    sub %>% \n      dataset_batch(window_size, drop_remainder = TRUE)\n  }\n  \n  windows %>% \n    dataset_flat_map(sub_to_batch)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nds <- make_window_dataset(range_ds, window_size = 10, shift = 5, stride = 3)\ncoro::collect(ds, 10) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([ 5,  8, 11, 14, 17, 20, 23, 26, 29, 32])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([10, 13, 16, 19, 22, 25, 28, 31, 34, 37])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([15, 18, 21, 24, 27, 30, 33, 36, 39, 42])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([20, 23, 26, 29, 32, 35, 38, 41, 44, 47])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([25, 28, 31, 34, 37, 40, 43, 46, 49, 52])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([30, 33, 36, 39, 42, 45, 48, 51, 54, 57])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([35, 38, 41, 44, 47, 50, 53, 56, 59, 62])>\n  [list output truncated]\n```\n:::\n:::\n\n\nThen it's easy to extract labels, as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndense_labels_ds <- ds %>% \n  dataset_map(dense_1_step)\n\ncoro::collect(dense_labels_ds, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]][[1]]\ntf.Tensor([ 0  3  6  9 12 15 18 21 24 27], shape=(10), dtype=int64)\n\n[[1]][[2]]\ntf.Tensor([ 3  6  9 12 15 18 21 24 27], shape=(9), dtype=int64)\n```\n:::\n:::\n\n\n### Resampling\n\n\nWhen working with a dataset that is very class-imbalanced, you may want to resample the dataset. tfdatasets provides two methods to do this. The credit card fraud dataset is a good example of this sort of problem.\n\nNote: See [Imbalanced Data](../tutorials/keras/imbalanced_data.qmd) for a full tutorial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzip_path <- get_file(\n  origin = 'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n  fname = 'creditcard.zip',\n  extract = TRUE\n)\n\ncsv_path <- gsub(\"zip\", \"csv\", zip_path)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncreditcard_ds <- tf$data$experimental$make_csv_dataset(\n  csv_path, \n  batch_size = 1024L, \n  label_name = \"Class\",\n  # Set the column types: 30 floats and an int.\n  column_defaults = c(lapply(seq_len(30), function(x) tf$float32), tf$int64)\n)\n```\n:::\n\n\nNow, check the distribution of classes, it is highly skewed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount <- function(counts, batch) {\n  \n  class_1 <- batch[[2]] == 1\n  class_1 <- tf$cast(class_1, tf$int32)\n\n  class_0 <- batch[[2]] == 0\n  class_0 <- tf$cast(class_0, tf$int32)\n\n  counts[['class_0']] <- counts[['class_0']] + tf$reduce_sum(class_0)\n  counts[['class_1']] <- counts[['class_1']] + tf$reduce_sum(class_1)\n\n  counts\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts <- creditcard_ds %>% \n  dataset_take(10) %>% \n  dataset_reduce(\n    initial_state = list('class_0' = 0L, 'class_1' = 0L),\n    reduce_func = count\n  )\n\ncounts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$class_0\ntf.Tensor(10198, shape=(), dtype=int32)\n\n$class_1\ntf.Tensor(42, shape=(), dtype=int32)\n```\n:::\n:::\n\n\nA common approach to training with an imbalanced dataset is to balance it. tfdatasets includes a few methods which enable this workflow:\n\n#### Datasets sampling\n\nOne approach to resampling a dataset is to use `sample_from_datasets`. This is more applicable when you have a separate `data$Dataset` for each class.\n\nHere, just use filter to generate them from the credit card fraud data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnegative_ds <- creditcard_ds %>% \n  dataset_unbatch() %>% \n  dataset_filter(function(features, label) label == 0) %>% \n  dataset_repeat()\n  \npositive_ds <- creditcard_ds %>% \n  dataset_unbatch() %>% \n  dataset_filter(function(features, label) label == 1) %>% \n  dataset_repeat()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npositive_ds %>% \n  coro::collect(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]][[1]]\nDict (30 items)\n\n[[1]][[2]]\ntf.Tensor(1, shape=(), dtype=int64)\n```\n:::\n:::\n\n\nTo use `sample_from_datasets` pass the datasets, and the weight for each:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalanced_ds <- list(negative_ds, positive_ds) %>% \n  sample_from_datasets(c(0.5, 0.5)) %>% \n  dataset_batch(10)\n```\n:::\n\n\nNow the dataset produces examples of each class with 50/50 probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalanced_ds %>% \n  dataset_map(function(x, y) y) %>% \n  coro::collect(10) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 1, 1, 0, 1, 1, 1, 0, 1, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 1, 0, 0, 0, 1, 1, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 1, 0, 1, 1, 1, 1, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 1, 1, 0, 0, 1, 1, 0, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 1, 1, 0, 0, 0, 0, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 1, 1, 1, 1, 0, 1, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 0, 0, 0, 1, 0, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 1, 0, 0, 1, 1, 0, 0, 1])>\n  [list output truncated]\n```\n:::\n:::\n\n\n#### Rejection resampling\n\nOne problem with the above `sample_from_datasets` approach is that\nit needs a separate TensorFlow Dataset per class. You could use `dataset_filter`\nto create those two datasets, but that results in all the data being loaded twice.\n\nThe `dataset_rejection_resample()` method can be applied to a dataset to rebalance it, while only loading it once. Elements will be dropped from the dataset to achieve balance.\n\n`dataset_rejection_resample()` takes a `class_func` argument. This `class_func` is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing.\n\nThe goal here is to balance the lable distribution, and the elements of `creditcard_ds` are already `(features, label)` pairs. So the `class_func` just needs to return those labels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_func <- function(features, label) { \n  label\n}\n```\n:::\n\n\nThe resampling method deals with individual examples, so in this case you must `unbatch` the dataset before applying that method.\n\nThe method needs a target distribution, and optionally an initial distribution estimate as inputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresample_ds <- creditcard_ds %>% \n  dataset_unbatch() %>% \n  dataset_rejection_resample(\n    class_func, \n    target_dist = c(0.5,0.5),\n    initial_dist = prop.table(sapply(counts, as.numeric))\n  ) %>% \n  dataset_batch(10)\n```\n:::\n\n\nThe `dataset_rejection_resample()` method returns `(class, example)` pairs where the `class` is the output of the `class_func`. In this case, the `example` was already a `(feature, label)` pair, so use `map` to drop the extra copy of the labels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalanced_ds <- resample_ds %>% \n  dataset_map(function(extra_label, features_and_label) features_and_label)\n```\n:::\n\n\nNow the dataset produces examples of each class with 50/50 probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbalanced_ds %>% \n  dataset_map(function(feat, label) label) %>% \n  coro::collect(10) %>% \n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 10\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 0, 1, 0, 0, 0, 0, 1, 1, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 0, 1, 1, 1, 1, 0, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 1, 0, 0, 0, 0, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([1, 0, 1, 1, 0, 0, 0, 0, 1, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 1, 0, 1, 1, 1, 0, 0, 0])>\n $ :<tf.Tensor: shape=(10), dtype=int64, numpy=array([0, 1, 1, 1, 1, 0, 0, 1, 0, 1])>\n  [list output truncated]\n```\n:::\n:::\n\n\n## Iterator Checkpointing\n\n\nTensorflow supports [taking checkpoints](https://www.tensorflow.org/guide/checkpoint) so that when your training process restarts it can restore the latest checkpoint to recover most of its progress. In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don't want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as `shuffle` and `prefetch` require buffering elements within the iterator. \n\nTo include your iterator in a checkpoint, pass the iterator to the `tf$train$Checkpoint` constructor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange_ds <- range_dataset(to = 20)\niterator <- reticulate::as_iterator(range_ds)\nckpt <- tf$train$Checkpoint(step = tf$Variable(0), iterator = iterator)\nmanager <- tf$train$CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep = 3)\n\nfor (i in 1:5) {\n  print(reticulate::iter_next(iterator))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\nsave_path <- manager$save()\n\nfor (i in 1:5) {\n  print(reticulate::iter_next(iterator))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\nckpt$restore(manager$latest_checkpoint)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x7f98fc3fa700>\n```\n:::\n\n```{.r .cell-code}\nfor (i in 1:5) {\n  print(reticulate::iter_next(iterator))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n```\n:::\n:::\n\n\nNote: It is not possible to checkpoint an iterator which relies on external state such as a `tf$py_function`. Attempting to do so will raise an exception complaining about the external state.\n\n## Using tfdatasets with Keras\n\n\nThe Keras API simplifies many aspects of creating and executing machine\nlearning models. Its `fit()` and `evaluate()` and `predict()` APIs support datasets as inputs. Here is a quick dataset and model setup:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(train, test) %<-% dataset_fashion_mnist()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfmnist_train_ds <- train %>% \n  tensor_slices_dataset() %>% \n  dataset_map(unname) %>% \n  dataset_shuffle(5000) %>% \n  dataset_batch(32)\n\nmodel <- keras_model_sequential() %>% \n  layer_rescaling(scale = 1/255) %>% \n  layer_flatten() %>% \n  layer_dense(10)\n\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE), \n  metrics = 'accuracy'\n)\n```\n:::\n\n\n Passing a dataset of `(feature, label)` pairs is all that's needed for `fit()` and `evaluate()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% fit(fmnist_train_ds, epochs = 2)\n```\n:::\n\n\nIf you pass an infinite dataset, for example by calling `dataset_repeat()`, you just need to also pass the `steps_per_epoch` argument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% fit(\n  fmnist_train_ds %>% dataset_repeat(), \n  epochs = 2, \n  steps_per_epoch = 20\n)\n```\n:::\n\n\nFor evaluation you can pass the number of evaluation steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(fmnist_train_ds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.4434702 0.8487500 \n```\n:::\n:::\n\n\nFor long datasets, set the number of steps to evaluate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(\n  fmnist_train_ds %>% dataset_repeat(), \n  steps = 10\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.4317979 0.8500000 \n```\n:::\n:::\n\n\nThe labels are not required in when calling `Model$predict`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_ds <- tensor_slices_dataset(train$x) %>% \n  dataset_batch(32)\nresult <- predict(model, predict_ds, steps = 10)\ndim(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 320  10\n```\n:::\n:::\n\n\nBut the labels are ignored if you do pass a dataset containing them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- predict(model, fmnist_train_ds, steps = 10)\ndim(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 320  10\n```\n:::\n:::\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-website/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tfdatasets_2.9.0.9000 tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           pillar_1.8.1         compiler_4.2.1      \n [4] base64enc_0.1-3      tools_4.2.1          bit_4.0.4           \n [7] zeallot_0.1.0        digest_0.6.29        tibble_3.1.8        \n[10] lifecycle_1.0.1      jsonlite_1.8.0       evaluate_0.16       \n[13] lattice_0.20-45      pkgconfig_2.0.3      png_0.1-7           \n[16] rlang_1.0.4          Matrix_1.4-2         cli_3.3.0           \n[19] parallel_4.2.1       yaml_2.3.5           xfun_0.32           \n[22] coro_1.0.3           fastmap_1.1.0        stringr_1.4.1       \n[25] knitr_1.40           fs_1.5.2             hms_1.1.2           \n[28] generics_0.1.3       htmlwidgets_1.5.4    vctrs_0.4.1         \n[31] bit64_4.0.5          rprojroot_2.0.3      grid_4.2.1          \n[34] tidyselect_1.1.2     reticulate_1.25-9000 glue_1.6.2          \n[37] here_1.0.1           R6_2.5.1             fansi_1.0.3         \n[40] vroom_1.5.7          rmarkdown_2.16       tzdb_0.3.0          \n[43] purrr_0.3.4          readr_2.1.2          magrittr_2.0.3      \n[46] whisker_0.4          ellipsis_0.3.2       tfruns_1.5.0.9000   \n[49] htmltools_0.5.3      utf8_1.2.2           stringi_1.7.8       \n[52] crayon_1.5.1        \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.2.0\nasttokens==2.0.8\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.6.15\ncharset-normalizer==2.1.1\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.7.1\nexecuting==1.0.0\nflatbuffers==1.12\ngast==0.4.0\ngoogle-auth==2.11.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.9.0\nipython==8.4.0\njedi==0.18.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.3\nkt-legacy==1.0.4\nlibclang==14.0.6\nMarkdown==3.4.1\nMarkupSafe==2.1.1\nmatplotlib-inline==0.1.6\nnumpy==1.23.2\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.13.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npytz==2022.2.1\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.9.1\nsix==1.16.0\nstack-data==0.5.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.10.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\nurllib3==1.26.12\nwcwidth==0.2.5\nWerkzeug==2.2.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-08-30 \nPage render time: 27 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}