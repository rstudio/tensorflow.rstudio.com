{
  "hash": "33d3e6bd3c97680d3eed35a5313c6cbd",
  "result": {
    "markdown": "---\ntitle: Customizing what happens in `fit()`\nauthor: \n  - name: Francois Chollet\n    url: https://twitter.com/fchollet\n  - name: Tomasz Kalinowski\n    url: https://github.com/t-kalinowski\nexecute:\n  eval: true\naliases:\n  - ../../articles/new-guides/customizing_what_happens_in_fit.html\n  - ../../guide/keras/custom_models/index.html\n---\n\n\n## Introduction\n\nWhen you're doing supervised learning, you can use `fit()` and\neverything works smoothly.\n\nWhen you need to write your own training loop from scratch, you can use\nthe `GradientTape` and take control of every little detail.\n\nBut what if you need a custom training algorithm, but you still want to\nbenefit from the convenient features of `fit()`, such as callbacks,\nbuilt-in distribution support, or step fusing?\n\nA core principle of Keras is **progressive disclosure of complexity**.\nYou should always be able to get into lower-level workflows in a gradual\nway. You shouldn't fall off a cliff if the high-level functionality\ndoesn't exactly match your use case. You should be able to gain more\ncontrol over the small details while retaining a commensurate amount of\nhigh-level convenience.\n\nWhen you need to customize what `fit()` does, you should **override the\ntraining step function of the `Model` class**. This is the function that\nis called by `fit()` for every batch of data. You will then be able to\ncall `fit()` as usual -- and it will be running your own learning\nalgorithm.\n\nNote that this pattern does not prevent you from building models with\nthe Functional API. You can do this whether you're building `Sequential`\nmodels, Functional API models, or subclassed models.\n\nLet's see how that works.\n\n## Setup\n\nRequires TensorFlow 2.2 or later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## A first simple example\n\nLet's start from a simple example:\n\n-   We create a new model class by calling `new_model_class()`.\n-   We just override the method `train_step(data)`.\n-   We return a dictionary mapping metric names (including the loss) to\n    their current value.\n\nThe input argument `data` is what gets passed to fit as training data:\n\n-   If you pass arrays, by calling `fit(x, y, ...)`, then `data` will be\n    the tuple `(x, y)`\n-   If you pass a `tf$data$Dataset`, by calling `fit(dataset, ...)`,\n    then `data` will be what gets yielded by `dataset` at each batch.\n\nIn the body of the `train_step` method, we implement a regular training\nupdate, similar to what you are already familiar with. Importantly, **we\ncompute the loss via `self$compiled_loss`**, which wraps the loss(es)\nfunction(s) that were passed to `compile()`.\n\nSimilarly, we call `self$compiled_metrics$update_state(y, y_pred)` to\nupdate the state of the metrics that were passed in `compile()`, and we\nquery results from `self$metrics` at the end to retrieve their current\nvalue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n```\n:::\n\n\nLet's try this out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/3\n32/32 - 1s - loss: 2.1307 - mae: 1.3566 - 514ms/epoch - 16ms/step\nEpoch 2/3\n32/32 - 0s - loss: 1.0457 - mae: 0.8961 - 78ms/epoch - 2ms/step\nEpoch 3/3\n32/32 - 0s - loss: 0.5223 - mae: 0.5961 - 47ms/epoch - 1ms/step\n```\n:::\n:::\n\n\n## Going lower-level\n\nNaturally, you could just skip passing a loss function in `compile()`,\nand instead do everything *manually* in `train_step`. Likewise for\nmetrics.\n\nHere's a lower-level example, that only uses `compile()` to configure\nthe optimizer:\n\n-   We start by creating `Metric` instances to track our loss and a MAE\n    score.\n-   We implement a custom `train_step()` that updates the state of these\n    metrics (by calling `update_state()` on them), then query them (via\n    `result()`) to return their current average value, to be displayed\n    by the progress bar and to be pass to any callback.\n-   Note that we would need to call `reset_states()` on our metrics\n    between each epoch! Otherwise calling `result()` would return an\n    average since the start of training, whereas we usually work with\n    per-epoch averages. Thankfully, the framework can do that for us:\n    just list any metric you want to reset in the `metrics` property of\n    the model. The model will call `reset_states()` on any object listed\n    here at the beginning of each `fit()` epoch or at the beginning of a\n    call to `evaluate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_tracker <- metric_mean(name = \"loss\")\nmae_metric <- metric_mean_absolute_error(name = \"mae\")\n\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \"adam\")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n32/32 - 0s - loss: 0.5503 - mae: 0.6102 - 341ms/epoch - 11ms/step\nEpoch 2/5\n32/32 - 0s - loss: 0.2866 - mae: 0.4246 - 64ms/epoch - 2ms/step\nEpoch 3/5\n32/32 - 0s - loss: 0.2568 - mae: 0.4032 - 56ms/epoch - 2ms/step\nEpoch 4/5\n32/32 - 0s - loss: 0.2500 - mae: 0.3975 - 54ms/epoch - 2ms/step\nEpoch 5/5\n32/32 - 0s - loss: 0.2437 - mae: 0.3927 - 53ms/epoch - 2ms/step\n```\n:::\n:::\n\n\n## Supporting `sample_weight` & `class_weight`\n\nYou may have noticed that our first basic example didn't make any\nmention of sample weighting. If you want to support the `fit()`\narguments `sample_weight` and `class_weight`, you'd simply do the\nfollowing:\n\n-   Unpack `sample_weight` from the `data` argument\n-   Pass it to `compiled_loss` & `compiled_metrics` (of course, you\n    could also just apply it manually if you don't rely on `compile()`\n    for losses & metrics)\n-   That's it. That's the list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/3\n32/32 - 0s - loss: 0.5403 - mae: 0.9025 - 375ms/epoch - 12ms/step\nEpoch 2/3\n32/32 - 0s - loss: 0.2226 - mae: 0.5340 - 46ms/epoch - 1ms/step\nEpoch 3/3\n32/32 - 0s - loss: 0.1318 - mae: 0.4008 - 45ms/epoch - 1ms/step\n```\n:::\n:::\n\n\n## Providing your own evaluation step\n\nWhat if you want to do the same for calls to `model$evaluate()`? Then\nyou would override `test_step` in exactly the same way. Here's what it\nlooks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \"CustomModel\",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \"mse\", metrics = \"mae\")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n32/32 - 0s - loss: 1.2278 - mae: 1.0050 - 133ms/epoch - 4ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss      mae \n1.227754 1.005029 \n```\n:::\n:::\n\n\n## Wrapping up: an end-to-end GAN example\n\nLet's walk through an end-to-end example that leverages everything you\njust learned.\n\nLet's consider:\n\n-   A generator network meant to generate 28x28x1 images.\n-   A discriminator network meant to classify 28x28x1 images into two\n    classes (\"fake\" and \"real\").\n-   One optimizer for each.\n-   A loss function to train the discriminator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \"discriminator\",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \"generator\",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \"same\") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \"same\", activation = \"sigmoid\")\n```\n:::\n\n\nHere's a feature-complete GAN class, overriding `compile()` to use its\nown signature, and implementing the entire GAN algorithm in 17 lines in\n`train_step`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGAN <- new_model_class(\n  classname = \"GAN\",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \"all real images\"\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n```\n:::\n\n\nLet's test-drive it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\"float32\") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n100/100 - 4s - d_loss: 0.2360 - g_loss: 1.2044 - 4s/epoch - 44ms/step\n```\n:::\n:::\n\n\nHappy training!\n\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.13.0 (~/.virtualenvs/r-tensorflow-website/lib/python3.10/site-packages/tensorflow)\nPython v3.10 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/New_York\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tfdatasets_2.9.0.9000  keras_2.13.0.9000      tensorflow_2.13.0.9000\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.3            cli_3.6.1              knitr_1.43            \n [4] zeallot_0.1.0          rlang_1.1.1            xfun_0.40             \n [7] png_0.1-8              generics_0.1.3         jsonlite_1.8.7        \n[10] glue_1.6.2             htmltools_0.5.6        fansi_1.0.4           \n[13] rmarkdown_2.24         grid_4.3.1             tfruns_1.5.1          \n[16] evaluate_0.21          tibble_3.2.1           base64enc_0.1-3       \n[19] fastmap_1.1.1          yaml_2.3.7             lifecycle_1.0.3       \n[22] whisker_0.4.1          compiler_4.3.1         htmlwidgets_1.6.2     \n[25] Rcpp_1.0.11            pkgconfig_2.0.3        rstudioapi_0.15.0     \n[28] lattice_0.21-8         digest_0.6.33          R6_2.5.1              \n[31] tidyselect_1.2.0       reticulate_1.31.0.9000 utf8_1.2.3            \n[34] pillar_1.9.0           magrittr_2.0.3         Matrix_1.5-4.1        \n[37] tools_4.3.1           \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.4.0\narray-record==0.4.1\nasttokens==2.2.1\nastunparse==1.6.3\nbackcall==0.2.0\nbleach==6.0.0\ncachetools==5.3.1\ncertifi==2023.7.22\ncharset-normalizer==3.2.0\nclick==8.1.7\ndecorator==5.1.1\ndm-tree==0.1.8\netils==1.4.1\nexecuting==1.2.0\nflatbuffers==23.5.26\ngast==0.4.0\ngoogle-auth==2.22.0\ngoogle-auth-oauthlib==1.0.0\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.60.0\ngrpcio==1.57.0\nh5py==3.9.0\nidna==3.4\nimportlib-resources==6.0.1\nipython==8.14.0\njedi==0.19.0\nkaggle==1.5.16\nkeras==2.13.1\nkeras-tuner==1.3.5\nkt-legacy==1.0.5\nlibclang==16.0.6\nMarkdown==3.4.4\nMarkupSafe==2.1.3\nmatplotlib-inline==0.1.6\nnumpy==1.24.3\nnvidia-cublas-cu11==11.11.3.6\nnvidia-cudnn-cu11==8.6.0.163\noauthlib==3.2.2\nopt-einsum==3.3.0\npackaging==23.1\npandas==2.0.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==10.0.0\npromise==2.3\nprompt-toolkit==3.0.39\nprotobuf==3.20.3\npsutil==5.9.5\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.5.0\npyasn1-modules==0.3.0\npydot==1.4.2\nPygments==2.16.1\npyparsing==3.1.1\npython-dateutil==2.8.2\npython-slugify==8.0.1\npytz==2023.3\nrequests==2.31.0\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.11.2\nsix==1.16.0\nstack-data==0.6.2\ntensorboard==2.13.0\ntensorboard-data-server==0.7.1\ntensorflow==2.13.0\ntensorflow-datasets==4.9.2\ntensorflow-estimator==2.13.0\ntensorflow-hub==0.14.0\ntensorflow-io-gcs-filesystem==0.33.0\ntensorflow-metadata==1.14.0\ntermcolor==2.3.0\ntext-unidecode==1.3\ntoml==0.10.2\ntqdm==4.66.1\ntraitlets==5.9.0\ntyping_extensions==4.5.0\ntzdata==2023.3\nurllib3==1.26.16\nwcwidth==0.2.6\nwebencodings==0.5.1\nWerkzeug==2.3.7\nwrapt==1.15.0\nzipp==3.16.2\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2023-08-28 \nPage render time: 12 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}