{
  "hash": "b3e28630f1deb961d240e46d5842afb9",
  "result": {
    "markdown": "---\ntitle: \"Guide to Keras Basics\"\naliases:\n  - /articles/guide_keras.html\n  - /keras/\n---\n\n\nKeras is a high-level API to build and train deep learning models. It's\nused for fast prototyping, advanced research, and production, with three\nkey advantages:\n\n-   *User friendly* -- Keras has a simple, consistent interface\n    optimized for common use cases. It provides clear and actionable\n    feedback for user errors.\n-   *Modular and composable* -- Keras models are made by connecting\n    configurable building blocks together, with few restrictions.\n-   *Easy to extend* -- Write custom building blocks to express new\n    ideas for research. Create new layers, loss functions, and develop\n    state-of-the-art models.\n\n## Import keras\n\nTo get started, load the `keras` library:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n```\n:::\n\n\n## Build a simple model\n\n### Sequential model\n\nIn Keras, you assemble *layers* to build *models*. A model is (usually)\na graph of layers. The most common type of model is a stack of layers:\nthe `sequential` model.\n\nTo build a simple, fully-connected network (i.e., a multi-layer\nperceptron):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\n\nmodel %>%\n\n  # Adds a densely-connected layer with 64 units to the model:\n  layer_dense(units = 64, activation = 'relu') %>%\n\n  # Add another:\n  layer_dense(units = 64, activation = 'relu') %>%\n\n  # Add a softmax layer with 10 output units:\n  layer_dense(units = 10, activation = 'softmax')\n```\n:::\n\n\n### Configure the layers\n\nThere are many `layers` available with some common constructor\nparameters:\n\n-   `activation`: Set the [activation\n    function](https://tensorflow.rstudio.com/reference/keras/#section-activation-layers)\n    for the layer. By default, no activation is applied.\n-   `kernel_initializer` and `bias_initializer`: The initialization\n    schemes that create the layer's weights (kernel and bias). This\n    defaults to the\n    [`Glorot uniform`](https://tensorflow.rstudio.com/keras/reference/initializer_glorot_uniform.html)\n    initializer.\n-   `kernel_regularizer` and `bias_regularizer`: The regularization\n    schemes that apply to the layer's weights (kernel and bias), such as\n    L1 or L2 regularization. By default, no regularization is applied.\n\nThe following instantiates `dense` layers using constructor arguments:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sigmoid layer:\nlayer_dense(units = 64, activation ='sigmoid')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.src.layers.core.dense.Dense object at 0x7f954cab7be0>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\nlayer_dense(units = 64, kernel_regularizer = regularizer_l1(0.01))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in keras$regularizers$l1(l = l): partial argument match of 'l' to\n'l1'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.src.layers.core.dense.Dense object at 0x7f954cb74100>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\nlayer_dense(units = 64, bias_regularizer = regularizer_l2(0.01))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in keras$regularizers$l2(l = l): partial argument match of 'l' to\n'l2'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.src.layers.core.dense.Dense object at 0x7f954cb744f0>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with a kernel initialized to a random orthogonal matrix:\nlayer_dense(units = 64, kernel_initializer = 'orthogonal')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.src.layers.core.dense.Dense object at 0x7f954cb74070>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with a bias vector initialized to 2.0:\nlayer_dense(units = 64, bias_initializer = initializer_constant(2.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.src.layers.core.dense.Dense object at 0x7f954cb74c40>\n```\n:::\n:::\n\n\n## Train and evaluate\n\n### Set up training\n\nAfter the model is constructed, configure its learning process by\ncalling the `compile` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n```\n:::\n\n\n`compile` takes three important arguments:\n\n-   `optimizer`: This object specifies the training procedure. Commonly\n    used optimizers are e.g.\\\n    [`adam`](https://tensorflow.rstudio.com/keras/reference/optimizer_adam.html),\n    [`rmsprop`](https://tensorflow.rstudio.com/keras/reference/optimizer_rmsprop.html),\n    or\n    [`sgd`](https://tensorflow.rstudio.com/keras/reference/optimizer_sgd.html).\n-   `loss`: The function to minimize during optimization. Common choices\n    include mean square error (`mse`), `categorical_crossentropy`, and\n    `binary_crossentropy`.\n-   `metrics`: Used to monitor training. In classification, this usually\n    is accuracy.\n\nThe following shows a few examples of configuring a model for training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Configure a model for mean-squared error regression.\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'mse',           # mean squared error\n  metrics = list('mae')   # mean absolute error\n)\n\n# Configure a model for categorical classification.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 0.01),\n  loss = \"categorical_crossentropy\",\n  metrics = list(\"categorical_accuracy\")\n)\n```\n:::\n\n\n### Input data\n\nYou can train keras models directly on R matrices and arrays (possibly\ncreated from R `data.frames`). A model is fit to the training data using\nthe `fit` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n32/32 - 1s - loss: -2.6513e-01 - categorical_accuracy: 0.0970 - 802ms/epoch - 25ms/step\nEpoch 2/10\n32/32 - 0s - loss: -3.8759e+00 - categorical_accuracy: 0.1110 - 79ms/epoch - 2ms/step\nEpoch 3/10\n32/32 - 0s - loss: -1.8322e+01 - categorical_accuracy: 0.1020 - 55ms/epoch - 2ms/step\nEpoch 4/10\n32/32 - 0s - loss: -5.0000e+01 - categorical_accuracy: 0.1070 - 59ms/epoch - 2ms/step\nEpoch 5/10\n32/32 - 0s - loss: -1.1058e+02 - categorical_accuracy: 0.0810 - 59ms/epoch - 2ms/step\nEpoch 6/10\n32/32 - 0s - loss: -1.6624e+02 - categorical_accuracy: 0.1150 - 59ms/epoch - 2ms/step\nEpoch 7/10\n32/32 - 0s - loss: -2.8676e+02 - categorical_accuracy: 0.1070 - 55ms/epoch - 2ms/step\nEpoch 8/10\n32/32 - 0s - loss: -4.4149e+02 - categorical_accuracy: 0.1140 - 57ms/epoch - 2ms/step\nEpoch 9/10\n32/32 - 0s - loss: -6.6879e+02 - categorical_accuracy: 0.1170 - 64ms/epoch - 2ms/step\nEpoch 10/10\n32/32 - 0s - loss: -8.8606e+02 - categorical_accuracy: 0.1050 - 61ms/epoch - 2ms/step\n```\n:::\n:::\n\n\n`fit` takes three important arguments:\n\n-   `epochs`: Training is structured into *epochs*. An epoch is one\n    iteration over the entire input data (this is done in smaller\n    batches).\n-   `batch_size`: When passed matrix or array data, the model slices the\n    data into smaller batches and iterates over these batches during\n    training. This integer specifies the size of each batch. Be aware\n    that the last batch may be smaller if the total number of samples is\n    not divisible by the batch size.\n-   `validation_data`: When prototyping a model, you want to easily\n    monitor its performance on some validation data. Passing this\n    argument --- a list of inputs and labels --- allows the model to\n    display the loss and metrics in inference mode for the passed data,\n    at the end of each epoch.\n\nHere's an example using `validation_data`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nval_data <- matrix(rnorm(1000 * 32), nrow = 100, ncol = 32)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in matrix(rnorm(1000 * 32), nrow = 100, ncol = 32): data length\ndiffers from size of matrix: [32000 != 100 x 32]\n```\n:::\n\n```{.r .cell-code}\nval_labels <- matrix(rnorm(100 * 10), nrow = 100, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(val_data, val_labels)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n32/32 - 0s - loss: 377.5257 - categorical_accuracy: 0.1010 - val_loss: -9.1629e+02 - val_categorical_accuracy: 0.1500 - 184ms/epoch - 6ms/step\nEpoch 2/10\n32/32 - 0s - loss: 268.6779 - categorical_accuracy: 0.1070 - val_loss: -9.0091e+02 - val_categorical_accuracy: 0.1400 - 80ms/epoch - 2ms/step\nEpoch 3/10\n32/32 - 0s - loss: 17.4030 - categorical_accuracy: 0.1140 - val_loss: -1.0150e+03 - val_categorical_accuracy: 0.1400 - 76ms/epoch - 2ms/step\nEpoch 4/10\n32/32 - 0s - loss: -5.5530e+01 - categorical_accuracy: 0.1070 - val_loss: -1.1427e+03 - val_categorical_accuracy: 0.1100 - 74ms/epoch - 2ms/step\nEpoch 5/10\n32/32 - 0s - loss: 49.4249 - categorical_accuracy: 0.0810 - val_loss: -1.1145e+03 - val_categorical_accuracy: 0.1500 - 74ms/epoch - 2ms/step\nEpoch 6/10\n32/32 - 0s - loss: -8.4104e+01 - categorical_accuracy: 0.1090 - val_loss: -1.6088e+03 - val_categorical_accuracy: 0.0800 - 75ms/epoch - 2ms/step\nEpoch 7/10\n32/32 - 0s - loss: -2.5985e+02 - categorical_accuracy: 0.0960 - val_loss: -1.9842e+03 - val_categorical_accuracy: 0.1100 - 82ms/epoch - 3ms/step\nEpoch 8/10\n32/32 - 0s - loss: -6.4850e+02 - categorical_accuracy: 0.0980 - val_loss: -1.8305e+03 - val_categorical_accuracy: 0.0900 - 86ms/epoch - 3ms/step\nEpoch 9/10\n32/32 - 0s - loss: -6.2961e+02 - categorical_accuracy: 0.1050 - val_loss: -3.0540e+03 - val_categorical_accuracy: 0.1200 - 75ms/epoch - 2ms/step\nEpoch 10/10\n32/32 - 0s - loss: -1.4109e+03 - categorical_accuracy: 0.0980 - val_loss: -4.7981e+03 - val_categorical_accuracy: 0.0800 - 78ms/epoch - 2ms/step\n```\n:::\n:::\n\n\n### Evaluate and predict\n\nSame as `fit`, the `evaluate` and `predict` methods can use raw R data\nas well as a `dataset`.\n\nTo *evaluate* the inference-mode loss and metrics for the data provided:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(test_data, test_labels, batch_size = 32)\n\nmodel %>% evaluate(test_dataset, steps = 30)\n```\n:::\n\n\nAnd to *predict* the output of the last layer in inference for the data\nprovided, again as R data as well as a `dataset`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% predict(test_data, batch_size = 32)\n\nmodel %>% predict(test_dataset, steps = 30)\n```\n:::\n\n\n## Build advanced models\n\n### Functional API\n\nThe `sequential` model is a simple stack of layers that cannot represent\narbitrary models. Use the [Keras functional API](functional_api.html) to\nbuild complex model topologies such as:\n\n-   multi-input models,\n-   multi-output models,\n-   models with shared layers (the same layer called several times),\n-   models with non-sequential data flows (e.g., residual connections).\n\nBuilding a model with the functional API works like this:\n\n1.  A layer instance is callable and returns a tensor.\n2.  Input tensors and output tensors are used to define a `keras_model`\n    instance.\n3.  This model is trained just like the `sequential` model.\n\nThe following example uses the functional API to build a simple,\nfully-connected network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = (32))  # Returns a placeholder tensor\n\npredictions <- inputs %>%\n  layer_dense(units = 64, activation = 'relu') %>%\n  layer_dense(units = 64, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\n# Instantiate the model given inputs and outputs.\nmodel <- keras_model(inputs = inputs, outputs = predictions)\n\n# The compile step specifies the training configuration.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n32/32 - 1s - loss: 0.4507 - accuracy: 0.0960 - 586ms/epoch - 18ms/step\nEpoch 2/5\n32/32 - 0s - loss: 0.1948 - accuracy: 0.1240 - 54ms/epoch - 2ms/step\nEpoch 3/5\n32/32 - 0s - loss: -3.9052e-03 - accuracy: 0.1500 - 55ms/epoch - 2ms/step\nEpoch 4/5\n32/32 - 0s - loss: -2.1561e-01 - accuracy: 0.1430 - 126ms/epoch - 4ms/step\nEpoch 5/5\n32/32 - 0s - loss: -4.4740e-01 - accuracy: 0.1530 - 61ms/epoch - 2ms/step\n```\n:::\n:::\n\n\n### Custom layers\n\nTo create a custom Keras layer, you create an R6 class derived from\n`KerasLayer`. There are three methods to implement (only one of which,\n`call()`, is required for all types of layer):\n\n-   `build(input_shape)`: This is where you will define your weights.\n    Note that if your layer doesn't define trainable weights then you\n    need not implement this method.\n-   `call(x)`: This is where the layer's logic lives. Unless you want\n    your layer to support masking, you only have to care about the first\n    argument passed to call: the input tensor.\n-   `compute_output_shape(input_shape)`: In case your layer modifies the\n    shape of its input, you should specify here the shape transformation\n    logic. This allows Keras to do automatic shape inference. If you\n    don't modify the shape of the input then you need not implement this\n    method.\n\nHere is an example custom layer that performs a matrix multiplication:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\"CustomLayer\",\n\n  inherit = KerasLayer,\n\n  public = list(\n\n    output_dim = NULL,\n\n    kernel = NULL,\n\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n)\n```\n:::\n\n\nIn order to use the custom layer within a Keras model you also need to\ncreate a wrapper function which instantiates the layer using the\n`create_layer()` function. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define layer wrapper function\nlayer_custom <- function(object, output_dim, name = NULL, trainable = TRUE) {\n  create_layer(CustomLayer, object, list(\n    output_dim = as.integer(output_dim),\n    name = name,\n    trainable = trainable\n  ))\n}\n```\n:::\n\n\nYou can now use the layer in a model as usual:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 32, input_shape = c(32,32)) %>%\n  layer_custom(output_dim = 32)\n```\n:::\n\n\n### Custom models\n\nIn addition to creating custom layers, you can also create a custom\nmodel. This might be necessary if you wanted to use TensorFlow eager\nexecution in combination with an imperatively written forward pass.\n\nIn cases where this is not needed, but flexibility in building the\narchitecture is required, it is recommended to just stick with the\nfunctional API.\n\nA custom model is defined by calling `keras_model_custom()` passing a\nfunction that specifies the layers to be created and the operations to\nbe executed on forward pass.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define a custom model type\nmy_model_constructor <- new_model_class(\n  \"MyModel\",\n\n  initialize = function(output_dim, ...) {\n    super$initialize(...)\n    # store our output dim in self until build() is called\n    self$output_dim <- output_dim\n  },\n\n  build = function(input_shape) {\n    # create layers we'll need for the call (this code executes once)\n    # note: the layers have to be created on the self object!\n    self$dense1 <- layer_dense(units = 64,\n                               activation = 'relu',\n                               input_shape = input_shape)\n    self$dense2 <- layer_dense(units = 64, activation = 'relu')\n    self$dense3 <- layer_dense(units = self$output_dim, activation = 'softmax')\n  },\n\n  # implement call (this code executes during training & inference)\n  call = function(inputs) {\n    x <- inputs %>%\n      self$dense1() %>%\n      self$dense2() %>%\n      self$dense3()\n    x\n  },\n\n  # define a `get_config()` method in custom objects\n  # to enable model saving and restoring\n  get_config = function() {\n    list(output_dim = self$output_dim)\n  }\n)\n\n\n\nmodel <- my_model_constructor(output_dim = 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n32/32 - 1s - loss: 0.4321 - accuracy: 0.0980 - 653ms/epoch - 20ms/step\nEpoch 2/5\n32/32 - 0s - loss: 0.1700 - accuracy: 0.1190 - 61ms/epoch - 2ms/step\nEpoch 3/5\n32/32 - 0s - loss: -3.2296e-02 - accuracy: 0.1220 - 58ms/epoch - 2ms/step\nEpoch 4/5\n32/32 - 0s - loss: -2.6427e-01 - accuracy: 0.1290 - 56ms/epoch - 2ms/step\nEpoch 5/5\n32/32 - 0s - loss: -5.0211e-01 - accuracy: 0.1330 - 53ms/epoch - 2ms/step\n```\n:::\n:::\n\n\n## Callbacks\n\nA callback is an object passed to a model to customize and extend its\nbehavior during training. You can write your own custom callback, or use\nthe built-in `callbacks` that include:\n\n-   `callback_model_checkpoint`: Save checkpoints of your model at\n    regular intervals.\n-   `callback_learning_rate_scheduler`: Dynamically change the learning\n    rate.\n-   `callback_early_stopping`: Interrupt training when validation\n    performance has stopped improving.\n-   `callbacks_tensorboard`: Monitor the model's behavior using\n    [TensorBoard](training_visualization.html#tensorboard).\n\nTo use a `callback`, pass it to the model's `fit` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncallbacks <- list(\n  callback_early_stopping(patience = 2, monitor = 'val_loss'),\n  callback_tensorboard(log_dir = './logs')\n)\n\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5,\n  callbacks = callbacks,\n  validation_data = list(val_data, val_labels)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n32/32 - 0s - loss: -7.4056e-01 - accuracy: 0.1470 - val_loss: -1.5806e+00 - val_accuracy: 0.0800 - 199ms/epoch - 6ms/step\nEpoch 2/5\n32/32 - 0s - loss: -1.0141e+00 - accuracy: 0.1400 - val_loss: -1.6356e+00 - val_accuracy: 0.1300 - 96ms/epoch - 3ms/step\nEpoch 3/5\n32/32 - 0s - loss: -1.2859e+00 - accuracy: 0.1340 - val_loss: -1.7233e+00 - val_accuracy: 0.1100 - 94ms/epoch - 3ms/step\nEpoch 4/5\n32/32 - 0s - loss: -1.5790e+00 - accuracy: 0.1350 - val_loss: -1.6630e+00 - val_accuracy: 0.0800 - 88ms/epoch - 3ms/step\nEpoch 5/5\n32/32 - 0s - loss: -1.8551e+00 - accuracy: 0.1450 - val_loss: -1.8464e+00 - val_accuracy: 0.0500 - 89ms/epoch - 3ms/step\n```\n:::\n:::\n\n\n## Save and restore\n\n### Weights only\n\nSave and load the weights of a model using `save_model_weights_hdf5` and\n`load_model_weights_hdf5`, respectively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save in SavedModel format\nmodel %>% save_model_weights_tf('my_model/')\n\n# Restore the model's state,\n# this requires a model with the same architecture.\nmodel %>% load_model_weights_tf('my_model/')\n```\n:::\n\n\n### Configuration only\n\nA model's configuration can be saved - this serializes the model\narchitecture without any weights. A saved configuration can recreate and\ninitialize the same model, even without the code that defined the\noriginal model. Keras supports JSON and YAML serialization formats:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Serialize a model to JSON format\njson_string <- model %>% model_to_json()\n\n# Recreate the model (freshly initialized)\nfresh_model <- model_from_json(json_string,\n                               custom_objects = list('MyModel' = my_model_constructor))\n```\n:::\n\n\n### Entire model\n\nThe entire model can be saved to a file that contains the weight values,\nthe model's configuration, and even the optimizer's configuration. This\nallows you to checkpoint a model and resume training later ---from the\nexact same state ---without access to the original code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save entire model to the SavedModel format\nmodel %>% save_model_tf('my_model/')\n\n# Recreate the exact same model, including weights and optimizer.\nmodel <- load_model_tf('my_model/')\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}