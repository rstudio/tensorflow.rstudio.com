{
  "hash": "256b5bd855d1ef84d175e1ddc5160846",
  "result": {
    "markdown": "---\ntitle: The Functional API\nAuthor: \"[fchollet](https://twitter.com/fchollet)\"\ndate-created: 2019/03/01\ndate-last-modified: 2020/04/12\ndescription: Complete guide to the Functional API.\naliases:\n  - ../../articles/functional_api.html\n  - ../../guide/keras/functional_api/index.html\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Introduction\n\nThe Keras *functional API* is a way to create models that are more\nflexible than the sequential API. The functional API can handle models\nwith non-linear topology, shared layers, and even multiple inputs or\noutputs.\n\nThe main idea is that a deep learning model is usually a directed\nacyclic graph (DAG) of layers. So the functional API is a way to build\n*graphs of layers*.\n\nConsider the following model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(input: 784-dimensional vectors)\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (10 units, softmax activation)]\n       ↧\n(output: logits of a probability distribution over 10 classes)\n```\n:::\n\n\nThis is a basic graph with three layers. To build this model using the\nfunctional API, start by creating an input node:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = c(784))\n```\n:::\n\n\nThe shape of the data is set as a 784-dimensional vector. The batch size\nis always omitted since only the shape of each sample is specified.\n\nIf, for example, you have an image input with a shape of `(32, 32, 3)`,\nyou would use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Just for demonstration purposes.\nimg_inputs <- layer_input(shape = c(32, 32, 3))\n```\n:::\n\n\nThe `inputs` that is returned contains information about the shape and\n`dtype` of the input data that you feed to your model. Here's the shape:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([None, 784])\n```\n:::\n:::\n\n\nHere's the dtype:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.float32\n```\n:::\n:::\n\n\nYou create a new node in the graph of layers by calling a layer on this\n`inputs` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndense <- layer_dense(units = 64, activation = \"relu\")\nx <- dense(inputs)\n```\n:::\n\n\nThe \"layer call\" action is like drawing an arrow from \"inputs\" to this\nlayer you created. You're \"passing\" the inputs to the `dense` layer, and\nyou get `x` as the output.\n\nYou can also conveniently create the layer and compose it with `inputs`\nin one step, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- inputs %>%\n  layer_dense(units = 64, activation = \"relu\")\n```\n:::\n\n\nLet's add a few more layers to the graph of layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutputs <- x %>%\n  layer_dense(64, activation = \"relu\") %>%\n  layer_dense(10)\n```\n:::\n\n\nAt this point, you can create a `Model` by specifying its inputs and\noutputs in the graph of layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model(inputs = inputs, outputs = outputs,\n                     name = \"mnist_model\")\n```\n:::\n\n\nLet's check out what the model summary looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"mnist_model\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n input_1 (InputLayer)             [(None, 784)]                 0           \n dense_1 (Dense)                  (None, 64)                    50240       \n dense_3 (Dense)                  (None, 64)                    4160        \n dense_2 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 55050 (215.04 KB)\nTrainable params: 55050 (215.04 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n:::\n\n\nYou can also plot the model as a graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAnd, optionally, display the input and output shapes of each layer in\nthe plotted graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, show_shapes = TRUE)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThis figure and the code are almost identical. In the code version, the\nconnection arrows are replaced by `%>%` operator.\n\nA \"graph of layers\" is an intuitive mental image for a deep learning\nmodel, and the functional API is a way to create models that closely\nmirrors this.\n\n## Training, evaluation, and inference\n\nTraining, evaluation, and inference work exactly in the same way for\nmodels built using the functional API as for `Sequential` models.\n\nThe `Model` class offers a built-in training loop (the `fit()` method)\nand a built-in evaluation loop (the `evaluate()` method). Note that you\ncan easily [customize these\nloops](/guides/customizing_what_happens_in_fit/) to implement training\nroutines beyond supervised learning (e.g.\n[GANs](/examples/generative/dcgan_overriding_train_step/)).\n\nHere, load the MNIST image data, reshape it into vectors, fit the model\non the data (while monitoring performance on a validation split), then\nevaluate the model on the test data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n\nx_train <- array_reshape(x_train, c(60000, 784)) / 255\nx_test <-  array_reshape(x_test, c(10000, 784)) / 255\n\nmodel %>% compile(\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_rmsprop(),\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, batch_size = 64, epochs = 2, validation_split = 0.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/2\n750/750 - 2s - loss: 0.3521 - accuracy: 0.9020 - val_loss: 0.1825 - val_accuracy: 0.9477 - 2s/epoch - 3ms/step\nEpoch 2/2\n750/750 - 1s - loss: 0.1668 - accuracy: 0.9510 - val_loss: 0.1585 - val_accuracy: 0.9538 - 1s/epoch - 2ms/step\n```\n:::\n\n```{.r .cell-code}\ntest_scores <- model %>% evaluate(x_test, y_test, verbose = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.1489 - accuracy: 0.9510 - 351ms/epoch - 1ms/step\n```\n:::\n\n```{.r .cell-code}\nprint(test_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.1489451 0.9510000 \n```\n:::\n:::\n\n\nFor further reading, see the [training and\nevaluation](/guides/training_with_built_in_methods/) guide.\n\n## Save and serialize\n\nSaving the model and serialization work the same way for models built\nusing the functional API as they do for `Sequential` models. The\nstandard way to save a functional model is to call `save_model_tf()` to\nsave the entire model as a single file. You can later recreate the same\nmodel from this file, even if the code that built the model is no longer\navailable.\n\nThis saved file includes the: - model architecture - model weight values\n(that were learned during training) - model training config, if any (as\npassed to `compile`) - optimizer and its state, if any (to restart\ntraining where you left off)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_to_my_model <- tempfile()\nsave_model_tf(model, path_to_my_model)\n\nrm(model)\n# Recreate the exact same model purely from the file:\nmodel <- load_model_tf(path_to_my_model)\n```\n:::\n\n\nFor details, read the model [serialization &\nsaving](/guides/serialization_and_saving/) guide.\n\n## Use the same graph of layers to define multiple models\n\nIn the functional API, models are created by specifying their inputs and\noutputs in a graph of layers. That means that a single graph of layers\ncan be used to generate multiple models.\n\nIn the example below, you use the same stack of layers to instantiate\ntwo models: an `encoder` model that turns image inputs into\n16-dimensional vectors, and an end-to-end `autoencoder` model for\ntraining.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_input <- layer_input(shape = c(28, 28, 1),\n                             name = \"img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output,\n                       name = \"encoder\")\nencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"encoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n conv2d_3 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_2 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d (MaxPooling2D)     (None, 8, 8, 32)              0           \n conv2d_1 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d (Conv2D)                  (None, 4, 4, 16)              4624        \n global_max_pooling2d (GlobalMax  (None, 16)                    0           \n Pooling2D)                                                                 \n============================================================================\nTotal params: 18672 (72.94 KB)\nTrainable params: 18672 (72.94 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\ndecoder_output <- encoder_output %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\nautoencoder <- keras_model(encoder_input, decoder_output,\n                           name = \"autoencoder\")\nautoencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"autoencoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n conv2d_3 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_2 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d (MaxPooling2D)     (None, 8, 8, 32)              0           \n conv2d_1 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d (Conv2D)                  (None, 4, 4, 16)              4624        \n global_max_pooling2d (GlobalMax  (None, 16)                    0           \n Pooling2D)                                                                 \n reshape (Reshape)                (None, 4, 4, 1)               0           \n conv2d_transpose_3 (Conv2DTrans  (None, 6, 6, 16)              160         \n pose)                                                                      \n conv2d_transpose_2 (Conv2DTrans  (None, 8, 8, 32)              4640        \n pose)                                                                      \n up_sampling2d (UpSampling2D)     (None, 24, 24, 32)            0           \n conv2d_transpose_1 (Conv2DTrans  (None, 26, 26, 16)            4624        \n pose)                                                                      \n conv2d_transpose (Conv2DTranspo  (None, 28, 28, 1)             145         \n se)                                                                        \n============================================================================\nTotal params: 28241 (110.32 KB)\nTrainable params: 28241 (110.32 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n:::\n\n\nHere, the decoding architecture is strictly symmetrical to the encoding\narchitecture, so the output shape is the same as the input shape\n`(28, 28, 1)`.\n\nThe reverse of a `Conv2D` layer is a `Conv2DTranspose` layer, and the\nreverse of a `MaxPooling2D` layer is an `UpSampling2D` layer.\n\n## All models are callable, just like layers\n\nYou can treat any model as if it were a layer by invoking it on an\n`Input` or on the output of another layer. By calling a model you aren't\njust reusing the architecture of the model, you're also reusing its\nweights.\n\nTo see this in action, here's a different take on the autoencoder\nexample that creates an encoder model, a decoder model, and chains them\nin two calls to obtain the autoencoder model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_input <- layer_input(shape = c(28, 28, 1), name = \"original_img\")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(16, 3, activation = \"relu\") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, name = \"encoder\")\nencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"encoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n original_img (InputLayer)        [(None, 28, 28, 1)]           0           \n conv2d_7 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_6 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d_1 (MaxPooling2D)   (None, 8, 8, 32)              0           \n conv2d_5 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d_4 (Conv2D)                (None, 4, 4, 16)              4624        \n global_max_pooling2d_1 (GlobalM  (None, 16)                    0           \n axPooling2D)                                                               \n============================================================================\nTotal params: 18672 (72.94 KB)\nTrainable params: 18672 (72.94 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\ndecoder_input <- layer_input(shape = c(16), name = \"encoded_img\")\ndecoder_output <- decoder_input %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%\n  layer_conv_2d_transpose(1, 3, activation = \"relu\")\n\ndecoder <- keras_model(decoder_input, decoder_output,\n                       name = \"decoder\")\ndecoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"decoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n encoded_img (InputLayer)         [(None, 16)]                  0           \n reshape_1 (Reshape)              (None, 4, 4, 1)               0           \n conv2d_transpose_7 (Conv2DTrans  (None, 6, 6, 16)              160         \n pose)                                                                      \n conv2d_transpose_6 (Conv2DTrans  (None, 8, 8, 32)              4640        \n pose)                                                                      \n up_sampling2d_1 (UpSampling2D)   (None, 24, 24, 32)            0           \n conv2d_transpose_5 (Conv2DTrans  (None, 26, 26, 16)            4624        \n pose)                                                                      \n conv2d_transpose_4 (Conv2DTrans  (None, 28, 28, 1)             145         \n pose)                                                                      \n============================================================================\nTotal params: 9569 (37.38 KB)\nTrainable params: 9569 (37.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nautoencoder_input <- layer_input(shape = c(28, 28, 1), name = \"img\")\nencoded_img <- encoder(autoencoder_input)\ndecoded_img <- decoder(encoded_img)\nautoencoder <- keras_model(autoencoder_input, decoded_img,\n                           name = \"autoencoder\")\nautoencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"autoencoder\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n encoder (Functional)             (None, 16)                    18672       \n decoder (Functional)             (None, 28, 28, 1)             9569        \n============================================================================\nTotal params: 28241 (110.32 KB)\nTrainable params: 28241 (110.32 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n:::\n\n\nAs you can see, the model can be nested: a model can contain sub-models\n(since a model is just like a layer). A common use case for model\nnesting is *ensembling*. For example, here's how to ensemble a set of\nmodels into a single model that averages their predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  inputs <- layer_input(shape = c(128))\n  outputs <- inputs %>% layer_dense(1)\n  keras_model(inputs, outputs)\n}\n\nmodel1 <- get_model()\nmodel2 <- get_model()\nmodel3 <- get_model()\n\ninputs <- layer_input(shape = c(128))\ny1 <- model1(inputs)\ny2 <- model2(inputs)\ny3 <- model3(inputs)\noutputs <- layer_average(list(y1, y2, y3))\nensemble_model <- keras_model(inputs = inputs, outputs = outputs)\n```\n:::\n\n\n## Manipulate complex graph topologies\n\n### Models with multiple inputs and outputs\n\nThe functional API makes it easy to manipulate multiple inputs and\noutputs. This cannot be handled with the `Sequential` API.\n\nFor example, if you're building a system for ranking customer issue\ntickets by priority and routing them to the correct department, then the\nmodel will have three inputs:\n\n-   the title of the ticket (text input),\n-   the text body of the ticket (text input), and\n-   any tags added by the user (categorical input)\n\nThis model will have two outputs:\n\n-   the priority score between 0 and 1 (scalar sigmoid output), and\n-   the department that should handle the ticket (softmax output over\n    the set of departments).\n\nYou can build this model in a few lines with the functional API:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_tags <- 12  # Number of unique issue tags\nnum_words <- 10000  # Size of vocabulary obtained when preprocessing text data\nnum_departments <- 4  # Number of departments for predictions\n\ntitle_input <- layer_input(shape = c(NA), name = \"title\")  # Variable-length sequence of ints\nbody_input <- layer_input(shape = c(NA), name = \"body\")  # Variable-length sequence of ints\ntags_input <- layer_input(shape = c(num_tags), name = \"tags\")  # Binary vectors of size `num_tags`\n\n\n# Embed each word in the title into a 64-dimensional vector\ntitle_features <- title_input %>% layer_embedding(num_words, 64)\n\n# Embed each word in the text into a 64-dimensional vector\nbody_features <- body_input %>% layer_embedding(num_words, 64)\n\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\ntitle_features <- title_features %>% layer_lstm(128)\n\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\nbody_features <- body_features %>% layer_lstm(32)\n\n# Merge all available features into a single large vector via concatenation\nx <- layer_concatenate(list(title_features, body_features, tags_input))\n\n# Stick a logistic regression for priority prediction on top of the features\npriority_pred <- x %>% layer_dense(1, name = \"priority\")\n\n# Stick a department classifier on top of the features\ndepartment_pred <- x %>% layer_dense(num_departments, name = \"department\")\n\n# Instantiate an end-to-end model predicting both priority and department\nmodel <- keras_model(\n  inputs <- list(title_input, body_input, tags_input),\n  outputs <- list(priority_pred, department_pred)\n)\n```\n:::\n\n\nNow plot the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, show_shapes = TRUE)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nWhen compiling this model, you can assign different losses to each\noutput. You can even assign different weights to each loss -- to\nmodulate their contribution to the total training loss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_binary_crossentropy(from_logits = TRUE),\n    loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights <- c(1, 0.2)\n)\n```\n:::\n\n\nSince the output layers have different names, you could also specify the\nlosses and loss weights with the corresponding layer names:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    priority = loss_binary_crossentropy(from_logits = TRUE),\n    department = loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights = c(priority =  1.0, department = 0.2),\n)\n```\n:::\n\n\nTrain the model by passing lists of NumPy arrays of inputs and targets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# some helpers to generate dummy input data\nrandom_uniform_array <- function(dim)\n  array(runif(prod(dim)), dim)\n\nrandom_vectorized_array <- function(num_words, dim)\n  array(sample(0:(num_words - 1), prod(dim), replace = TRUE), dim)\n\n# Dummy input data\ntitle_data <- random_vectorized_array(num_words, c(1280, 10))\nbody_data <- random_vectorized_array(num_words, c(1280, 100))\ntags_data <- random_vectorized_array(2, c(1280, num_tags))\n# storage.mode(tags_data) <- \"double\" # from integer\n\n# Dummy target data\npriority_targets <- random_uniform_array(c(1280, 1))\ndept_targets <- random_vectorized_array(2, c(1280, num_departments))\n\nmodel %>% fit(\n  list(title = title_data, body = body_data, tags = tags_data),\n  list(priority = priority_targets, department = dept_targets),\n  epochs = 2,\n  batch_size = 32\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/2\n40/40 - 5s - loss: 1.2531 - priority_loss: 0.7012 - department_loss: 2.7599 - 5s/epoch - 137ms/step\nEpoch 2/2\n40/40 - 2s - loss: 1.2548 - priority_loss: 0.7022 - department_loss: 2.7629 - 2s/epoch - 50ms/step\n```\n:::\n:::\n\n\nWhen calling fit with a `tfdataset` object, it should yield either a\ntuple of lists like\n`tuple(list(title_data, body_data, tags_data), list(priority_targets, dept_targets))`\nor a tuple of named lists like\n`tuple(list(title = title_data, body = body_data, tags = tags_data), list(priority= priority_targets, department= dept_targets))`.\n\nFor more detailed explanation, refer to the [training and\nevaluation](/guides/training_with_built_in_methods/) guide.\n\n### A toy ResNet model\n\nIn addition to models with multiple inputs and outputs, the functional\nAPI makes it easy to manipulate non-linear connectivity topologies --\nthese are models with layers that are not connected sequentially, which\nthe `Sequential` API cannot handle.\n\nA common use case for this is residual connections. Let's build a toy\nResNet model for CIFAR10 to demonstrate this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = c(32, 32, 3), name = \"img\")\nblock_1_output <- inputs %>%\n  layer_conv_2d(32, 3, activation = \"relu\") %>%\n  layer_conv_2d(64, 3, activation = \"relu\") %>%\n  layer_max_pooling_2d(3)\n\nblock_2_output <- block_1_output %>%\n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>%\n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>%\n  layer_add(block_1_output)\n\nblock_3_output <- block_2_output %>%\n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>%\n  layer_conv_2d(64, 3, activation = \"relu\", padding = \"same\") %>%\n  layer_add(block_2_output)\n\noutputs <- block_3_output %>%\n  layer_conv_2d(64, 3, activation = \"relu\") %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(10)\n\nmodel <- keras_model(inputs, outputs, name = \"toy_resnet\")\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"toy_resnet\"\n____________________________________________________________________________\n Layer (type)         Output Shape           Param   Connected to           \n                                              #                             \n============================================================================\n img (InputLayer)     [(None, 32, 32, 3)]    0       []                     \n conv2d_9 (Conv2D)    (None, 30, 30, 32)     896     ['img[0][0]']          \n conv2d_8 (Conv2D)    (None, 28, 28, 64)     18496   ['conv2d_9[0][0]']     \n max_pooling2d_2 (Ma  (None, 9, 9, 64)       0       ['conv2d_8[0][0]']     \n xPooling2D)                                                                \n conv2d_11 (Conv2D)   (None, 9, 9, 64)       36928   ['max_pooling2d_2[0][0]\n                                                     ']                     \n conv2d_10 (Conv2D)   (None, 9, 9, 64)       36928   ['conv2d_11[0][0]']    \n add (Add)            (None, 9, 9, 64)       0       ['conv2d_10[0][0]',    \n                                                      'max_pooling2d_2[0][0]\n                                                     ']                     \n conv2d_13 (Conv2D)   (None, 9, 9, 64)       36928   ['add[0][0]']          \n conv2d_12 (Conv2D)   (None, 9, 9, 64)       36928   ['conv2d_13[0][0]']    \n add_1 (Add)          (None, 9, 9, 64)       0       ['conv2d_12[0][0]',    \n                                                      'add[0][0]']          \n conv2d_14 (Conv2D)   (None, 7, 7, 64)       36928   ['add_1[0][0]']        \n global_average_pool  (None, 64)             0       ['conv2d_14[0][0]']    \n ing2d (GlobalAverag                                                        \n ePooling2D)                                                                \n dense_8 (Dense)      (None, 256)            16640   ['global_average_poolin\n                                                     g2d[0][0]']            \n dropout (Dropout)    (None, 256)            0       ['dense_8[0][0]']      \n dense_7 (Dense)      (None, 10)             2570    ['dropout[0][0]']      \n============================================================================\nTotal params: 223242 (872.04 KB)\nTrainable params: 223242 (872.04 KB)\nNon-trainable params: 0 (0.00 Byte)\n____________________________________________________________________________\n```\n:::\n:::\n\n\nPlot the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, show_shapes = TRUE)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nNow train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()\n\nx_train <- x_train / 255\nx_test <- x_test / 255\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = loss_categorical_crossentropy(from_logits = TRUE),\n  metrics = \"acc\"\n)\n# We restrict the data to the first 1000 samples so as to limit execution time\n# for this guide. Try to train on the entire dataset until convergence!\nmodel %>% fit(\n  x_train[1:1000, , , ],\n  y_train[1:1000, ],\n  batch_size = 64,\n  epochs = 1,\n  validation_split = 0.2\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n13/13 - 2s - loss: 2.3036 - acc: 0.1013 - val_loss: 2.2991 - val_acc: 0.1250 - 2s/epoch - 166ms/step\n```\n:::\n:::\n\n\n## Shared layers\n\nAnother good use for the functional API are models that use *shared\nlayers*. Shared layers are layer instances that are reused multiple\ntimes in the same model -- they learn features that correspond to\nmultiple paths in the graph-of-layers.\n\nShared layers are often used to encode inputs from similar spaces (say,\ntwo different pieces of text that feature similar vocabulary). They\nenable sharing of information across these different inputs, and they\nmake it possible to train such a model on less data. If a given word is\nseen in one of the inputs, that will benefit the processing of all\ninputs that pass through the shared layer.\n\nTo share a layer in the functional API, call the same layer instance\nmultiple times. For instance, here's an `Embedding` layer shared across\ntwo different text inputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding <- layer_embedding(input_dim = 1000, output_dim = 128)\n\n# Variable-length sequence of integers\ntext_input_a <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Variable-length sequence of integers\ntext_input_b <- layer_input(shape = c(NA), dtype = \"int32\")\n\n# Reuse the same layer to encode both inputs\nencoded_input_a <- shared_embedding(text_input_a)\nencoded_input_b <- shared_embedding(text_input_b)\n```\n:::\n\n\n## Extract and reuse nodes in the graph of layers\n\nBecause the graph of layers you are manipulating is a static data\nstructure, it can be accessed and inspected. And this is how you are\nable to plot functional models as images.\n\nThis also means that you can access the activations of intermediate\nlayers (\"nodes\" in the graph) and reuse them elsewhere -- which is very\nuseful for something like feature extraction.\n\nLet's look at an example. This is a VGG19 model with weights pretrained\non ImageNet:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvgg19 <- application_vgg19()\n```\n:::\n\n\nAnd these are the intermediate activations of the model, obtained by\nquerying the graph data structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\n```\n:::\n\n\nUse these features to create a new feature-extraction model that returns\nthe values of the intermediate layer activations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeat_extraction_model <-  keras_model(inputs = vgg19$input,\n                                      outputs = features_list)\n\nimg <- random_uniform_array(c(1, 224, 224, 3))\nextracted_features <- feat_extraction_model(img)\n```\n:::\n\n\nThis comes in handy for tasks like [neural style\ntransfer](https://keras$io/examples/generative/neural_style_transfer/),\namong other things.\n\n## Extend the API using custom layers\n\n`tf$keras` includes a wide range of built-in layers, for example:\n\n-   Convolutional layers: `Conv1D`, `Conv2D`, `Conv3D`,\n    `Conv2DTranspose`\n-   Pooling layers: `MaxPooling1D`, `MaxPooling2D`, `MaxPooling3D`,\n    `AveragePooling1D`\n-   RNN layers: `GRU`, `LSTM`, `ConvLSTM2D`\n-   `BatchNormalization`, `Dropout`, `Embedding`, etc.\n\nBut if you don't find what you need, it's easy to extend the API by\ncreating your own layers. All layers subclass the `Layer` class and\nimplement:\n\n-   `call` method, that specifies the computation done by the layer.\n-   `build` method, that creates the weights of the layer (this is just\n    a style convention since you can create weights in `__init__`, as\n    well).\n\nTo learn more about creating layers from scratch, read [custom layers\nand models](/guides/making_new_layers_and_models_via_subclassing) guide.\n\nThe following is a basic implementation of `layer_dense()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units = as.integer(units)\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\n```\n:::\n\n\nFor serialization support in your custom layer, define a `get_config`\nmethod that returns the constructor arguments of the layer instance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_custom_dense <- new_layer_class(\n  \"CustomDense\",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units <- as.integer(units)\n  },\n\n  build = function(input_shape) {\n    self$w <-\n      self$add_weight(\n        shape = shape(tail(input_shape, 1), self$units),\n        initializer = \"random_normal\",\n        trainable = TRUE\n      )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n\n  get_config = function() {\n    list(units = self$units)\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\nconfig <- model %>% get_config()\n\nnew_model <- from_config(config, custom_objects = list(layer_custom_dense))\n```\n:::\n\n\nOptionally, implement the class method `from_config(class_constructor, config)` which\nis used when recreating a layer instance given its config.\nThe default implementation of `from_config` is approximately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrom_config <- function(layer_constructor, config)\n  do.call(layer_constructor, config)\n```\n:::\n\n\n## When to use the functional API\n\nShould you use the Keras functional API to create a new model, or just\nsubclass the `Model` class directly? In general, the functional API is\nhigher-level, easier and safer, and has a number of features that\nsubclassed models do not support.\n\nHowever, model subclassing provides greater flexibility when building\nmodels that are not easily expressible as directed acyclic graphs of\nlayers. For example, you could not implement a Tree-RNN with the\nfunctional API and would have to subclass `Model` directly.\n\nFor an in-depth look at the differences between the functional API and\nmodel subclassing, read [What are Symbolic and Imperative APIs in\nTensorFlow\n2.0?](https://blog$tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n\n### Functional API strengths:\n\nThe following properties are also true for Sequential models (which are\nalso data structures), but are not true for subclassed models (which are\nR code, not data structures).\n\n#### Less verbose\n\nThere is no `super$initialize(...)`, no `call <- function(...) {   }`,\netc.\n\nCompare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = c(32))\noutputs <- inputs %>%\n  layer_dense(64, activation = 'relu') %>%\n  layer_dense(10)\nmlp <- keras_model(inputs, outputs)\n```\n:::\n\n\nWith the subclassed version:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMLP <- new_model_class(\n  classname = \"MLP\",\n\n  initialize = function(...) {\n    super$initialize(...)\n    self$dense_1 <- layer_dense(units = 64, activation = 'relu')\n    self$dense_2 <- layer_dense(units = 10)\n  },\n\n  call = function(inputs) {\n    inputs %>%\n      self$dense_1() %>%\n      self$dense_2()\n  }\n)\n\n# Instantiate the model.\nmlp <- MLP()\n\n# Necessary to create the model's state.\n# The model doesn't have a state until it's called at least once.\ninvisible(mlp(tf$zeros(shape(1, 32))))\n```\n:::\n\n\n#### Model validation while defining its connectivity graph\n\nIn the functional API, the input specification (shape and dtype) is\ncreated in advance (using `layer_input`). Every time you call a layer, the\nlayer checks that the specification passed to it matches its\nassumptions, and it will raise a helpful error message if not.\n\nThis guarantees that any model you can build with the functional API\nwill run. All debugging -- other than convergence-related debugging --\nhappens statically during the model construction and not at execution\ntime. This is similar to type checking in a compiler.\n\n#### A functional model is plottable and inspectable\n\nYou can plot the model as a graph, and you can easily access\nintermediate nodes in this graph. For example, to extract and reuse the\nactivations of intermediate layers (as seen in a previous example):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\nfeat_extraction_model <- keras_model(inputs = vgg19$input,\n                                     outputs = features_list)\n```\n:::\n\n\n#### A functional model can be serialized or cloned\n\nBecause a functional model is a data structure rather than a piece of\ncode, it is safely serializable and can be saved as a single file that\nallows you to recreate the exact same model without having access to any\nof the original code. See the [serialization & saving\nguide](/guides/serialization_and_saving/).\n\nTo serialize a subclassed model, it is necessary for the implementer to\nspecify a `get_config()` and `from_config()` method at the model level.\n\n### Functional API weakness:\n\n#### It does not support dynamic architectures\n\nThe functional API treats models as DAGs of layers. This is true for\nmost deep learning architectures, but not all -- for example, recursive\nnetworks or Tree RNNs do not follow this assumption and cannot be\nimplemented in the functional API.\n\n## Mix-and-match API styles\n\nChoosing between the functional API or Model subclassing isn't a binary\ndecision that restricts you into one category of models. All models in\nthe `tf$keras` API can interact with each other, whether they're\n`Sequential` models, functional models, or subclassed models that are\nwritten from scratch.\n\nYou can always use a functional model or `Sequential` model as part of a\nsubclassed model or layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunits <- 32L\ntimesteps <- 10L\ninput_dim <- 5L\n\n# Define a Functional model\n\ninputs <- layer_input(c(NA, units))\noutputs <- inputs %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\n\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units\n    self$projection_1 <-\n      layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <-\n      layer_dense(units = units, activation = \"tanh\")\n    # Our previously-defined Functional model\n    self$classifier <- model\n  },\n\n  call = function(inputs) {\n    message(\"inputs shape: \", format(inputs$shape))\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n\n    features <- tf$stack(outputs, axis = 1L) # axis is 1-based\n    message(\"features shape: \", format(features$shape))\n    self$classifier(features)\n  }\n)\n\nlayer_custom_rnn(tf$zeros(shape(1, timesteps, input_dim)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ninputs shape: (1, 10, 5)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nfeatures shape: (1, 10, 32)\n```\n:::\n:::\n\n\nYou can use any subclassed layer or model in the functional API as long\nas it implements a `call` method that follows one of the following\npatterns:\n\n-   `call(inputs, ..., training = NULL, mask = NULL)` -- Where `inputs` is a tensor or a\n    nested structure of tensors (e.g. a list of tensors), and where\n    optional named arguments `training` and `mask` can be present.\n\n    are non-tensor arguments (non-inputs).\n-   `call(self, inputs, training = NULL, **kwargs)` -- Where `training`\n    is a boolean indicating whether the layer should behave in training\n    mode and inference mode.\n-   `call(self, inputs, mask = NULL, **kwargs)` -- Where `mask` is a\n    boolean mask tensor (useful for RNNs, for instance).\n-   `call(self, inputs, training = NULL, mask = NULL, **kwargs)` -- Of\n    course, you can have both masking and training-specific behavior at\n    the same time.\n\nAdditionally, if you implement the `get_config` method on your custom\nLayer or model, the functional models you create will still be\nserializable and cloneable.\n\nHere's a quick example of a custom RNN, written from scratch, being used\nin a functional model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunits <- 32\ntimesteps <- 10\ninput_dim <- 5\nbatch_size <- 16\n\nlayer_custom_rnn <- new_layer_class(\n  \"CustomRNN\",\n  initialize = function() {\n    super$initialize()\n    self$units <- units\n    self$projection_1 <- layer_dense(units = units, activation = \"tanh\")\n    self$projection_2 <- layer_dense(units = units, activation = \"tanh\")\n    self$classifier <- layer_dense(units = 1)\n  },\n\n  call = function(inputs) {\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\"list\", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n\n    features <- tf$stack(outputs, axis = 1L) # axis arg is 1-based\n    self$classifier(features)\n  }\n)\n\n# Note that you specify a static batch size for the inputs with the `batch_shape`\n# arg, because the inner computation of `CustomRNN` requires a static batch size\n# (when you create the `state` zeros tensor).\ninputs <- layer_input(batch_shape = c(batch_size, timesteps, input_dim))\noutputs <- inputs %>%\n  layer_conv_1d(32, 3) %>%\n  layer_custom_rnn()\n\nmodel <- keras_model(inputs, outputs)\nmodel(tf$zeros(shape(1, 10, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]]], shape=(1, 8, 1), dtype=float32)\n```\n:::\n:::\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.13.0 (~/.virtualenvs/r-tensorflow-website/lib/python3.10/site-packages/tensorflow)\nPython v3.10 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/New_York\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.13.0.9000      tensorflow_2.13.0.9000\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.3            cli_3.6.1              knitr_1.43            \n [4] zeallot_0.1.0          rlang_1.1.1            xfun_0.40             \n [7] png_0.1-8              generics_0.1.3         jsonlite_1.8.7        \n[10] glue_1.6.2             htmltools_0.5.6        fansi_1.0.4           \n[13] rmarkdown_2.24         grid_4.3.1             tfruns_1.5.1          \n[16] evaluate_0.21          tibble_3.2.1           base64enc_0.1-3       \n[19] fastmap_1.1.1          yaml_2.3.7             lifecycle_1.0.3       \n[22] whisker_0.4.1          compiler_4.3.1         htmlwidgets_1.6.2     \n[25] Rcpp_1.0.11            pkgconfig_2.0.3        rstudioapi_0.15.0     \n[28] lattice_0.21-8         digest_0.6.33          R6_2.5.1              \n[31] reticulate_1.31.0.9000 utf8_1.2.3             pillar_1.9.0          \n[34] magrittr_2.0.3         Matrix_1.5-4.1         tools_4.3.1           \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.4.0\narray-record==0.4.1\nasttokens==2.2.1\nastunparse==1.6.3\nbackcall==0.2.0\nbleach==6.0.0\ncachetools==5.3.1\ncertifi==2023.7.22\ncharset-normalizer==3.2.0\nclick==8.1.7\ndecorator==5.1.1\ndm-tree==0.1.8\netils==1.4.1\nexecuting==1.2.0\nflatbuffers==23.5.26\ngast==0.4.0\ngoogle-auth==2.22.0\ngoogle-auth-oauthlib==1.0.0\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.60.0\ngrpcio==1.57.0\nh5py==3.9.0\nidna==3.4\nimportlib-resources==6.0.1\nipython==8.14.0\njedi==0.19.0\nkaggle==1.5.16\nkeras==2.13.1\nkeras-tuner==1.3.5\nkt-legacy==1.0.5\nlibclang==16.0.6\nMarkdown==3.4.4\nMarkupSafe==2.1.3\nmatplotlib-inline==0.1.6\nnumpy==1.24.3\nnvidia-cublas-cu11==11.11.3.6\nnvidia-cudnn-cu11==8.6.0.163\noauthlib==3.2.2\nopt-einsum==3.3.0\npackaging==23.1\npandas==2.0.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==10.0.0\npromise==2.3\nprompt-toolkit==3.0.39\nprotobuf==3.20.3\npsutil==5.9.5\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.5.0\npyasn1-modules==0.3.0\npydot==1.4.2\nPygments==2.16.1\npyparsing==3.1.1\npython-dateutil==2.8.2\npython-slugify==8.0.1\npytz==2023.3\nrequests==2.31.0\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.11.2\nsix==1.16.0\nstack-data==0.6.2\ntensorboard==2.13.0\ntensorboard-data-server==0.7.1\ntensorflow==2.13.0\ntensorflow-datasets==4.9.2\ntensorflow-estimator==2.13.0\ntensorflow-hub==0.14.0\ntensorflow-io-gcs-filesystem==0.33.0\ntensorflow-metadata==1.14.0\ntermcolor==2.3.0\ntext-unidecode==1.3\ntoml==0.10.2\ntqdm==4.66.1\ntraitlets==5.9.0\ntyping_extensions==4.5.0\ntzdata==2023.3\nurllib3==1.26.16\nwcwidth==0.2.6\nwebencodings==0.5.1\nWerkzeug==2.3.7\nwrapt==1.15.0\nzipp==3.16.2\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2023-08-28 \nPage render time: 27 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [
      "functional_api_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}