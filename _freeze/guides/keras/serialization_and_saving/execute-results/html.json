{
  "hash": "32668219535822f29d34c03a23bfdb46",
  "result": {
    "markdown": "---\ntitle: Serialization and saving\nauthor: Kathy Wu, Francois Chollet\ndate-created: 2020/04/28\ndate-last-modified: 2020/04/28\ndescription: Complete guide to saving & serializing models.\n---\n\n\n## Introduction\n\n\nA Keras model consists of multiple components:\n\n- The architecture, or configuration, which specifies what layers the model\ncontain, and how they're connected.\n- A set of weights values (the \"state of the model\").\n- An optimizer (defined by compiling the model).\n- A set of losses and metrics (defined by compiling the model or calling\n`add_loss()` or `add_metric()`).\n\nThe Keras API makes it possible to save all of these pieces to disk at once,\nor to only selectively save some of them:\n\n- Saving everything into a single archive in the TensorFlow SavedModel format\n(or in the older Keras H5 format). This is the standard practice.\n- Saving the architecture / configuration only, typically as a JSON file.\n- Saving the weights values only. This is generally used when training the model.\n\nLet's take a look at each of these options. When would you use one or the other,\nand how do they work?\n\n\n## How to save and load a model\n\nIf you only have 10 seconds to read this guide, here's what you need to know.\n\n**Saving a Keras model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- ...  # Get model (Sequential, Functional Model, or Model subclass)\nsave_model_tf(\"path/to/location\")\n```\n:::\n\n\n**Loading the model back:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nmodel <- load_model_tf(\"path/to/location\")\n```\n:::\n\n\nNow, let's look at the details.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Whole-model saving & loading\n\nYou can save an entire model to a single artifact. It will include:\n\n- The model's architecture/config\n- The model's weight values (which were learned during training)\n- The model's compilation information (if `compile()` was called)\n- The optimizer and its state, if any (this enables you to restart training\nwhere you left)\n\n#### APIs\n\n\n- `model$save()` or `save_model_tf()`\n- `load_model_tf()`\n\nThere are two formats you can use to save an entire model to disk:\n**the TensorFlow SavedModel format**, and the older Keras **H5 format**.\nThe recommended format is SavedModel. It is the default when you use `model$save()`.\n\nYou can switch to the H5 format by:\n\n- Passing `save_format = 'h5'` to `save_model_hdf5()`.\n- Passing a filename that ends in `.h5` or `.keras` to `$save()`.\n\n### SavedModel format\n\nSavedModel is the more comprehensive save format that saves the model architecture,\nweights, and the traced Tensorflow subgraphs of the call functions. This enables\nKeras to restore both built-in layers as well as custom objects.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  # Create a simple model.\n  inputs <- layer_input(shape = shape(32))\n  outputs <- layer_dense(inputs, 1)\n  model <- keras_model(inputs, outputs)\n  model %>% compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n  model\n}\nmodel <- get_model()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Train the model.\ntest_input <- array(runif(128*32), dim = c(128, 32))\ntest_target <- array(runif(128), dim = c(128, 1))\nmodel %>% fit(test_input, test_target)\n# Calling `save('my_model')` creates a SavedModel folder `my_model`.\nsave_model_tf(model, \"my_model\")\n# It can be used to reconstruct the model identically.\nreconstructed_model <- load_model_tf(\"my_model\")\n# Let's check:\nall.equal(\n  predict(model, test_input),\n  predict(reconstructed_model, test_input)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# The reconstructed model is already compiled and has retained the optimizer\n# state, so training can resume:\nreconstructed_model %>% fit(test_input, test_target)\n```\n:::\n\n\n#### What the SavedModel contains\n\nCalling `save_model_tf(model, 'my_model')` creates a folder named `my_model`,\ncontaining the following:\n\n\n::: {.cell}\n\n```{.sh .cell-code}\nls my_model\n```\n\n\n::: {.cell-output .cell-output-stdout}\n```\nassets\nkeras_metadata.pb\nsaved_model.pb\nvariables\n```\n:::\n:::\n\n\nThe model architecture, and training configuration\n(including the optimizer, losses, and metrics) are stored in `saved_model.pb`.\nThe weights are saved in the `variables/` directory.\n\nFor detailed information on the SavedModel format, see the\n[SavedModel guide (*The SavedModel format on disk*)](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk).\n\n#### How SavedModel handles custom objects\n\nWhen saving the model and its layers, the SavedModel format stores the\nclass name, **call function**, losses, and weights (and the config, if implemented).\nThe call function defines the computation graph of the model/layer.\n\nIn the absence of the model/layer config, the call function is used to create\na model that exists like the original model which can be trained, evaluated,\nand used for inference.\n\nNevertheless, it is always a good practice to define the `get_config`\nand `from_config` methods when writing a custom model or layer class.\nThis allows you to easily update the computation later if needed.\nSee the section about [Custom objects](#custom-objects)\nfor more information.\n\n:::{.callout-note}\nThe default `from_config` definition in R is something similar just calls the \n`initialize` method with the config lsit using `do.call`. That way you don't need\nto implement a `from_config` method unless `get_config()` dictionary names don't\nmatch the initialize arguments.\n:::\n\nExample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustom_model <- new_model_class(\n  \"custom_model\",\n  initialize = function(hidden_units) {\n    super()$`__init__`()\n    self$hidden_units <- hidden_units\n    self$dense_layers <- lapply(hidden_units, function(x) layer_dense(units = x))\n  },\n  call = function(inputs) {\n    x <- inputs\n    for (layer in self$dense_layers) {\n      x <- layer(x)\n    }\n    x\n  },\n  get_config = function() {\n    list(hidden_units = self$hidden_units)\n  }\n)\nmodel <- custom_model(c(16, 16, 10))\n# Build the model by calling it\ninput_arr <- tf$random$uniform(shape(1, 5))\noutputs <- model(input_arr)\nsave_model_tf(model, \"my_model\")\n# Option 1: Load with the custom_object argument.\nloaded_1 <- load_model_tf(\n    \"my_model\", custom_objects = list(\"custom_model\" = custom_model)\n)\n# Option 2: Load without the CustomModel class.\n# Delete the custom-defined model class to ensure that the loader does not have\n# access to it.\nrm(custom_model); gc();\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          used (Mb) gc trigger  (Mb) limit (Mb) max used  (Mb)\nNcells 1861943 99.5    3665865 195.8         NA  2534025 135.4\nVcells 3287266 25.1    8388608  64.0     102400  5238666  40.0\n```\n:::\n\n```{.r .cell-code}\nloaded_2 <- load_model_tf(\"my_model\")\nall.equal(predict(loaded_1, input_arr), as.array(outputs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nall.equal(predict(loaded_2, input_arr), as.array(outputs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nThe first loaded model is loaded using the config and `custom_model` class. The second\nmodel is loaded by dynamically creating the model class that acts like the original model.\n\n#### Configuring the SavedModel\n\n*New in TensoFlow 2.4*\n\nThe argument `save_traces` has been added to `model$save`, which allows you to toggle\nSavedModel function tracing. Functions are saved to allow the Keras to re-load custom\nobjects without the original class definitons, so when `save_traces = FALSE`, all custom\nobjects must have defined `get_config`/`from_config` methods. When loading, the custom\nobjects must be passed to the `custom_objects` argument. `save_traces = FALSE` reduces the\ndisk space used by the SavedModel and saving time.\n\n### Keras H5 format\n\nKeras also supports saving a single HDF5 file containing the model's architecture,\nweights values, and `compile()` information.\nIt is a light-weight alternative to SavedModel.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- get_model()\n# Train the model.\ntest_input <- array(runif(128*32), dim = c(128, 32))\ntest_target <- array(runif(128), dim = c(128, 1))\nmodel %>% fit(test_input, test_target)\n# Calling `save_model_hdf5('my_model.h5')` creates a h5 file `my_model.h5`.\nsave_model_hdf5(model, \"my_h5_model.h5\")\n# It can be used to reconstruct the model identically.\nreconstructed_model <- load_model_hdf5(\"my_h5_model.h5\")\n# Let's check:\nall.equal(\n  predict(model, test_input), \n  predict(reconstructed_model, test_input)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# The reconstructed model is already compiled and has retained the optimizer\n# state, so training can resume:\nreconstructed_model %>% fit(test_input, test_target)\n```\n:::\n\n\n### Format Limitations\n\nKeras SavedModel format limitations:\n\nThe tracing done by SavedModel to produce the graphs of the layer call functions allows\nSavedModel be more portable than H5, but it comes with drawbacks.\n\n- Can be slower and bulkier than H5.\n- Cannot serialize the ops generated from the mask argument (i$e. if a layer is called\n  with `layer(..., mask = mask_value)`, the mask argument is not saved to SavedModel).\n- Does not save the overridden `train_step()` in subclassed models.\n\nCustom objects that use masks or have a custom training loop can still be saved and loaded\nfrom SavedModel, except they must override `get_config()`/`from_config()`, and the classes\nmust be passed to the `custom_objects` argument when loading.\n\nH5 limitations:\n\n- External losses & metrics added via `model$add_loss()`\n& `model$add_metric()` are not saved (unlike SavedModel).\nIf you have such losses & metrics on your model and you want to resume training,\nyou need to add these losses back yourself after loading the model.\nNote that this does not apply to losses/metrics created *inside* layers via\n`self$add_loss()` & `self$add_metric()`. As long as the layer gets loaded,\nthese losses & metrics are kept, since they are part of the `call` method of the layer.\n- The *computation graph of custom objects* such as custom layers\nis not included in the saved file. At loading time, Keras will need access\nto the Python classes/functions of these objects in order to reconstruct the model.\nSee [Custom objects](#custom-objects).\n- Does not support preprocessing layers.\n\n## Saving the architecture\n\nThe model's configuration (or architecture) specifies what layers the model\ncontains, and how these layers are connected*. If you have the configuration of a model,\nthen the model can be created with a freshly initialized state for the weights\nand no compilation information.\n\n*Note this only applies to models defined using the functional or Sequential apis\n not subclassed models.\n\n### Configuration of a Sequential model or Functional API model\n\nThese types of models are explicit graphs of layers: their configuration\nis always available in a structured form.\n\n#### APIs\n\n- `get_config()` and `from_config()`\n- `model_to_json()` and `model_from_json()`\n\n#### `get_config()` and `from_config()`\n\nCalling `config = model$get_config()` will return a Python dict containing\nthe configuration of the model. The same model can then be reconstructed via\n`Sequential$from_config(config)` (for a `Sequential` model) or\n`Model$from_config(config)` (for a Functional API model).\n\nThe same workflow also works for any serializable layer.\n\n**Layer example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3, activation = \"relu\")\nlayer_config <- get_config(layer)\nnew_layer <- from_config(config = layer_config)\n```\n:::\n\n\n**Sequential model example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(list(\n  layer_input(shape = 32), \n  layer_dense(units = 1)\n))\nconfig <- get_config(model)\nnew_model <- from_config(config)\n```\n:::\n\n\n**Functional model example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = 32)\noutputs <- layer_dense(inputs, 1)\nmodel <- keras_model(inputs, outputs)\nconfig <- get_config(model)\nnew_model <- from_config(config)\n```\n:::\n\n\n#### `model_to_json()` and `model_from_json()`\n\nThis is similar to `get_config` / `from_config`, except it turns the model\ninto a JSON string, which can then be loaded without the original model class.\nIt is also specific to models, it isn't meant for layers.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(list(\n  layer_input(shape = 32), \n  layer_dense(units = 1)\n))\njson_config <- model_to_json(model)\nnew_model <- model_from_json(json_config)\n```\n:::\n\n\n### Custom objects\n\n**Models and layers**\n\nThe architecture of subclassed models and layers are defined in the methods\n`initialize` and `call`. They are considered R bytecode,\nwhich cannot be serialized into a JSON-compatible config\n-- you could try serializing the bytecode (e.g. via `saveRDS`),\nbut it's completely unsafe and means your model cannot be loaded on a different system.\n\nIn order to save/load a model with custom-defined layers, or a subclassed model,\nyou should overwrite the `get_config` and optionally `from_config` methods.\nAdditionally, you should use register the custom object so that Keras is aware of it.\n\n**Custom functions**\n\nCustom-defined functions (e.g. activation loss or initialization) do not need\na `get_config` method. The function name is sufficient for loading as long\nas it is registered as a custom object.\n\n**Loading the TensorFlow graph only**\n\nIt's possible to load the TensorFlow graph generated by the Keras. If you\ndo so, you won't need to provide any `custom_objects`. You can do so like\nthis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_tf(model, \"my_model\")\ntensorflow_graph <- tf$saved_model$load(\"my_model\")\nx <- as_tensor(array(runif(4*32), dim = c(4, 32)), \"float32\")\npredicted <- tensorflow_graph(x)$numpy()\n```\n:::\n\n\nNote that this method has several drawbacks:\n* For traceability reasons, you should always have access to the custom\nobjects that were used. You wouldn't want to put in production a model\nthat you cannot re-create.\n* The object returned by `tf$saved_model$load` isn't a Keras model. So it's\nnot as easy to use. For example, you won't have access to `predict()` or `fit()`\n\nEven if its use is discouraged, it can help you if you're in a tight spot,\nfor example, if you lost the code of your custom objects or have issues\nloading the model with `load_model_tf()`.\n\nYou can find out more in\nthe [page about `tf$saved_model$load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load)\n\n#### Defining the config methods\n\nSpecifications:\n\n* `get_config` should return a JSON-serializable dictionary in order to be\ncompatible with the Keras architecture - and model-saving APIs.\n* `from_config(config)` (`classmethod`) should return a new layer or model\nobject that is created from the config.\nThe default implementation returns `do.call(cls, config)`.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustom_layer <- new_layer_class(\n  \"custom_layer\",\n  initialize = function(a) {\n    self$var <- tf$Variable(a, name = \"var_a\")\n  },\n  call = function(inputs, training = FALSE) {\n    if(training) {\n      inputs*self$var\n    } else {\n      inputs\n    }\n  },\n  get_config = function() {\n    list(\"a\" = as.array(self$var))\n  }\n)\n\nlayer <- custom_layer(a = 5)\nlayer$var$assign(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.0>\n```\n:::\n\n```{.r .cell-code}\nserialized_layer <- keras$layers$serialize(layer)\nnew_layer <- keras$layers$deserialize(\n    serialized_layer, custom_objects = list(\"custom_layer\" = custom_layer)\n)\n```\n:::\n\n\n\n#### Registering the custom object\n\nKeras keeps a note of which class generated the config.\nFrom the example above, `tf$keras$layers$serialize`\ngenerates a serialized form of the custom layer:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nlist(class_name = \"custom_layer\", config = list(a = 2))\n```\n:::\n:::\n\n\nKeras keeps a master list of all built-in layer, model, optimizer,\nand metric classes, which is used to find the correct class to call `from_config`.\nIf the  class can't be found, then an error is raised (`Value Error: Unknown layer`).\nThere are a few ways to register custom classes to this list:\n1. Setting `custom_objects` argument in the loading function. (see the example\nin section above \"Defining the config methods\")\n2. `tf$keras$utils$custom_object_scope` or `tf$keras$utils$CustomObjectScope`\n3. `tf$keras$utils$register_keras_serializable`\n\n#### Custom layer and function example\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustom_layer <- new_layer_class(\n  \"custom_layer\",\n  initialize = function(units = 32, ...) {\n    super()$`__init__`(...)\n    self$units <- units\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \"random_normal\",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n  get_config = function() {\n    config <- super()$get_config()\n    config$units <- self$units\n    config\n  }\n)\n\ncustom_activation <- function(x) {\n  tf$nn$tanh(x)^2\n}\n\n# Make a model with the custom_layer and custom_activation\ninputs <- layer_input(shape = shape(32))\nx <- custom_layer(inputs, 32)\noutputs <- layer_activation(x, custom_activation)\nmodel <- keras_model(inputs, outputs)\n\n# Retrieve the config\nconfig <- get_config(model)\n\n# At loading time, register the custom objects with a `custom_object_scope`:\ncustom_objects <- list(\n  \"custom_layer\" = custom_layer,\n  \"python_function\" = custom_activation\n)\n\nwith(tf$keras$utils$custom_object_scope(custom_objects), {\n  new_model <- keras$Model$from_config(config)\n})\n```\n:::\n\n    \n\n### In-memory model cloning\n\nYou can also do in-memory cloning of a model via `tf$keras$models$clone_model()`.\nThis is equivalent to getting the config then recreating the model from its config\n(so it does not preserve compilation information or layer weights values).\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$keras$utils$custom_object_scope(custom_objects), {\n  new_model <- clone_model(model)\n})\n```\n:::\n\n\n## Saving & loading only the model's weights values\n\nYou can choose to only save & load a model's weights. This can be useful if:\n- You only need the model for inference: in this case you won't need to\nrestart training, so you don't need the compilation information or optimizer state.\n- You are doing transfer learning: in this case you will be training a new model\nreusing the state of a prior model, so you don't need the compilation\ninformation of the prior model.\n\n### APIs for in-memory weight transfer\n\nWeights can be copied between different objects by using `get_weights`\nand `set_weights`:\n\n* `get_weights()`: Returns a list of arrays.\n* `set_weights()`: Sets the model weights to the values in the `weights` argument.\n\nExamples below.\n\nTransfering weights from one layer to another, in memory\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_layer <- function() {\n  layer <- layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\n  layer$build(shape(NULL, 784))\n  layer\n}\n    \nlayer_1 <- create_layer()\nlayer_2 <- create_layer()\n\n# Copy weights from layer 1 to layer 2\nset_weights(layer_2, get_weights(layer_1))\n```\n:::\n\n\nTransfering weights from one model to another model with a\ncompatible architecture, in memory\n\n# Create a simple functional model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = 784, name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(10, name = \"predictions\")\nfunctional_model <- keras_model(\n  inputs = inputs, \n  outputs = outputs, \n  name = \"3_layer_mlp\"\n)\n```\n:::\n\n\n# Define a subclassed model with the same architecture\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubclassed_model <- new_model_class(\n  \"subclassed_model\",\n  initialize = function(output_dim, name = NULL) {\n    super()$`__init__`(name = name)\n    self$output_dim <- output_dim\n    self$dense_1 <- layer_dense(units = 64, activation = \"relu\", name = \"dense_1\")\n    self$dense_2 <- layer_dense(units = 64, activation = \"relu\", name = \"dense_2\")\n    self$dense_3 <- layer_dense(units = output_dim, name = \"predictions\")\n  },\n  call = function(inputs) {\n    inputs %>% \n      self$dense_1() %>% \n      self$dense_2() %>% \n      self$dense_3()\n  },\n  get_config = function() {\n    list(\n      output_dim = self$output_dim,\n      name = self$name\n    )\n  }\n)\n\nmodel <- subclassed_model(output_dim = 10)\n\n\n# Call the subclassed model once to create the weights.\nmodel(tf$ones(shape(1, 784)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.58785105  1.8791215   0.39550525  1.3584478   0.73631656 -1.0365686\n  -1.0839475  -0.42656037 -1.6266857  -0.40332988]], shape=(1, 10), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# Copy weights from functional_model to subclassed_model.\nset_weights(model, get_weights(functional_model))\n\nlength(functional_model$weights) == length(model$weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nall.equal(get_weights(functional_model), get_weights(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n***The case of stateless layers***\n\nBecause stateless layers do not change the order or number of weights,\nmodels can have compatible architectures even if there are extra/missing\nstateless layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(10, name = \"predictions\")\nfunctional_model <- keras_model(\n  inputs = inputs, \n  outputs = outputs, \n  name = \"3_layer_mlp\"\n)\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  # Add a dropout layer, which does not contain any weights.\n  layer_dropout(0.5) %>% \n  layer_dense(10, name = \"predictions\")\n\nfunctional_model_with_dropout <- keras_model(\n  inputs = inputs, \n  outputs = outputs, \n  name = \"3_layer_mlp\"\n)\n\nset_weights(functional_model_with_dropout, get_weights(functional_model))\n```\n:::\n\n\n### APIs for saving weights to disk & loading them back\n\nWeights can be saved to disk by calling `model$save_weights`\nin the following formats:\n\n* TensorFlow Checkpoint: `save_model_weights_tf()`\n* HDF5: `save_model_weights_hdf5()`\n\nEach format has its pros and cons which are detailed below.\n\n### TF Checkpoint format\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Runnable example\n\nsequential_model <- keras_model_sequential(input_shape = shape(784)) %>%\n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(10, name = \"predictions\")\n\nsave_model_weights_tf(sequential_model, \"ckpt\")\nload_model_weights_tf(sequential_model, \"ckpt\")\n```\n:::\n\n\n#### Format details\n\nThe TensorFlow Checkpoint format saves and restores the weights using\nobject attribute names. For instance, consider the `layer_dense` layer.\nThe layer contains two weights: `dense$kernel` and `dense$bias`.\nWhen the layer is saved to the `tf` format, the resulting checkpoint contains the keys\n`\"kernel\"` and `\"bias\"` and their corresponding weight values.\nFor more information see\n[\"Loading mechanics\" in the TF Checkpoint guide](https://www.tensorflow.org/guide/checkpoint#loading_mechanics).\nNote that attribute/graph edge is named after **the name used in parent object,\nnot the name of the variable**. Consider the `custom_layer` in the example below.\nThe variable `custom_layer$var` is saved with `\"var\"` as part of key, not `\"var_a\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustom_layer <- new_layer_class(\n  \"custom_layer\",\n  initialize = function(a) {\n    self$var <- tf$Variable(a, name = \"var_a\")\n  }\n)\n\nlayer <- custom_layer(a = 5)\nlayer_ckpt <- tf$train$Checkpoint(layer = layer)$save(\"custom_layer\")\nckpt_reader <- tf$train$load_checkpoint(layer_ckpt)\nckpt_reader$get_variable_to_dtype_map()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`save_counter/.ATTRIBUTES/VARIABLE_VALUE`\ntf.int64\n\n$`layer/var/.ATTRIBUTES/VARIABLE_VALUE`\ntf.float32\n\n$`_CHECKPOINTABLE_OBJECT_GRAPH`\ntf.string\n```\n:::\n:::\n\n\n#### Transfer learning example\n\nEssentially, as long as two models have the same architecture,\nthey are able to share the same checkpoint.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(10, name = \"predictions\")\n\nfunctional_model <- keras_model(\n  inputs = inputs, \n  outputs = outputs, \n  name = \"3_layer_mlp\"\n)\n\n# Extract a portion of the functional model defined in the Setup section.\n# The following lines produce a new model that excludes the final output\n# layer of the functional model.\n\npretrained <- keras_model(\n    inputs = functional_model$inputs, \n    outputs = functional_model$layers[[4]]$input, \n    name = \"pretrained_model\"\n)\n\n# Randomly assign \"trained\" weights.\nfor (w in pretrained$weights) {\n  w$assign(tf$random$normal(w$shape))\n}\n    \nsave_model_weights_tf(pretrained, \"pretrained_ckpt\")\nsummary(pretrained)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"pretrained_model\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n digits (InputLayer)              [(None, 784)]                 0           \n dense_1 (Dense)                  (None, 64)                    50240       \n dense_2 (Dense)                  (None, 64)                    4160        \n============================================================================\nTotal params: 54,400\nTrainable params: 54,400\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Assume this is a separate program where only 'pretrained_ckpt' exists.\n# Create a new functional model with a different output dimension.\n\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(5, name = \"predictions\")\n\nmodel <- keras_model(inputs = inputs, outputs = outputs, name = \"new_model\")\n\n# Load the weights from pretrained_ckpt into model.\n\nload_model_weights_tf(model, \"pretrained_ckpt\")\n\n# Check that all of the pretrained weights have been loaded.\nall.equal(get_weights(pretrained), head(get_weights(model), 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"new_model\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n digits (InputLayer)              [(None, 784)]                 0           \n dense_1 (Dense)                  (None, 64)                    50240       \n dense_2 (Dense)                  (None, 64)                    4160        \n predictions (Dense)              (None, 5)                     325         \n============================================================================\nTotal params: 54,725\nTrainable params: 54,725\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Example 2: Sequential model\n# Recreate the pretrained model, and load the saved weights.\ninputs <- layer_input(shape = shape(784), name = \"digits\")\noutputs <- inputs %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\")\n\npretrained_model <- keras_model(\n  inputs = inputs, \n  outputs = outputs, \n  name = \"pretrained\"\n)\n\n# Sequential example:\nmodel <- keras_model_sequential() %>% \n  pretrained_model() %>% \n  layer_dense(5, name = \"predictions\")\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_3\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n pretrained (Functional)          (None, 64)                    54400       \n predictions (Dense)              (None, 5)                     325         \n============================================================================\nTotal params: 54,725\nTrainable params: 54,725\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nload_model_weights_tf(pretrained_model, \"pretrained_ckpt\")\n```\n:::\n\n\nWarning! Calling `model$load_weights('pretrained_ckpt')` won't throw an error,\nbut will *not* work as expected. If you inspect the weights, you'll see that\nnone of the weights will have loaded. `pretrained_model$load_weights()` is the\ncorrect method to call.\n\nIt is generally recommended to stick to the same API for building models. If you\nswitch between Sequential and Functional, or Functional and subclassed,\netc., then always rebuild the pre-trained model and load the pre-trained\nweights to that model.\n\n\nThe next question is, how can weights be saved and loaded to different models\nif the model architectures are quite different?\nThe solution is to use `tf$train$Checkpoint` to save and restore the exact layers/variables.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a subclassed model that essentially uses functional_model's first\n# and last layers.\n# First, save the weights of functional_model's first and last dense layers.\nfirst_dense <- functional_model$layers[[2]]\nlast_dense <- functional_model$layers[[4]]\nckpt_path <- tf$train$Checkpoint(\n    dense = first_dense, \n    kernel = last_dense$kernel, \n    bias = last_dense$bias\n)$save(\"ckpt\")\n\n# Define the subclassed model.\ncontrived_model <- new_model_class(\n  \"contrived_model\",\n  initialize = function() {\n    super()$`__init__`()\n    self$first_dense <- layer_dense(units = 64)\n    self$kernel <- self$add_weight(\"kernel\", shape = shape(64, 10))\n    self$bias <- self$add_weight(\"bias\", shape = shape(10))\n  },\n  call = function(inputs) {\n    x <- self$first_dense(inputs)\n    tf$matmul(x, self$kernel) + self$bias\n  }\n)\n\nmodel <- contrived_model()\n# Call model on inputs to create the variables of the dense layer.\ninvisible(model(tf$ones(shape(1, 784))))\n\n# Create a Checkpoint with the same structure as before, and load the weights.\ntf$train$Checkpoint(\n    dense = model$first_dense, \n    kernel = model$kernel, \n    bias = model$bias\n)$restore(ckpt_path)$assert_consumed()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x7fe6e8feeb20>\n```\n:::\n:::\n\n\n### HDF5 format\n\nThe HDF5 format contains weights grouped by layer names.\nThe weights are lists ordered by concatenating the list of trainable weights\nto the list of non-trainable weights (same as `layer$weights`).\n\nThus, a model can use a hdf5 checkpoint if it has the same layers and trainable\nstatuses as saved in the checkpoint.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Runnable example\nsequential_model <- keras_model_sequential(input_shape = 784) %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n  layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n  layer_dense(10, name = \"predictions\")\n\nsave_model_weights_hdf5(sequential_model, \"weights.h5\")\nload_model_weights_hdf5(sequential_model, \"weights.h5\")\n```\n:::\n\n\nNote that changing `layer$trainable` may result in a different\n`layer$weights` ordering when the model contains nested layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnested_dense_layer <- new_layer_class(\n  \"nested_dense_layer\",\n  initialize = function(units, name = NULL) {\n    super()$`__init__`(name = name)\n    self$dense_1 <- layer_dense(units = units, name = \"dense_1\")\n    self$dense_2 <- layer_dense(units = units, name = \"dense_2\")\n  },\n  call = function(inputs) {\n    inputs %>% \n      self$dense_1() %>% \n      self$dense_2()\n  }\n)\n\nnested_model <- keras_model_sequential(input_shape = 784) %>% \n  nested_dense_layer(units = 10, name = \"nested\")\n\nvariable_names <- lapply(nested_model$weights, function(x) x$name)\nstr(variable_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ : chr \"nested/dense_1/kernel:0\"\n $ : chr \"nested/dense_1/bias:0\"\n $ : chr \"nested/dense_2/kernel:0\"\n $ : chr \"nested/dense_2/bias:0\"\n```\n:::\n\n```{.r .cell-code}\nprint(\"\\nChanging trainable status of one of the nested layers...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nChanging trainable status of one of the nested layers...\"\n```\n:::\n\n```{.r .cell-code}\nlayer <- nested_model %>% \n  get_layer(\"nested\")\nlayer$dense_1$trainable <- FALSE\n\nvariable_names_2 <- lapply(nested_model$weights, function(x) x$name)\nstr(variable_names_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ : chr \"nested/dense_2/kernel:0\"\n $ : chr \"nested/dense_2/bias:0\"\n $ : chr \"nested/dense_1/kernel:0\"\n $ : chr \"nested/dense_1/bias:0\"\n```\n:::\n:::\n\n\n#### Transfer learning example\n\nWhen loading pretrained weights from HDF5, it is recommended to load\nthe weights into the original checkpointed model, and then extract\nthe desired weights/layers into a new model.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_functional_model <- function() {\n  inputs <- layer_input(shape = shape(784), name = \"digits\")\n  outputs <- inputs %>% \n    layer_dense(64, activation = \"relu\", name = \"dense_1\") %>% \n    layer_dense(64, activation = \"relu\", name = \"dense_2\") %>% \n    layer_dense(10, name = \"predictions\")\n  keras_model(inputs = inputs, outputs = outputs, name = \"3_layer_mlp\")\n}\n\n\nfunctional_model <- create_functional_model()\nsave_model_weights_hdf5(functional_model, \"pretrained_weights.h5\")\n\n# In a separate program:\npretrained_model <- create_functional_model()\nload_model_weights_hdf5(pretrained_model, \"pretrained_weights.h5\")\n\n# Create a new model by extracting layers from the original model:\nextracted_layers <- pretrained_model$layers[1:3]\nextracted_layers <- c(extracted_layers, layer_dense(units = 5, name = \"dense_3\"))\nmodel <- keras_model_sequential(extracted_layers)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_6\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_1 (Dense)                  (None, 64)                    50240       \n dense_2 (Dense)                  (None, 64)                    4160        \n dense_3 (Dense)                  (None, 5)                     325         \n============================================================================\nTotal params: 54,725\nTrainable params: 54,725\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n---\nformat: html\n---\n\n## Environment Details \n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_version()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] '2.9'\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-note appearance=\"simple\"  collapse=\"true\"}\n\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                           sysname \n                                                                                          \"Darwin\" \n                                                                                           release \n                                                                                          \"21.4.0\" \n                                                                                           version \n\"Darwin Kernel Version 21.4.0: Mon Feb 21 20:34:37 PST 2022; root:xnu-8020.101.4~2/RELEASE_X86_64\" \n                                                                                          nodename \n                                                                       \"Daniels-MacBook-Pro.local\" \n                                                                                           machine \n                                                                                          \"x86_64\" \n                                                                                             login \n                                                                                            \"root\" \n                                                                                              user \n                                                                                         \"dfalbel\" \n                                                                                    effective_user \n                                                                                         \"dfalbel\" \n```\n:::\n:::\n\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}