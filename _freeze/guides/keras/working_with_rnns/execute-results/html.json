{
  "hash": "882e44ceb53edb5d9881357cd89afcb4",
  "result": {
    "markdown": "---\ntitle: Working with RNNs\ndescription: >\n  Guide to using and customizing recurrent neural networks, a class \n  of neural networks for modeling sequence data such as time series \n  or natural language.\nauthor: Scott Zhu, Francois Chollet, Tomasz Kalinowski\naliases:\n  - ../../articles/new-guides/working_with_rnns.html\n---\n\n\n## Introduction\n\nRecurrent neural networks (RNN) are a class of neural networks that is\npowerful for modeling sequence data such as time series or natural\nlanguage.\n\nSchematically, a RNN layer uses a `for` loop to iterate over the\ntimesteps of a sequence, while maintaining an internal state that\nencodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n-   **Ease of use**: the built-in `layer_rnn()`, `layer_lstm()`,\n    `layer_gru()` layers enable you to quickly build recurrent models\n    without having to make difficult configuration choices.\n\n-   **Ease of customization**: You can also define your own RNN cell\n    layer (the inner part of the `for` loop) with custom behavior, and\n    use it with the generic `layer_rnn` layer (the `for` loop itself).\n    This allows you to quickly prototype different research ideas in a\n    flexible way with minimal code.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Built-in RNN layers: a simple example\n\nThere are three built-in RNN layers in Keras:\n\n1.  `layer_simple_rnn()`, a fully-connected RNN where the output from\n    the previous timestep is to be fed to the next timestep.\n\n2.  `layer_gru()`, first proposed in [Cho et al.,\n    2014](https://arxiv.org/abs/1406.1078).\n\n3.  `layer_lstm()`, first proposed in [Hochreiter & Schmidhuber,\n    1997](http://www.bioinf.jku.at/publications/older/2604.pdf).\n\nHere is a simple example of a sequential model that processes sequences\nof integers, embeds each integer into a 64-dimensional vector, then\nprocesses the sequence of vectors using a `layer_lstm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding (Embedding)            (None, None, 64)              64000       \n lstm (LSTM)                      (None, 128)                   98816       \n dense (Dense)                    (None, 10)                    1290        \n============================================================================\nTotal params: 164,106\nTrainable params: 164,106\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nBuilt-in RNNs support a number of useful features:\n\n-   Recurrent dropout, via the `dropout` and `recurrent_dropout`\n    arguments\n-   Ability to process an input sequence in reverse, via the\n    `go_backwards` argument\n-   Loop unrolling (which can lead to a large speedup when processing\n    short sequences on CPU), via the `unroll` argument\n-   ...and more.\n\nFor more information, see the [RNN API\ndocumentation](https://keras.io/api/layers/recurrent_layers/).\n\n## Outputs and states\n\nBy default, the output of a RNN layer contains a single vector per\nsample. This vector is the RNN cell output corresponding to the last\ntimestep, containing information about the entire input sequence. The\nshape of this output is `(batch_size, units)` where `units` corresponds\nto the `units` argument passed to the layer's constructor.\n\nA RNN layer can also return the entire sequence of outputs for each\nsample (one vector per timestep per sample), if you set\n`return_sequences = TRUE`. The shape of this output is\n`(batch_size, timesteps, units)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding_1 (Embedding)          (None, None, 64)              64000       \n gru (GRU)                        (None, None, 256)             247296      \n simple_rnn (SimpleRNN)           (None, 128)                   49280       \n dense_1 (Dense)                  (None, 10)                    1290        \n============================================================================\nTotal params: 361,866\nTrainable params: 361,866\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nIn addition, a RNN layer can return its final internal state(s). The\nreturned states can be used to resume the RNN execution later, or [to\ninitialize another RNN](https://arxiv.org/abs/1409.3215). This setting\nis commonly used in the encoder-decoder sequence-to-sequence model,\nwhere the encoder final state is used as the initial state of the\ndecoder.\n\nTo configure a RNN layer to return its internal state, set\n`return_state = TRUE` when creating the layer. Note that `LSTM` has 2\nstate tensors, but `GRU` only has one.\n\nTo configure the initial state of the layer, call the layer instance\nwith the additional named argument `initial_state`. Note that the shape\nof the state needs to match the unit size of the layer, like in the\nexample below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\"encoder\")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \"decoder\")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model\"\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n input_1 (InputLayer)    [(None, None)]  0        []                        \n input_2 (InputLayer)    [(None, None)]  0        []                        \n embedding_2 (Embedding)  (None, None, 6  64000   ['input_1[0][0]']         \n                         4)                                                 \n embedding_3 (Embedding)  (None, None, 6  128000  ['input_2[0][0]']         \n                         4)                                                 \n encoder (LSTM)          [(None, 64),    33024    ['embedding_2[0][0]']     \n                          (None, 64),                                       \n                          (None, 64)]                                       \n decoder (LSTM)          (None, 64)      33024    ['embedding_3[0][0]',     \n                                                   'encoder[0][1]',         \n                                                   'encoder[0][2]']         \n dense_2 (Dense)         (None, 10)      650      ['decoder[0][0]']         \n============================================================================\nTotal params: 258,698\nTrainable params: 258,698\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\n## RNN layers and RNN cells\n\nIn addition to the built-in RNN layers, the RNN API also provides\ncell-level APIs. Unlike RNN layers, which process whole batches of input\nsequences, the RNN cell only processes a single timestep.\n\nThe cell is the inside of the `for` loop of a RNN layer. Wrapping a cell\ninside a `layer_rnn()` layer gives you a layer capable of processing a\nsequence, e.g. `layer_rnn(layer_lstm_cell(10))`.\n\nMathematically, `layer_rnn(layer_lstm_cell(10))` produces the same\nresult as `layer_lstm(10)`. In fact, the implementation of this layer in\nTF v1.x was just creating the corresponding RNN cell and wrapping it in\na RNN layer. However using the built-in `layer_gru()` and `layer_lstm()`\nlayers enable the use of CuDNN and you may see better performance.\n\nThere are three built-in RNN cells, each of them corresponding to the\nmatching RNN layer.\n\n-   `layer_simple_rnn_cell()` corresponds to the `layer_simple_rnn()`\n    layer.\n\n-   `layer_gru_cell` corresponds to the `layer_gru` layer.\n\n-   `layer_lstm_cell` corresponds to the `layer_lstm` layer.\n\nThe cell abstraction, together with the generic `layer_rnn()` class,\nmakes it very easy to implement custom RNN architectures for your\nresearch.\n\n## Cross-batch statefulness\n\nWhen processing very long (possibly infinite) sequences, you may want to\nuse the pattern of **cross-batch statefulness**.\n\nNormally, the internal state of a RNN layer is reset every time it sees\na new batch (i.e. every sample seen by the layer is assumed to be\nindependent of the past). The layer will only maintain a state while\nprocessing a given sample.\n\nIf you have very long sequences though, it is useful to break them into\nshorter sequences, and to feed these shorter sequences sequentially into\na RNN layer without resetting the layer's state. That way, the layer can\nretain information about the entirety of the sequence, even though it's\nonly seeing one sub-sequence at a time.\n\nYou can do this by setting `stateful = TRUE` in the constructor.\n\nIf you have a sequence `s = c(t0, t1, ... t1546, t1547)`, you would\nsplit it into e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n```\n:::\n\n\nThen you would process it via:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n```\n:::\n\n\nWhen you want to clear the state, you can use `layer$reset_states()`.\n\n> Note: In this setup, sample `i` in a given batch is assumed to be the\n> continuation of sample `i` in the previous batch. This means that all\n> batches should contain the same number of samples (batch size). E.g.\n> if a batch contains\n> `[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]`, the next\n> batch should contain\n> `[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]`.\n\nHere is a complete example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n```\n:::\n\n\n### RNN State Reuse\n\nThe recorded states of the RNN layer are not included in the\n`layer$weights()`. If you would like to reuse the state from a RNN\nlayer, you can retrieve the states value by `layer$states` and use it as\nthe initial state of a new layer instance via the Keras functional API\nlike `new_layer(inputs, initial_state = layer$states)`, or model\nsubclassing.\n\nPlease also note that a sequential model cannot be used in this case\nsince it only supports layers with single input and output. The extra\ninput of initial state makes it impossible to use here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \"float32\")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)\n```\n:::\n\n\n## Bidirectional RNNs\n\nFor sequences other than time series (e.g. text), it is often the case\nthat a RNN model can perform better if it not only processes sequence\nfrom start to end, but also backwards. For example, to predict the next\nword in a sentence, it is often useful to have the context around the\nword, not only just the words that come before it.\n\nKeras provides an easy API for you to build such bidirectional RNNs: the\n`bidirectional()` wrapper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_2\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n bidirectional_1 (Bidirectional)  (None, 5, 128)                38400       \n bidirectional (Bidirectional)    (None, 64)                    41216       \n dense_3 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nUnder the hood, `bidirectional()` will copy the RNN layer passed in, and\nflip the `go_backwards` field of the newly copied layer, so that it will\nprocess the inputs in reverse order.\n\nThe output of the `bidirectional` RNN will be, by default, the\nconcatenation of the forward layer output and the backward layer output.\nIf you need a different merging behavior, e.g. averaging, change the\n`merge_mode` parameter in the `bidirectional` wrapper constructor. For\nmore details about `bidirectional`, please check [the API\ndocs](https://keras.io/api/layers/recurrent_layers/bidirectional/).\n\n## Performance optimization and CuDNN kernels\n\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to\nleverage CuDNN kernels by default when a GPU is available. With this\nchange, the prior `layer_cudnn_gru/layer_cudnn_lstm` layers have been\ndeprecated, and you can build your model without worrying about the\nhardware it will run on.\n\nSince the CuDNN kernel is built with certain assumptions, this means the\nlayer **will not be able to use the CuDNN kernel if you change the\ndefaults of the built-in LSTM or GRU layers**. E.g.:\n\n-   Changing the `activation` function from `\"tanh\"` to something else.\n-   Changing the `recurrent_activation` function from `\"sigmoid\"` to\n    something else.\n-   Using `recurrent_dropout > 0`.\n-   Setting `unroll` to `TRUE`, which forces LSTM/GRU to decompose the\n    inner `tf$while_loop` into an unrolled `for` loop.\n-   Setting `use_bias` to `FALSE`.\n-   Using masking when the input data is not strictly right padded (if\n    the mask corresponds to strictly right padded data, CuDNN can still\n    be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for\nthe [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) and\n[GRU](https://keras.io/api/layers/recurrent_layers/gru/) layers.\n\n### Using CuDNN kernels when available\n\nLet's build a simple LSTM model to demonstrate the performance\ndifference.\n\nWe'll use as input sequences the sequence of rows of MNIST digits\n(treating each row of pixels as a timestep), and we'll predict the\ndigit's label.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n```\n:::\n\n\nLet's load the MNIST dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n```\n:::\n\n\nLet's create a model instance and train it.\n\nWe choose `sparse_categorical_crossentropy()` as the loss function for\nthe model. The output of the model has shape of `(batch_size, 10)`. The\ntarget for the model is an integer vector, each of the integer is in the\nrange of 0 to 9.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \"sgd\",\n    metrics = \"accuracy\"\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n```\n:::\n\n\nNow, let's compare to a model that does not use the CuDNN kernel:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\"sgd\",\n    metrics=\"accuracy\",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n```\n:::\n\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the\nmodel built with CuDNN is much faster to train compared to the model\nthat uses the regular TensorFlow kernel.\n\nThe same CuDNN-enabled model can also be used to run inference in a\nCPU-only environment. The `tf$device()` annotation below is just forcing\nthe device placement. The model will run on CPU by default if no GPU is\navailable.\n\nYou simply don't have to worry about the hardware you're running on\nanymore. Isn't that pretty cool?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device(\"CPU:0\"), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \"Predicted result is: %s, target result is: %s\\n\", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted result is: 3, target result is: 5\n```\n:::\n\n::: {.cell-output-display}\n![](working_with_rnns_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## RNNs with list/dict inputs, or nested inputs\n\nNested structures allow implementers to include more information within\na single timestep. For example, a video frame could have audio and video\ninput at the same time. The data shape in this case could be:\n\n`[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]`\n\nIn another example, handwriting data could have both coordinates x and y\nfor the current position of the pen, as well as pressure information. So\nthe data representation could be:\n\n`[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]`\n\nThe following code provides an example of how to build a custom RNN cell\nthat accepts such structured inputs.\n\n### Define a custom cell that supports nested input/output\n\nSee [Making new Layers & Models via\nsubclassing](/guides/making_new_layers_and_models_via_subclassing/) for\ndetails on writing your own layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \"uniform\",\n      name = \"kernel_1\"\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \"uniform\",\n      name = \"kernel_2_3\"\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\"bij,ijkl->bkl\", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\"unit_1\" = self$unit_1,\n         \"unit_2\" = self$unit_2,\n         \"unit_3\" = self$unit_3)\n  }\n}\n```\n:::\n\n\n### Build a RNN model with nested input/output\n\nLet's build a Keras model that uses a `layer_rnn` layer and the custom\ncell we just defined.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"accuracy\")\n```\n:::\n\n\n### Train the model with randomly generated data\n\nSince there isn't a good candidate dataset for this model, we use random\ndata for demonstration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n```\n:::\n\n\nWith `keras::layer_rnn()`, you are only expected to define the math\nlogic for an individual step within the sequence, and the `layer_rnn()`\nwill handle the sequence iteration for you. It's an incredibly powerful\nway to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\n\nFor more details, please visit the [API\ndocs](https://keras.io/api/layers/recurrent_layers/rnn/).\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-site/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-site/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           knitr_1.39           whisker_0.4         \n [4] magrittr_2.0.3       here_1.0.1           lattice_0.20-45     \n [7] R6_2.5.1             rlang_1.0.4          fastmap_1.1.0       \n[10] stringr_1.4.0        tools_4.2.1          grid_4.2.1          \n[13] xfun_0.31            png_0.1-7            cli_3.3.0           \n[16] htmltools_0.5.2      tfruns_1.5.0         rprojroot_2.0.3     \n[19] yaml_2.3.5           digest_0.6.29        Matrix_1.4-1        \n[22] base64enc_0.1-3      htmlwidgets_1.5.4    zeallot_0.1.0       \n[25] evaluate_0.15        rmarkdown_2.14       stringi_1.7.8       \n[28] compiler_4.2.1       generics_0.1.3       reticulate_1.25-9000\n[31] jsonlite_1.8.0      \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.1.0\nasttokens==2.0.5\nastunparse==1.6.3\nbackcall==0.2.0\nbeautifulsoup4==4.11.1\ncachetools==5.2.0\ncertifi==2022.6.15\ncharset-normalizer==2.1.0\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.6.0\nexecuting==0.8.3\nfilelock==3.7.1\nflatbuffers==1.12\ngast==0.4.0\ngdown==4.5.1\ngoogle-auth==2.9.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.8.0\nipython==8.4.0\njedi==0.18.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.2\nkt-legacy==1.0.4\nlibclang==14.0.1\nMarkdown==3.3.7\nmatplotlib-inline==0.1.3\nnumpy==1.23.1\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.12.0\npyparsing==3.0.9\nPySocks==1.7.1\npython-dateutil==2.8.2\npytz==2022.1\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscipy==1.8.1\nsix==1.16.0\nsoupsieve==2.3.2.post1\nstack-data==0.3.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.9.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\nurllib3==1.26.10\nwcwidth==0.2.5\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-07-14 \nPage render time: 30 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [
      "working_with_rnns_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}