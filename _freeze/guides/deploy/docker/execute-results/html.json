{
  "hash": "d849426286180a00e0a4ba99f48c9ee7",
  "result": {
    "markdown": "---\ntitle: \"TensorFlow serving\"\ntype: docs\nmenu:\n  main:\n    name: \"TensorFlow Serving\"\n    identifier: \"deploy-tf-serving\"\n    parent: \"deploy-top\"\n    weight: 30\neditor_options: \n  chunk_output_type: console\naliases:\n  - ../../deploy/docker/index.html\n---\n\n\nIn this tutorial you will learn how to deploy a TensorFlow model using [TensorFlow serving](https://www.tensorflow.org/tfx/serving/docker).\n\nWe will use the Docker container provided by the TensorFlow organization to deploy a model that classifies images of handwritten digits.\n\nUsing the Docker container is a an easy way to test the API locally and\nthen deploy it to any cloud provider.\n\n## Building the model\n\nThe first thing we are going to do is to build our model.\nWe will use the Keras API to build this model.\n\nWe will use the MNIST dataset to build our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nmnist$train$x <- (mnist$train$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x <- (mnist$test$x/255) %>% \n  array_reshape(., dim = c(dim(.), 1))\n```\n:::\n\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural \nnetwork.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% \n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n```\n:::\n\n\nNext, we fit the model using the MNIST dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n```\n:::\n\n\nWhen we are happy with our model accuracy in the validation dataset we can `evaluate` \nthe results on the test dataset with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(x = mnist$test$x, y = mnist$test$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      loss   accuracy \n0.04058329 0.98740000 \n```\n:::\n:::\n\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model.\nFirst, let's save the model in the `SavedModel` format using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_tf(model, \"cnn-mnist\")\n```\n:::\n\n\nWith the model built and saved we can now start building our plumber API file.\n\n## Running locally\n\nYou can run the `tensorflow/serving` Docker image locally using the great [`stevedore`](https://github.com/richfitz/stevedore)\npackage. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocker <- stevedore::docker_client()\ncontainer <- docker$container$run(\n  image = \"tensorflow/serving\", # name of the image\n  \n  # host port and docker port - if you set 4000:8501, the API \n  # will be accecible in localhost:4000\n  port = \"8501:8501\", \n  \n  # a string path/to/the/saved/model/locally:models/modelname/version\n  # you must put the model file in the /models/ folder.\n  volume = paste0(normalizePath(\"cnn-mnist\"), \":/models/model/1\"), \n  \n  # the name of the model - it's the name of the folder inside `/models`\n  # above.\n  env = c(\"MODEL_NAME\" = \"model\"),\n  \n  # to run the container detached\n  detach = TRUE\n)\n```\n:::\n\n\nNow we have initialized the container serving the model. You can see the container\nlogs with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontainer$logs()\n```\n:::\n\n\nNow you can make POST requests no the following endpoint : `http://localhost:8501/v1/models/model/versions/1:predict`.\nThe input data must be passed in a special format 0 - see the format definition [here](https://www.tensorflow.org/tfx/serving/api_rest), which may\nseem unnatural for R users. Here is an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstances <- purrr::array_tree(mnist$test$x[1:5,,,,drop=FALSE]) %>% \n  purrr::map(~list(input_1 = .x))\ninstances <- list(instances = instances)\n\nreq <- httr::POST(\n  \"http://localhost:8501/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n```\n:::\n\n\nThis is how you can serve TensorFlow models with TF serving locally.\nAdditionaly, we can deploy this to multiple clouds. In the next section we will show how it can be deployed to Google Cloud.\n\nWhen done, you can stop the container with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontainer$stop()\n```\n:::\n\n\n## Deploying to Google Cloud Run\n\nTHe first thing you need to do is to follow the section `Before you begin` in this [page](https://cloud.google.com/run/docs/quickstarts/build-and-deploy#before-you-begin).\n\nNow let's create a `Dockerfile` that will copy the SavedModel to the container image. We assume in this section some experience with Docker.\n\nHere's an example - create a file called `Dockerfile` in the same root folder as your SavedModel and paste the following:\n\n```\nFROM tensorflow/serving\nCOPY cnn-mnist /models/model/1\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\", \"--rest_api_port=8080\"]\n```\n\nWe need to run the rest service in the 8080 port. The only that is open by Google Cloud Run. Now you can build this image and send it to gcr.io.\nRun the following in your terminal:\n\n```\ndocker build -t gcr.io/PROJECT-ID/cnn-mnist .\ndocker push gcr.io/PROJECT-ID/cnn-mnist\n```\n\nYou can get your `PROJECT-ID` by running:\n\n```\ngcloud config get-value project\n```\n\nNext, we can create the service in Google Cloud Run using:\n\n```\ngcloud run deploy --image gcr.io/rstudio-162821/cnn-mnist --platform managed\n```\n\nYou will be prompted to select a region, a name for the service and wether you allow unauthorized requests. If everything works correctly \nyou will get a url like `https://cnn-mnist-ld4lzfalyq-ue.a.run.app` \nwhich you can now use to make requests to your model. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreq <- httr::POST(\n  \"https://cnn-mnist-ld4lzfalyq-ue.a.run.app/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n```\n:::\n\n\nNote that in this case, all pre-processing must be done in R before sending the data to the API. \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}