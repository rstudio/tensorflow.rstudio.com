---
title: "Overview"
type: docs
menu:
  main:
    name: "Overview"
    identifier: "installation-gpu-quick"
    parent: "installation-gpu"
    weight: 10
aliases:
  - /tools/gpu.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

It's highly recommended, although not strictly necessary, that you run
deep-learning code on a modern NVIDIA GPU. Some applications -- in
particular, image processing with convolutional networks and sequence
processing with recurrent neural networks -- will be excruciatingly slow
on CPU, even a fast multicore CPU. And even for applications that can
realistically be run on CPU, you'll generally see speed increase by a
factor or 5 or 10 by using a modern GPU.

If your local workstation doesn't already have a GPU that you can use
for deep learning (a recent, high-end NVIDIA GPU), then running deep
learning experiments in the cloud is a simple, low-cost way for you to
get started without having to buy any additional hardware. See the
documentation below for details on using both local and cloud GPUs.

|                                                                                                                                                   |                                                                                                                                                                                                                                                                               |
|---------------------|---------------------------------------------------|
| <a href="/installation/gpu/local_gpu">Local GPU<br/><img src="images/local-gpu-illustration.png" class="nav-image" width="64/"/></a>              | For systems that have a recent, high-end NVIDIAÂ® GPU, TensorFlow is available in a GPU version that takes advantage of the CUDA and cuDNN libraries to accelerate training performance. Similarly, Arm Macs can take advantage of the GPU.                                    |
| <a href="/tools/cloudml/getting_started">CloudML<br/><img src="images/cloud-ml-illustration.png" class="nav-image" width="64/"/></a>              | Google CloudML is a managed service that provides on-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA. CloudML also provides hyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy. |
| <a href="/installation/gpu/cloud_server_gpu">Cloud Server<br/><img src="images/cloud-server-illustration.png" class="nav-image" width="64/"/></a> | Cloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally.                                                 |

                                                                                                                                                      |


<!-- Paperspace image needs to be updated -->
<!-- | <a href="/installation/gpu/cloud_desktop_gpu">Cloud Desktop<br/><img src="images/cloud-desktop-illustration.png" class="nav-image" width="64/"/></a> | Virtual cloud desktops with GPUs are available from Paperspace. This provides an Ubuntu 16.04 desktop environment that you can access entirely within a web browser (note that this requires a reasonably fast internet connection to be usable).                                                                                                                                                 | -->
